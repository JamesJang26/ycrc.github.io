{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"COVID-19/SARS-CoV-2 Research If you're doing computational research related to COVID-19/SARS-CoV-2 on an HPC cluster and experience any long wait times for jobs to run, please let us know . Introduction The Yale Center for Research Computing provides support for research computing at Yale University. Our most active area for support is High Performance Computing, however we also support other computational intensive research such as Geo-Spatial Computation and Data Visualization. In addition, we work with faculty and research groups across disciplines to design and maintain cost-effective computing capabilities. Get Help To best serve the research community, we hold office hours and are also available through a support tracking system. Troubleshooting Login Issues If you are experiencing issues logging into one of the clusters, please first check the current System Status for known issues and use the Troubleshoot Login guide to address common issues before seeking additional assistance. In Person Attention Consistent with University guidance in response to COVID-19, office hours will be held remotely via Zoom . Research support staff will be online during regular business hours (Monday - Friday, 9:00 am to 5:00 pm), however, we ask that you either contact a staff member individually via email to set up a virtual appointment, or email us at research.computing@yale.edu with details about your request and we will have one of our staff members reach out to you. The YCRC is located on the second floor of 160 Saint Ronan Street . Visitor parking is available upon request. There are also Red, Orange, Blue and Green shuttle stops within a block of our center. Specialist Day Hours Office Areas of Focus Kaylea Nelson, Ph.D. Mon 10am-Noon 225 Grace / Milgram , Astronomy, G&G, MPI, Python Tom Langford, Ph.D. Mon 2-4pm 225 Grace , Physics, Python Michael Strickler, Ph.D. Tues 9-11am 222 Farnam / Ruddle / Milgram , Life Sciences, Structural Biology David Huberdeau, Ph.D. Tues Thurs 11am-Noon 11am-Noon 216 Milgram , Matlab, XNAT, Brain Imaging Dong Wang, Ph.D. Tues 1:30-3:30pm 216 Grace , Physics, Matlab, Image Processing Ben Evans, Ph.D. Wed 10am-Noon 224 Farnam / Ruddle / Milgram , Life Sciences, Bioinformatics, Python, OOD Portal Ping Luo Wed 2pm-4pm 216 Grace / Milgram , MPI, OpenMP, C/C++, Fortran, ParaView, OOD Portal Giuseppe Amatulli, Ph.D. Wed Fri 2-4pm 10:30am-Noon 216 Geo-Computation, GIS and Remote Sensing, Forestry Rob Bjornson, Ph.D. Thurs 2-4pm 216 Farnam / Ruddle , Life Sciences, Bioinformatics, Python Andy Sherman, Ph.D. Fri 1:30-3:30pm 217 Grace , MPI, GPUs, Matlab Misha Guy, Ph.D. via email SRSC Software and Mathematica Web and Email Support To submit requests, issues, or questions please send us an email at hpc@yale.edu or sign on to our online support system at help.ycrc.yale.edu . Your login credentials are your email and a password of your choosing, not your CAS password. Once received, our system will send an automated response to your inquiry with a link to a ticket. From there we'll track your ticket and make sure it's handled properly by the right person. Replies via email or the online support system go to the same place and are interchangeable. Constructive feedback is much appreciated. Q&A Platform The YCRC recently joined a Q&A platform at ask.cyberinfrastructure.org . Feel free to post questions about the clusters there and receive answers from YCRC staff or even your peers! The sub-site for YCRC related questions is available at ask.cyberinfrastructure.org/c/yale .","title":"Index"},{"location":"#introduction","text":"The Yale Center for Research Computing provides support for research computing at Yale University. Our most active area for support is High Performance Computing, however we also support other computational intensive research such as Geo-Spatial Computation and Data Visualization. In addition, we work with faculty and research groups across disciplines to design and maintain cost-effective computing capabilities.","title":"Introduction"},{"location":"#get-help","text":"To best serve the research community, we hold office hours and are also available through a support tracking system. Troubleshooting Login Issues If you are experiencing issues logging into one of the clusters, please first check the current System Status for known issues and use the Troubleshoot Login guide to address common issues before seeking additional assistance.","title":"Get Help"},{"location":"#in-person","text":"Attention Consistent with University guidance in response to COVID-19, office hours will be held remotely via Zoom . Research support staff will be online during regular business hours (Monday - Friday, 9:00 am to 5:00 pm), however, we ask that you either contact a staff member individually via email to set up a virtual appointment, or email us at research.computing@yale.edu with details about your request and we will have one of our staff members reach out to you. The YCRC is located on the second floor of 160 Saint Ronan Street . Visitor parking is available upon request. There are also Red, Orange, Blue and Green shuttle stops within a block of our center. Specialist Day Hours Office Areas of Focus Kaylea Nelson, Ph.D. Mon 10am-Noon 225 Grace / Milgram , Astronomy, G&G, MPI, Python Tom Langford, Ph.D. Mon 2-4pm 225 Grace , Physics, Python Michael Strickler, Ph.D. Tues 9-11am 222 Farnam / Ruddle / Milgram , Life Sciences, Structural Biology David Huberdeau, Ph.D. Tues Thurs 11am-Noon 11am-Noon 216 Milgram , Matlab, XNAT, Brain Imaging Dong Wang, Ph.D. Tues 1:30-3:30pm 216 Grace , Physics, Matlab, Image Processing Ben Evans, Ph.D. Wed 10am-Noon 224 Farnam / Ruddle / Milgram , Life Sciences, Bioinformatics, Python, OOD Portal Ping Luo Wed 2pm-4pm 216 Grace / Milgram , MPI, OpenMP, C/C++, Fortran, ParaView, OOD Portal Giuseppe Amatulli, Ph.D. Wed Fri 2-4pm 10:30am-Noon 216 Geo-Computation, GIS and Remote Sensing, Forestry Rob Bjornson, Ph.D. Thurs 2-4pm 216 Farnam / Ruddle , Life Sciences, Bioinformatics, Python Andy Sherman, Ph.D. Fri 1:30-3:30pm 217 Grace , MPI, GPUs, Matlab Misha Guy, Ph.D. via email SRSC Software and Mathematica","title":"In Person"},{"location":"#web-and-email-support","text":"To submit requests, issues, or questions please send us an email at hpc@yale.edu or sign on to our online support system at help.ycrc.yale.edu . Your login credentials are your email and a password of your choosing, not your CAS password. Once received, our system will send an automated response to your inquiry with a link to a ticket. From there we'll track your ticket and make sure it's handled properly by the right person. Replies via email or the online support system go to the same place and are interchangeable. Constructive feedback is much appreciated.","title":"Web and Email Support"},{"location":"#qa-platform","text":"The YCRC recently joined a Q&A platform at ask.cyberinfrastructure.org . Feel free to post questions about the clusters there and receive answers from YCRC staff or even your peers! The sub-site for YCRC related questions is available at ask.cyberinfrastructure.org/c/yale .","title":"Q&amp;A Platform"},{"location":"national-hpcs/","text":"National HPCs Beyond Yale\u2019s on campus clusters, there are a number of ways for researchers to obtain compute resources (both cycles and storage) at national facilities. Yale researchers may use the Data Management Planning Tool ( DMPtool ) to create, review, and share data management plans that are in accordance with institutional and funder requirements. XSEDE Quarterly | Application & Info Startup Allocations are readily available on XSEDE resources (typically approved within 2 weeks) for benchmarking and planning runs. For even lower commitment allocations (e.g. to just explore the resource), YCRC staff members have \"Campus Champions\" allocations on all XSEDE resources that can be shared upon request. Send an email to hpc@yale.edu for access. XSEDE resources include the following. Up to date information is available at xsede.org : Stampede2: traditional compute and Phis Jetstream: Science Gateways Bridges: traditional compute and GPUs Comet: traditional compute and GPUs XStream: GPU cluster Department of Energy NERSC, Argonne Leadership Computing Facility (ALCF), Oak Ridge Leadership Computing Facility (OLCF) INCITE Due in June | Application & Info ALCC Due in June | Application & Info ANL Director\u2019s Discretionary Rolling submission | Application & Info 3-6 month duration. Expectation is that you are using it to gather data for ALCC or INCITE proposal OLCF Director\u2019s Discretionary Rolling submission | Application & Info NCSA: Blue Waters PRAC Due in November | Application & Info Blue Water\u2019s Innovation Allocations Rolling submission | Application & Info Open Science Grid (OSG) Rolling Submission | Application & Info The OSG facilitates access to distributed high throughput computing for research in the US. The resources accessible through the OSG are contributed by the community, organized by the OSG, and governed by the OSG consortium.","title":"National HPCs"},{"location":"national-hpcs/#national-hpcs","text":"Beyond Yale\u2019s on campus clusters, there are a number of ways for researchers to obtain compute resources (both cycles and storage) at national facilities. Yale researchers may use the Data Management Planning Tool ( DMPtool ) to create, review, and share data management plans that are in accordance with institutional and funder requirements.","title":"National HPCs"},{"location":"national-hpcs/#xsede","text":"Quarterly | Application & Info Startup Allocations are readily available on XSEDE resources (typically approved within 2 weeks) for benchmarking and planning runs. For even lower commitment allocations (e.g. to just explore the resource), YCRC staff members have \"Campus Champions\" allocations on all XSEDE resources that can be shared upon request. Send an email to hpc@yale.edu for access. XSEDE resources include the following. Up to date information is available at xsede.org : Stampede2: traditional compute and Phis Jetstream: Science Gateways Bridges: traditional compute and GPUs Comet: traditional compute and GPUs XStream: GPU cluster","title":"XSEDE"},{"location":"national-hpcs/#department-of-energy","text":"NERSC, Argonne Leadership Computing Facility (ALCF), Oak Ridge Leadership Computing Facility (OLCF)","title":"Department of Energy"},{"location":"national-hpcs/#incite","text":"Due in June | Application & Info","title":"INCITE"},{"location":"national-hpcs/#alcc","text":"Due in June | Application & Info","title":"ALCC"},{"location":"national-hpcs/#anl-directors-discretionary","text":"Rolling submission | Application & Info 3-6 month duration. Expectation is that you are using it to gather data for ALCC or INCITE proposal","title":"ANL Director\u2019s Discretionary"},{"location":"national-hpcs/#olcf-directors-discretionary","text":"Rolling submission | Application & Info","title":"OLCF Director\u2019s Discretionary"},{"location":"national-hpcs/#ncsa-blue-waters","text":"","title":"NCSA: Blue Waters"},{"location":"national-hpcs/#prac","text":"Due in November | Application & Info","title":"PRAC"},{"location":"national-hpcs/#blue-waters-innovation-allocations","text":"Rolling submission | Application & Info","title":"Blue Water\u2019s Innovation Allocations"},{"location":"national-hpcs/#open-science-grid-osg","text":"Rolling Submission | Application & Info The OSG facilitates access to distributed high throughput computing for research in the US. The resources accessible through the OSG are contributed by the community, organized by the OSG, and governed by the OSG consortium.","title":"Open Science Grid (OSG)"},{"location":"online-tutorials/","text":"Online Tutorials Linux/Unix and Command Line Introduction to Linux Note: You can learn more about most commands you come across by typing \"man [command]\" into the terminal. Practical Introduction to Linux , ( Video ) *Recommended Unix for Beginners Interactive command-line bootcamp Introduction to Bash awk (text extraction/parsing) Examples of awk command usage Full awk tutorial grep Grep is useful for searching command line output for a certain phrase or regular expression Basic Usage In-depth guide sed Sed (Stream EDitor) is a useful tool for making substitutions in a text file. The syntax sed uses for substitutions is common in Linux (for example, the same syntax is used in the VIM text editor) Basic Usage Brief guide to sed In-depth guide to sed SSH (connecting to the clusters) Connecting to the Yale clusters Transfer files to/from the cluster Basics of SSH Bashrc Basics Bashrc vs bash_profile How create and extract a tar or tar.gz archive The command you are looking for is likely: tar xvf archive.tar , but see the following for details. Creating and extracting from a tar file How to Compile and Link Libraries How Programs Work on Linux Dual Booting Windows and Linux Dual Boot Linux Mint and Windows Dual Boot Ubuntu and Windows Python Intro to Python Fantastic resource for anyone interested in Python LinkedIn Learning: Learning Python (Yale only) Parallel Programming with Python Quick Tutorial: Python Multiprocessing Parallel Programming with Python YCRC tutorial course material mpi4py Documentation for mpi4py YCRC mpi4py tutorial YCRC mpi4py tutorial video YCRC mpi4py example scripts R Intro to R Brief Intro to R Thorough intro to R foreach Using the foreach package, by Steve Weston foreach + dompi Introduction to doMPI Matlab Mathworks Online Classses Singularity Documentation Sylabs Docs Page Singularity Google Groups Tutorials NIH tutorial YCRC container tutorial NVIDIA tutorial for using GPUs","title":"Online Tutorials"},{"location":"online-tutorials/#online-tutorials","text":"","title":"Online Tutorials"},{"location":"online-tutorials/#linuxunix-and-command-line","text":"","title":"Linux/Unix and Command Line"},{"location":"online-tutorials/#introduction-to-linux","text":"Note: You can learn more about most commands you come across by typing \"man [command]\" into the terminal. Practical Introduction to Linux , ( Video ) *Recommended Unix for Beginners Interactive command-line bootcamp Introduction to Bash","title":"Introduction to Linux"},{"location":"online-tutorials/#awk-text-extractionparsing","text":"Examples of awk command usage Full awk tutorial","title":"awk (text extraction/parsing)"},{"location":"online-tutorials/#grep","text":"Grep is useful for searching command line output for a certain phrase or regular expression Basic Usage In-depth guide","title":"grep"},{"location":"online-tutorials/#sed","text":"Sed (Stream EDitor) is a useful tool for making substitutions in a text file. The syntax sed uses for substitutions is common in Linux (for example, the same syntax is used in the VIM text editor) Basic Usage Brief guide to sed In-depth guide to sed","title":"sed"},{"location":"online-tutorials/#ssh-connecting-to-the-clusters","text":"Connecting to the Yale clusters Transfer files to/from the cluster Basics of SSH","title":"SSH (connecting to the clusters)"},{"location":"online-tutorials/#bashrc","text":"Basics Bashrc vs bash_profile","title":"Bashrc"},{"location":"online-tutorials/#how-create-and-extract-a-tar-or-targz-archive","text":"The command you are looking for is likely: tar xvf archive.tar , but see the following for details. Creating and extracting from a tar file","title":"How create and extract a tar or tar.gz archive"},{"location":"online-tutorials/#how-to-compile-and-link-libraries","text":"How Programs Work on Linux","title":"How to Compile and Link Libraries"},{"location":"online-tutorials/#dual-booting-windows-and-linux","text":"Dual Boot Linux Mint and Windows Dual Boot Ubuntu and Windows","title":"Dual Booting Windows and Linux"},{"location":"online-tutorials/#python","text":"","title":"Python"},{"location":"online-tutorials/#intro-to-python","text":"Fantastic resource for anyone interested in Python LinkedIn Learning: Learning Python (Yale only)","title":"Intro to Python"},{"location":"online-tutorials/#parallel-programming-with-python","text":"Quick Tutorial: Python Multiprocessing Parallel Programming with Python YCRC tutorial course material","title":"Parallel Programming with Python"},{"location":"online-tutorials/#mpi4py","text":"Documentation for mpi4py YCRC mpi4py tutorial YCRC mpi4py tutorial video YCRC mpi4py example scripts","title":"mpi4py"},{"location":"online-tutorials/#r","text":"","title":"R"},{"location":"online-tutorials/#intro-to-r","text":"Brief Intro to R Thorough intro to R","title":"Intro to R"},{"location":"online-tutorials/#foreach","text":"Using the foreach package, by Steve Weston","title":"foreach"},{"location":"online-tutorials/#foreach-dompi","text":"Introduction to doMPI","title":"foreach + dompi"},{"location":"online-tutorials/#matlab","text":"Mathworks Online Classses","title":"Matlab"},{"location":"online-tutorials/#singularity","text":"","title":"Singularity"},{"location":"online-tutorials/#documentation","text":"Sylabs Docs Page Singularity Google Groups","title":"Documentation"},{"location":"online-tutorials/#tutorials","text":"NIH tutorial YCRC container tutorial NVIDIA tutorial for using GPUs","title":"Tutorials"},{"location":"user-group/","text":"YCRC User Group The YCRC User Group is a community of researchers at Yale who utilize computing resources and technology to enable their research. The User Group holds monthly meetings, each on a distinct topic of interest to the community. During meeting there are opportunities for members of the research community to teach and learn from their peers in a mix of panel discussions, presentations, lightning talks, working groups and informal discussions. User Group meetings are held monthly in the YCRC Auditorium on the second Wednesday of each month at 4:00 pm. You can join the User Group mailing list and forum at https://groups.io/g/ycrcusergroup .","title":"YCRC User Group"},{"location":"user-group/#ycrc-user-group","text":"The YCRC User Group is a community of researchers at Yale who utilize computing resources and technology to enable their research. The User Group holds monthly meetings, each on a distinct topic of interest to the community. During meeting there are opportunities for members of the research community to teach and learn from their peers in a mix of panel discussions, presentations, lightning talks, working groups and informal discussions. User Group meetings are held monthly in the YCRC Auditorium on the second Wednesday of each month at 4:00 pm. You can join the User Group mailing list and forum at https://groups.io/g/ycrcusergroup .","title":"YCRC User Group"},{"location":"clusters-at-yale/","text":"Getting Started HPC Clusters Broadly speaking, a compute cluster is a collection of networked computers which we call nodes. Our clusters are only accessible to researchers remotely; your gateways to the cluster are the login nodes . From these nodes, you will be able to view your files and dispatch jobs to one or several other nodes across the cluster configured for computation, called compute nodes . The tool we use to submit these jobs is called a job scheduler . All compute nodes on a cluster mount a shared filesystem ; a file server or set of servers that keeps track of all files on a large array of disks, so that you can access and edit your data from any compute node. Detailed information about each of our clusters is available here . Request an Account The first step in gaining access to our clusters is to request an account. There are several HPC clusters available at Yale. There is no charge for using these clusters. To understand which cluster is appropriate for you and to request an account, visit the account request page . Log in All of Yale's clusters are accessed via a protocol called secure shell (SSH). Once you have an account, go to our Log on to the Clusters page login information and configuration. If you want to access the clusters from outside Yale, you must use the Yale VPN. Schedule a Job On our clusters, you control your jobs using a job scheduling system called Slurm that dedicates and manages compute resources for you. Schedulers are usually used in one of two ways. For testing and small jobs you may want to run a job interactively . This way you can directly interact with the compute node(s) in real time to make sure your jobs will behave as expected. The other way, which is the preferred way for long-running jobs, involves writing your job commands in a script and submitting that to the job scheduler. Please see our Slurm documentation or attend the HPC bootcamp for more details. Linux A basically familiarity with Linux commands is required for interacting with the clusters. We periodically run an Intro to Linux Bootcamp to get you started. There are also many excellent beginner tutorials available for free online, including the following: Unix Tutorial for Beginners Interactive Command Line Bootcamp Move Your Files You will likely find it necessary to copy files between your local machines and the clusters. Just as with logging in, there are different ways to do this, depending on your local operating system. See the documentation on transferring data for more information. Use Software To best serve the diverse needs of all our researhcers, we use a module system to manage the most commonly used software. This allows you to swap between different applications and versions of those applications with relative ease and focus on getting your work done. See the Modules documentation in our User Guide for more information. We also provide assistance for installing less commonly used packages. See our Applications & Software documentation for more details. Rules of the Road Before you begin using the cluster, here are some important things to remember: Do not run jobs or do real work on the login node. Always allocate a compute node and run programs there. Never give your password or ssh key to anyone else. Do not store any protected or regulated data on the cluster (e.g. PHI data) Use of the clusters is also governed by our official guidelines . Hands on Training We offer several courses that will assist you with your work on our clusters. They range from orientation for absolute beginners to advanced topics on application-specific optimization. Please peruse our catalog of training to see what is available. Get Additional Help If you have additional questions/comments, please contact the YCRC team . If possible, please include the following information: Your netid Cluster Queue/partition name Job ID(s) Error messages Command used to submit the job(s) Path(s) to scripts called by the submission command Path(s) to output files from your jobs","title":"Getting Started"},{"location":"clusters-at-yale/#getting-started","text":"","title":"Getting Started"},{"location":"clusters-at-yale/#hpc-clusters","text":"Broadly speaking, a compute cluster is a collection of networked computers which we call nodes. Our clusters are only accessible to researchers remotely; your gateways to the cluster are the login nodes . From these nodes, you will be able to view your files and dispatch jobs to one or several other nodes across the cluster configured for computation, called compute nodes . The tool we use to submit these jobs is called a job scheduler . All compute nodes on a cluster mount a shared filesystem ; a file server or set of servers that keeps track of all files on a large array of disks, so that you can access and edit your data from any compute node. Detailed information about each of our clusters is available here .","title":"HPC Clusters"},{"location":"clusters-at-yale/#request-an-account","text":"The first step in gaining access to our clusters is to request an account. There are several HPC clusters available at Yale. There is no charge for using these clusters. To understand which cluster is appropriate for you and to request an account, visit the account request page .","title":"Request an Account"},{"location":"clusters-at-yale/#log-in","text":"All of Yale's clusters are accessed via a protocol called secure shell (SSH). Once you have an account, go to our Log on to the Clusters page login information and configuration. If you want to access the clusters from outside Yale, you must use the Yale VPN.","title":"Log in"},{"location":"clusters-at-yale/#schedule-a-job","text":"On our clusters, you control your jobs using a job scheduling system called Slurm that dedicates and manages compute resources for you. Schedulers are usually used in one of two ways. For testing and small jobs you may want to run a job interactively . This way you can directly interact with the compute node(s) in real time to make sure your jobs will behave as expected. The other way, which is the preferred way for long-running jobs, involves writing your job commands in a script and submitting that to the job scheduler. Please see our Slurm documentation or attend the HPC bootcamp for more details.","title":"Schedule a Job"},{"location":"clusters-at-yale/#linux","text":"A basically familiarity with Linux commands is required for interacting with the clusters. We periodically run an Intro to Linux Bootcamp to get you started. There are also many excellent beginner tutorials available for free online, including the following: Unix Tutorial for Beginners Interactive Command Line Bootcamp","title":"Linux"},{"location":"clusters-at-yale/#move-your-files","text":"You will likely find it necessary to copy files between your local machines and the clusters. Just as with logging in, there are different ways to do this, depending on your local operating system. See the documentation on transferring data for more information.","title":"Move Your Files"},{"location":"clusters-at-yale/#use-software","text":"To best serve the diverse needs of all our researhcers, we use a module system to manage the most commonly used software. This allows you to swap between different applications and versions of those applications with relative ease and focus on getting your work done. See the Modules documentation in our User Guide for more information. We also provide assistance for installing less commonly used packages. See our Applications & Software documentation for more details.","title":"Use Software"},{"location":"clusters-at-yale/#rules-of-the-road","text":"Before you begin using the cluster, here are some important things to remember: Do not run jobs or do real work on the login node. Always allocate a compute node and run programs there. Never give your password or ssh key to anyone else. Do not store any protected or regulated data on the cluster (e.g. PHI data) Use of the clusters is also governed by our official guidelines .","title":"Rules of the Road"},{"location":"clusters-at-yale/#hands-on-training","text":"We offer several courses that will assist you with your work on our clusters. They range from orientation for absolute beginners to advanced topics on application-specific optimization. Please peruse our catalog of training to see what is available.","title":"Hands on Training"},{"location":"clusters-at-yale/#get-additional-help","text":"If you have additional questions/comments, please contact the YCRC team . If possible, please include the following information: Your netid Cluster Queue/partition name Job ID(s) Error messages Command used to submit the job(s) Path(s) to scripts called by the submission command Path(s) to output files from your jobs","title":"Get Additional Help"},{"location":"clusters-at-yale/troubleshoot/","text":"Troubleshoot Login If you are having trouble logging in, please check the following: Check the status page to make sure that your cluster is online: System Status . Accounts are only created for you on the clusters as you requested them. To get access to additional clusters, submit another account request . Verify that you are attempting to ssh to the correct hostname. Please see the cluster page for a list of login addresses. Verify that your ssh keys are setup correctly. Make sure your public key is uploaded to the ssh key uploader . It will take a few minutes for newly uploaded keys to appear on the clusters. If you are on Windows, make sure you have pointed MobaXterm to your private ssh key and if you are on macOS or Linux, your private key needs to be in ${HOME}/.ssh . We use ssh keys to authenticate logins to the clusters, and not NetID passwords. If you are asked for a \"passphrase\" upon logging in, this is the ssh key passphrase you configured when first creating your key. If you have forgotten your passphrase, you will need to create and upload a new ssh key pair (see our SSH Guide ). Make sure you are accessing the cluster from either Yale's campus networks (ethernet or YaleSecure for wireless) or Yale's VPN if you are off-campus. The home directory should only be write-able by your NetID. If you recently modified the permissions to your home directory, contact us at hpc@yale.edu and we can fix the permissions for you. If you get an error like \"could not resolve hostname\" make sure that you are using the Yale DNS servers (130.132.1.9,10,11). External DNS servers do not list our clusters. If you are using Ruddle, they require Duo MFA on your smartphone. If Duo is not working for you, try testing it on this ITS site: http://access.yale.edu/mfa . If that doesn't work, contact the Yale Help Desk. If all of the above check out ok, please contact us and we will investigate further. Please include your netid and the cluster you are attempting to connect to in your email. \"REMOTE HOST IDENTIFICATION HAS CHANGED!\" Warning If you are seeing the following error: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! .... Offending key in /home/user/.ssh/known_hosts:34 ... This error means that the host keys on the cluster have changed. This may be the result of system upgrades on the cluster. It could also mean someone is trying to intercept your ssh session. Please contact us if you receive this error. If the host keys have indeed changed on the server you are connecting to, you can edit ~/.ssh/known_hosts and remove the offending line. In the example above, line 34 in ~/.ssh/known_hosts would have to be deleted before you re-connect.","title":"Troubleshoot Login"},{"location":"clusters-at-yale/troubleshoot/#troubleshoot-login","text":"If you are having trouble logging in, please check the following: Check the status page to make sure that your cluster is online: System Status . Accounts are only created for you on the clusters as you requested them. To get access to additional clusters, submit another account request . Verify that you are attempting to ssh to the correct hostname. Please see the cluster page for a list of login addresses. Verify that your ssh keys are setup correctly. Make sure your public key is uploaded to the ssh key uploader . It will take a few minutes for newly uploaded keys to appear on the clusters. If you are on Windows, make sure you have pointed MobaXterm to your private ssh key and if you are on macOS or Linux, your private key needs to be in ${HOME}/.ssh . We use ssh keys to authenticate logins to the clusters, and not NetID passwords. If you are asked for a \"passphrase\" upon logging in, this is the ssh key passphrase you configured when first creating your key. If you have forgotten your passphrase, you will need to create and upload a new ssh key pair (see our SSH Guide ). Make sure you are accessing the cluster from either Yale's campus networks (ethernet or YaleSecure for wireless) or Yale's VPN if you are off-campus. The home directory should only be write-able by your NetID. If you recently modified the permissions to your home directory, contact us at hpc@yale.edu and we can fix the permissions for you. If you get an error like \"could not resolve hostname\" make sure that you are using the Yale DNS servers (130.132.1.9,10,11). External DNS servers do not list our clusters. If you are using Ruddle, they require Duo MFA on your smartphone. If Duo is not working for you, try testing it on this ITS site: http://access.yale.edu/mfa . If that doesn't work, contact the Yale Help Desk. If all of the above check out ok, please contact us and we will investigate further. Please include your netid and the cluster you are attempting to connect to in your email.","title":"Troubleshoot Login"},{"location":"clusters-at-yale/troubleshoot/#remote-host-identification-has-changed-warning","text":"If you are seeing the following error: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! .... Offending key in /home/user/.ssh/known_hosts:34 ... This error means that the host keys on the cluster have changed. This may be the result of system upgrades on the cluster. It could also mean someone is trying to intercept your ssh session. Please contact us if you receive this error. If the host keys have indeed changed on the server you are connecting to, you can edit ~/.ssh/known_hosts and remove the offending line. In the example above, line 34 in ~/.ssh/known_hosts would have to be deleted before you re-connect.","title":"\"REMOTE HOST IDENTIFICATION HAS CHANGED!\" Warning"},{"location":"clusters-at-yale/access/","text":"Log on to the Clusters We use SSH with SSH key pairs to log in to the clusters. You must be on campus to access the clusters or our SSH key uploader. For off-campus access you need to use the Yale VPN . Quick Start Send us your public SSH key with our SSH key uploader . Allow up to ten minutes for it to propagate. Once we have your public key you can connect with ssh netid@clustername.hpc.yale.edu . Login node addresses and other useful info can be found on the clusters page . To use graphical programs on the clusters, please see our guides on X11 Forwarding and VNC . If you are having trouble logging in : please read the rest of this page and our Troubleshoot Login page, then email us if you're still having issues. What are SSH keys SSH (Secure Shell) keys are a set of two pieces of information that you use to identify yourself and encrypt communication to and from a server. Usually this takes the form of two files: a public key (often saved as id_rsa.pub ) and a private key ( id_rsa or id_rsa.ppk ). To use an analogy, your public key is like a lock and your private key is what unlocks it. It is ok for others to see the lock (public key), but anyone who knows the private key can open your lock (and impersonate you). When you connect to a remote server in order to sign in, it will present your lock. You prove your identity by unlocking it with your secret key. As you continue communicating with the remote server, the data sent to you is also locked with your public key such that only you can unlock it with your private key. We use an automated system to distribute your public key onto the clusters, which you can log in to here . It is only accessible on campus or through the Yale VPN . All the public keys that are authorized to your account are stored in the file ~/.ssh/authorized_keys on the clusters you have been given access to. If you use multiple computers, you can either keep the same ssh key pair on every one or have a different set for each. Having only one is less complicated, but if your key pair is compromised you have to be worried about everywhere it is authorized. Warning Keep your private keys private! Anyone who has them can assume your identity on any server where your keys are authorized. We will never ask for your private key . For further reading we recommend starting with the Wikipedia articles about public-key cryptography and challenge-response authentication . macOS and Linux Generate Your Key Pair on macOS and Linux To generate a new key pair, first open a terminal/xterm session. If you are on macOS, open Applications -> Utilities -> Terminal. Generate your public and private ssh keys. Type the following into the terminal window: ssh-keygen Your terminal should respond: Generating public/private rsa key pair. Enter file in which to save the key (/home/yourusername/.ssh/id_rsa): Press Enter to accept the default value. Your terminal should respond: Enter passphrase (empty for no passphrase): Choose a secure passphrase. Your passphrase will prevent access to your account in the event your private key is stolen. You will not see any characters appear on the screen as you type. The response will be: Enter same passphrase again: Enter the passphrase again. The key pair is generated and written to a directory called .ssh in your home directory. The public key is stored in ~/.ssh/id_rsa.pub . If you forget your passphrase, it cannot be recovered. Instead, you will need to generate and upload a new SSH key pair. Next, upload your public SSH key on the cluster. Run the following command in a terminal: cat ~/.ssh/id_rsa.pub Copy and paste the output to our SSH key uploader . Note: It can take a few minutes for newly uploaded keys to sync out to the clusters so your login may not work immediately. Connect on macOS and Linux Once your key has been copied to the appropriate places on the clusters, you can log in with the command: ssh netid@clustername.hpc.yale.edu Check out our Advanced SSH Configuration for tips on maintaining connections and adding tab complete to your ssh commands on linux/macOS. Windows We recommend using MobaXterm to connect to the clusters. You can download, extract & install MobaXterm from this page . We recommend using the \"Installer Edition\", but make sure to extract the zip file before running the installer. You can also use one of the Windows Subsystem for Linux (WSL) distributions and follow the Linux instructions above. However, you will probably run into issues if you try to use any graphical applications. Generate Your Key Pair on Windows First, generate an SSH key pair if you haven't already: Open MobaXterm. From the top menu choose Tools -> MobaKeyGen (SSH key generator). Leave all defaults and click the \"Generate\" button. Wiggle your mouse. Click \"Save public key\" and save your public key as id_rsa.pub. Choose a secure passphrase and enter into the two relevant fields. Your passphrase will prevent access to your account in the event your private key is stolen. Click \"Save private key\" and save your private key as id_rsa.ppk (this one is secret, don't give it to other people ). Copy the text of your public key and paste it into the text box in our SSH key uploader . Your key will be synced out to the clusters in a few minutes. Connect with Windows To make a new connection to one of the clusters: Open MobaXterm. From the top menu select Sessions -> New Session. Click the SSH icon in the top left. Enter the cluster login node address (e.g. farnam.hpc.yale.edu) as the Remote Host. Check \"Specify Username\" and Enter your netID as the the username. Click the \"Advanced SSH Settings\" tab and check the \"Use private key box\", then click the file icon / magnifying glass to choose where you saved your private key (id_rsa.ppk). Click OK. In the future, your session should be saved in the sessions bar on the left in the main window.","title":"Log on to the Clusters"},{"location":"clusters-at-yale/access/#log-on-to-the-clusters","text":"We use SSH with SSH key pairs to log in to the clusters. You must be on campus to access the clusters or our SSH key uploader. For off-campus access you need to use the Yale VPN .","title":"Log on to the Clusters"},{"location":"clusters-at-yale/access/#quick-start","text":"Send us your public SSH key with our SSH key uploader . Allow up to ten minutes for it to propagate. Once we have your public key you can connect with ssh netid@clustername.hpc.yale.edu . Login node addresses and other useful info can be found on the clusters page . To use graphical programs on the clusters, please see our guides on X11 Forwarding and VNC . If you are having trouble logging in : please read the rest of this page and our Troubleshoot Login page, then email us if you're still having issues.","title":"Quick Start"},{"location":"clusters-at-yale/access/#what-are-ssh-keys","text":"SSH (Secure Shell) keys are a set of two pieces of information that you use to identify yourself and encrypt communication to and from a server. Usually this takes the form of two files: a public key (often saved as id_rsa.pub ) and a private key ( id_rsa or id_rsa.ppk ). To use an analogy, your public key is like a lock and your private key is what unlocks it. It is ok for others to see the lock (public key), but anyone who knows the private key can open your lock (and impersonate you). When you connect to a remote server in order to sign in, it will present your lock. You prove your identity by unlocking it with your secret key. As you continue communicating with the remote server, the data sent to you is also locked with your public key such that only you can unlock it with your private key. We use an automated system to distribute your public key onto the clusters, which you can log in to here . It is only accessible on campus or through the Yale VPN . All the public keys that are authorized to your account are stored in the file ~/.ssh/authorized_keys on the clusters you have been given access to. If you use multiple computers, you can either keep the same ssh key pair on every one or have a different set for each. Having only one is less complicated, but if your key pair is compromised you have to be worried about everywhere it is authorized. Warning Keep your private keys private! Anyone who has them can assume your identity on any server where your keys are authorized. We will never ask for your private key . For further reading we recommend starting with the Wikipedia articles about public-key cryptography and challenge-response authentication .","title":"What are SSH keys"},{"location":"clusters-at-yale/access/#macos-and-linux","text":"","title":"macOS and Linux"},{"location":"clusters-at-yale/access/#generate-your-key-pair-on-macos-and-linux","text":"To generate a new key pair, first open a terminal/xterm session. If you are on macOS, open Applications -> Utilities -> Terminal. Generate your public and private ssh keys. Type the following into the terminal window: ssh-keygen Your terminal should respond: Generating public/private rsa key pair. Enter file in which to save the key (/home/yourusername/.ssh/id_rsa): Press Enter to accept the default value. Your terminal should respond: Enter passphrase (empty for no passphrase): Choose a secure passphrase. Your passphrase will prevent access to your account in the event your private key is stolen. You will not see any characters appear on the screen as you type. The response will be: Enter same passphrase again: Enter the passphrase again. The key pair is generated and written to a directory called .ssh in your home directory. The public key is stored in ~/.ssh/id_rsa.pub . If you forget your passphrase, it cannot be recovered. Instead, you will need to generate and upload a new SSH key pair. Next, upload your public SSH key on the cluster. Run the following command in a terminal: cat ~/.ssh/id_rsa.pub Copy and paste the output to our SSH key uploader . Note: It can take a few minutes for newly uploaded keys to sync out to the clusters so your login may not work immediately.","title":"Generate Your Key Pair on macOS and Linux"},{"location":"clusters-at-yale/access/#connect-on-macos-and-linux","text":"Once your key has been copied to the appropriate places on the clusters, you can log in with the command: ssh netid@clustername.hpc.yale.edu Check out our Advanced SSH Configuration for tips on maintaining connections and adding tab complete to your ssh commands on linux/macOS.","title":"Connect on macOS and Linux"},{"location":"clusters-at-yale/access/#windows","text":"We recommend using MobaXterm to connect to the clusters. You can download, extract & install MobaXterm from this page . We recommend using the \"Installer Edition\", but make sure to extract the zip file before running the installer. You can also use one of the Windows Subsystem for Linux (WSL) distributions and follow the Linux instructions above. However, you will probably run into issues if you try to use any graphical applications.","title":"Windows"},{"location":"clusters-at-yale/access/#generate-your-key-pair-on-windows","text":"First, generate an SSH key pair if you haven't already: Open MobaXterm. From the top menu choose Tools -> MobaKeyGen (SSH key generator). Leave all defaults and click the \"Generate\" button. Wiggle your mouse. Click \"Save public key\" and save your public key as id_rsa.pub. Choose a secure passphrase and enter into the two relevant fields. Your passphrase will prevent access to your account in the event your private key is stolen. Click \"Save private key\" and save your private key as id_rsa.ppk (this one is secret, don't give it to other people ). Copy the text of your public key and paste it into the text box in our SSH key uploader . Your key will be synced out to the clusters in a few minutes.","title":"Generate Your Key Pair on Windows"},{"location":"clusters-at-yale/access/#connect-with-windows","text":"To make a new connection to one of the clusters: Open MobaXterm. From the top menu select Sessions -> New Session. Click the SSH icon in the top left. Enter the cluster login node address (e.g. farnam.hpc.yale.edu) as the Remote Host. Check \"Specify Username\" and Enter your netID as the the username. Click the \"Advanced SSH Settings\" tab and check the \"Use private key box\", then click the file icon / magnifying glass to choose where you saved your private key (id_rsa.ppk). Click OK. In the future, your session should be saved in the sessions bar on the left in the main window.","title":"Connect with Windows"},{"location":"clusters-at-yale/access/advanced-config/","text":"Advanced SSH Configuration Example SSH config The following configuration is an example ssh client configuration file specific to our clusters. You can use it on Linux, Windows Subsystem for Linux (WSL) , and macOS. It allows you to use tab completion of the clusters, without the .hpc.yale.edu suffixes (i.e. ssh farnam or scp ~/my_file farnam:my_file should work). It will also allow you to re-use and multiplex authenticated sessions. This means clusters that require Duo MFA will not force you to re-authenticate, as you use the same ssh connection to host multiple sessions. If you attempt to close your first connection with others running, it will wait until all others are closed. Save the text below to ~/.ssh/config and replace NETID with your Yale netid. Lines that begin with # will be ignored. # To re - use your connections with multi - factor authentication ( e . g . Ruddle ) # Uncomment the two lines below # ControlMaster auto # ControlPath ~/ . ssh / tmp /% h_ % p_ % r # If you use a ssh key that is named something other than id_rsa , # you can specify your private key like this : # IdentityFile ~/ . ssh / other_key_rsa # Uncomment the ForwardX11 options line to enable X11 Forwarding by default ( no - Y necessary ) # On a Mac you still need xquartz installed Host * . hpc . yale . edu farnam grace milgram ruddle User NETID # ForwardX11 yes Host farnam HostName farnam . hpc . yale . edu Host grace HostName grace . hpc . yale . edu Host milgram Hostname milgram . hpc . yale . edu Host ruddle HostName ruddle . hpc . yale . edu Warning For multiplexing to work, the ~/.ssh/tmp directory must exist. Create it with mkdir -p ~/.ssh/tmp For more info on ssh configuration, run: man ssh_config Store Passphrase and Use SSH Agent on macOS By default, macOS won't always remember your ssh key passphrase and keep your ssh key in the agent for SSH agent forwarding. In order to not repeatedly enter your passphrase and instead store it in your keychain, enter the following command on your Mac (just once): ssh-add -K ~/.ssh/id_rsa Or whatever your private key file is named. Note If you use homebrew your default OpenSSH may have changed. To add your key(s) to the system ssh agent, use the absolute path: /usr/bin/ssh-add Then and add the following to your ~/.ssh/config file (create this file if it doesn't exist, or add these settings to the Host *.hpc.yale.edu ... rule if it does). Host *.hpc.yale.edu farnam grace milgram ruddle UseKeychain yes AddKeystoAgent yes You can view a list of the keys currently in your agent with: ssh-add -L","title":"Advanced SSH Configuration"},{"location":"clusters-at-yale/access/advanced-config/#advanced-ssh-configuration","text":"","title":"Advanced SSH Configuration"},{"location":"clusters-at-yale/access/advanced-config/#example-ssh-config","text":"The following configuration is an example ssh client configuration file specific to our clusters. You can use it on Linux, Windows Subsystem for Linux (WSL) , and macOS. It allows you to use tab completion of the clusters, without the .hpc.yale.edu suffixes (i.e. ssh farnam or scp ~/my_file farnam:my_file should work). It will also allow you to re-use and multiplex authenticated sessions. This means clusters that require Duo MFA will not force you to re-authenticate, as you use the same ssh connection to host multiple sessions. If you attempt to close your first connection with others running, it will wait until all others are closed. Save the text below to ~/.ssh/config and replace NETID with your Yale netid. Lines that begin with # will be ignored. # To re - use your connections with multi - factor authentication ( e . g . Ruddle ) # Uncomment the two lines below # ControlMaster auto # ControlPath ~/ . ssh / tmp /% h_ % p_ % r # If you use a ssh key that is named something other than id_rsa , # you can specify your private key like this : # IdentityFile ~/ . ssh / other_key_rsa # Uncomment the ForwardX11 options line to enable X11 Forwarding by default ( no - Y necessary ) # On a Mac you still need xquartz installed Host * . hpc . yale . edu farnam grace milgram ruddle User NETID # ForwardX11 yes Host farnam HostName farnam . hpc . yale . edu Host grace HostName grace . hpc . yale . edu Host milgram Hostname milgram . hpc . yale . edu Host ruddle HostName ruddle . hpc . yale . edu Warning For multiplexing to work, the ~/.ssh/tmp directory must exist. Create it with mkdir -p ~/.ssh/tmp For more info on ssh configuration, run: man ssh_config","title":"Example SSH config"},{"location":"clusters-at-yale/access/advanced-config/#store-passphrase-and-use-ssh-agent-on-macos","text":"By default, macOS won't always remember your ssh key passphrase and keep your ssh key in the agent for SSH agent forwarding. In order to not repeatedly enter your passphrase and instead store it in your keychain, enter the following command on your Mac (just once): ssh-add -K ~/.ssh/id_rsa Or whatever your private key file is named. Note If you use homebrew your default OpenSSH may have changed. To add your key(s) to the system ssh agent, use the absolute path: /usr/bin/ssh-add Then and add the following to your ~/.ssh/config file (create this file if it doesn't exist, or add these settings to the Host *.hpc.yale.edu ... rule if it does). Host *.hpc.yale.edu farnam grace milgram ruddle UseKeychain yes AddKeystoAgent yes You can view a list of the keys currently in your agent with: ssh-add -L","title":"Store Passphrase and Use SSH Agent on macOS"},{"location":"clusters-at-yale/access/mfa/","text":"Multi-factor Authentication To improve security, access to Ruddle requires both a public key and multi-factor authentication (MFA). We use the same MFA (Duo) as is used elsewhere at Yale. To get set up with Duo, see these instructions. You will need upload your ssh public key to our site . For more info on how to use ssh, please see SSH instructions . Once you've set up Duo and your key is registered, you can finally log in. Use ssh to connect to your cluster of choice, and you will be prompted for a passcode or to select a notification option. We recommend choosing Duo Push (option 1). If you chose this option you should receive a notification on your phone. Once approved, you should be allowed to continue to log in. Note You can set up more than one phone for Duo. For example, you can set up your smartphone plus your office landline. That way, if you forget or lose your phone, you can still authenticate. For instructions on how to add additional phones go here . File Transfer and Duo MFA Some file transfer clients attempt new and sometimes multiple concurrent connections to transfer files for you. When this happens, you will be asked to Duo authenticate for each connection. Setting up a config file lets you re-uses your authenticated sessions for command-line tools and tools that respect your ssh configuration. For CyberDuck, see our section on the Transfer Data page . Troubleshoot MFA If you are having problems initially registering Duo, please contact the Yale ITS Helpdesk . If you have successfully used MFA connect to a cluster before, but cannot now, first please check the following: Test MFA using http://access.yale.edu Verify that your ssh client is using the correct login node Verify you are attempting to connect from a Yale machine or via the proper VPN If all of this is true, please contact us at hpc@yale.edu . Include the following information (and anything else you think is helpful): Your netid Have you ever successfully used ssh and Duo to connect to a cluster? How long have you been having problems? Where are you trying to connect from? (fully qualified hostname/IP, if possible) Are you using a VPN? What is the error message you see?","title":"Multi-factor Authentication"},{"location":"clusters-at-yale/access/mfa/#multi-factor-authentication","text":"To improve security, access to Ruddle requires both a public key and multi-factor authentication (MFA). We use the same MFA (Duo) as is used elsewhere at Yale. To get set up with Duo, see these instructions. You will need upload your ssh public key to our site . For more info on how to use ssh, please see SSH instructions . Once you've set up Duo and your key is registered, you can finally log in. Use ssh to connect to your cluster of choice, and you will be prompted for a passcode or to select a notification option. We recommend choosing Duo Push (option 1). If you chose this option you should receive a notification on your phone. Once approved, you should be allowed to continue to log in. Note You can set up more than one phone for Duo. For example, you can set up your smartphone plus your office landline. That way, if you forget or lose your phone, you can still authenticate. For instructions on how to add additional phones go here .","title":"Multi-factor Authentication"},{"location":"clusters-at-yale/access/mfa/#file-transfer-and-duo-mfa","text":"Some file transfer clients attempt new and sometimes multiple concurrent connections to transfer files for you. When this happens, you will be asked to Duo authenticate for each connection. Setting up a config file lets you re-uses your authenticated sessions for command-line tools and tools that respect your ssh configuration. For CyberDuck, see our section on the Transfer Data page .","title":"File Transfer and Duo MFA"},{"location":"clusters-at-yale/access/mfa/#troubleshoot-mfa","text":"If you are having problems initially registering Duo, please contact the Yale ITS Helpdesk . If you have successfully used MFA connect to a cluster before, but cannot now, first please check the following: Test MFA using http://access.yale.edu Verify that your ssh client is using the correct login node Verify you are attempting to connect from a Yale machine or via the proper VPN If all of this is true, please contact us at hpc@yale.edu . Include the following information (and anything else you think is helpful): Your netid Have you ever successfully used ssh and Duo to connect to a cluster? How long have you been having problems? Where are you trying to connect from? (fully qualified hostname/IP, if possible) Are you using a VPN? What is the error message you see?","title":"Troubleshoot MFA"},{"location":"clusters-at-yale/access/ood/","text":"Open OnDemand Open OnDemand (OOD) is platform for accessing the clusters that only requires a web browser. This web-portal provides a shell, file browser, and graphical interface for certain apps (like Jupyter or MATLAB). Access If you access Open OnDemand installed on YCRC clusters from off campus, you will need to first connect to Yale's VPN . Open OnDemand is available on the following clusters using your NetID credentials (CAS login). Cluster OOD site Grace ood-grace.hpc.yale.edu Farnam ood-farnam.hpc.yale.edu Ruddle * ood-ruddle.hpc.yale.edu DUO Everywhere for OOD Ruddle If you want access to ood-ruddle, you must enroll in the early pilot of DUO Everywhere. Enrollment will force Multi-Factor Authentication (MFA) for CAS on every login, even on-campus. To enroll in DUO Everywhere, please email netal.patel@yale.edu indicating you want to use OnDemand on Ruddle. Include your your NetID and your desired deployment date. The deployment date should be any day from Monday to Thursday. The Dashboard On login you will then be greeted with a welcome page showing the standard message of the day. Along the top are a pull-down menus for a File Browser, a Job Builder, a list of Interactive Apps, and a Shell. File Browser The file browser is a graphical interface to manage, upload, and download files from the clusters. You can use the built-in file editor to view and edit files from your browser without having to download and upload scripts, etc. You can also drag and drop files, download entire directories, and move files between directories using this interface. Shell You can launch a traditional command-line interface to the cluster using the Shell pull-down menu. This opens a terminal in a web-browser that you can use in the exact same way as when logging into the cluster via SSH. This is a convenient way to access the clusters when you don't have access to an ssh client or do not have your ssh keys. Interactive Apps We have deployed a selection of common graphical programs as Interactive Apps on Open OneDemand. Currently, we have apps for Remote Desktop, MATLAB, Mathematica, RStudio Desktop, and Jupyter Notebook. Remote Desktop Occasionally, it is helpful to use a graphical interface to explore data or run certain programs. In the past your options were to use VNC or X11 forwarding . These tools can be complex to setup or suffer from reduced performance. The Remote Desktop app from OOD simplifies the configuration of a VNC desktop session on a compute node. The MATLAB, Mathematica, and RStudio Desktop Apps are special versions of this app. To get started choose Remote Desktop (or another desktop app) from the Interactive Apps menu on the dashboard. Use the form to request resources and decide what partition your job should run on (use interactive or your lab's partition). Once you launch the job, you will be presented with a notification that your job has been queued. Depending on the resources requested, you may need to wait for a bit. When the job starts you will see the option to launch the Remote Desktop: Note you can share a view only link for your session if you would like to share your screen. After you click on Launch Remote Desktop, a standard desktop interface will open in a new tab. You can find the terminal application (for loading modules and launching programs) in the \"Applications\" > \"System Tools\" menu. Copy/Paste Because of the way modern borowsers protect your computer's clipboard you have to use a special text box to copy and paste from the Remote Desktop App. Click the arrow on the left side of your window for a menu, then click the clipboard icon to get access to your Remote Desktop's clipboard. Jupyter Notebooks One of the most common uses of Open OnDemand is the Jupyter Notebook interface for Python and R. Jupyter Notebooks provide a flexible way to interactively work with code and plots presented in-line together. To get started choose Jupyter Notebook from the Interactive Apps menu on the dashboard. Make sure that you chose the right conda environment for your from the drop-down menu. If you have not yet set one up, follow our instructions on how to create a new one. After specifying the required resources (number of CPUs/GPUs, amount of RAM, etc.), you can submit the job. When it launches you can open the standard Jupyter interface where you can start working with notebooks. Tip If you have installed and want to use Jupyter Lab replace /tree? with /lab in the url to your Jupyter job.","title":"Open OnDemand"},{"location":"clusters-at-yale/access/ood/#open-ondemand","text":"Open OnDemand (OOD) is platform for accessing the clusters that only requires a web browser. This web-portal provides a shell, file browser, and graphical interface for certain apps (like Jupyter or MATLAB).","title":"Open OnDemand"},{"location":"clusters-at-yale/access/ood/#access","text":"If you access Open OnDemand installed on YCRC clusters from off campus, you will need to first connect to Yale's VPN . Open OnDemand is available on the following clusters using your NetID credentials (CAS login). Cluster OOD site Grace ood-grace.hpc.yale.edu Farnam ood-farnam.hpc.yale.edu Ruddle * ood-ruddle.hpc.yale.edu","title":"Access"},{"location":"clusters-at-yale/access/ood/#duo-everywhere-for-ood-ruddle","text":"If you want access to ood-ruddle, you must enroll in the early pilot of DUO Everywhere. Enrollment will force Multi-Factor Authentication (MFA) for CAS on every login, even on-campus. To enroll in DUO Everywhere, please email netal.patel@yale.edu indicating you want to use OnDemand on Ruddle. Include your your NetID and your desired deployment date. The deployment date should be any day from Monday to Thursday.","title":"DUO Everywhere for OOD Ruddle"},{"location":"clusters-at-yale/access/ood/#the-dashboard","text":"On login you will then be greeted with a welcome page showing the standard message of the day. Along the top are a pull-down menus for a File Browser, a Job Builder, a list of Interactive Apps, and a Shell.","title":"The Dashboard"},{"location":"clusters-at-yale/access/ood/#file-browser","text":"The file browser is a graphical interface to manage, upload, and download files from the clusters. You can use the built-in file editor to view and edit files from your browser without having to download and upload scripts, etc. You can also drag and drop files, download entire directories, and move files between directories using this interface.","title":"File Browser"},{"location":"clusters-at-yale/access/ood/#shell","text":"You can launch a traditional command-line interface to the cluster using the Shell pull-down menu. This opens a terminal in a web-browser that you can use in the exact same way as when logging into the cluster via SSH. This is a convenient way to access the clusters when you don't have access to an ssh client or do not have your ssh keys.","title":"Shell"},{"location":"clusters-at-yale/access/ood/#interactive-apps","text":"We have deployed a selection of common graphical programs as Interactive Apps on Open OneDemand. Currently, we have apps for Remote Desktop, MATLAB, Mathematica, RStudio Desktop, and Jupyter Notebook.","title":"Interactive Apps"},{"location":"clusters-at-yale/access/ood/#remote-desktop","text":"Occasionally, it is helpful to use a graphical interface to explore data or run certain programs. In the past your options were to use VNC or X11 forwarding . These tools can be complex to setup or suffer from reduced performance. The Remote Desktop app from OOD simplifies the configuration of a VNC desktop session on a compute node. The MATLAB, Mathematica, and RStudio Desktop Apps are special versions of this app. To get started choose Remote Desktop (or another desktop app) from the Interactive Apps menu on the dashboard. Use the form to request resources and decide what partition your job should run on (use interactive or your lab's partition). Once you launch the job, you will be presented with a notification that your job has been queued. Depending on the resources requested, you may need to wait for a bit. When the job starts you will see the option to launch the Remote Desktop: Note you can share a view only link for your session if you would like to share your screen. After you click on Launch Remote Desktop, a standard desktop interface will open in a new tab. You can find the terminal application (for loading modules and launching programs) in the \"Applications\" > \"System Tools\" menu.","title":"Remote Desktop"},{"location":"clusters-at-yale/access/ood/#copypaste","text":"Because of the way modern borowsers protect your computer's clipboard you have to use a special text box to copy and paste from the Remote Desktop App. Click the arrow on the left side of your window for a menu, then click the clipboard icon to get access to your Remote Desktop's clipboard.","title":"Copy/Paste"},{"location":"clusters-at-yale/access/ood/#jupyter-notebooks","text":"One of the most common uses of Open OnDemand is the Jupyter Notebook interface for Python and R. Jupyter Notebooks provide a flexible way to interactively work with code and plots presented in-line together. To get started choose Jupyter Notebook from the Interactive Apps menu on the dashboard. Make sure that you chose the right conda environment for your from the drop-down menu. If you have not yet set one up, follow our instructions on how to create a new one. After specifying the required resources (number of CPUs/GPUs, amount of RAM, etc.), you can submit the job. When it launches you can open the standard Jupyter interface where you can start working with notebooks. Tip If you have installed and want to use Jupyter Lab replace /tree? with /lab in the url to your Jupyter job.","title":"Jupyter Notebooks"},{"location":"clusters-at-yale/access/vnc/","text":"VNC As an alternative to X11 Forwarding, using VNC to access the cluster is another way to run graphically intensive applications. Open OnDemand On Grace and Farnam we have web dashboards set up that can run VNC for you as a job and forward your session back to you via your browser. Give them a try at ood-grace.hpc.yale.edu and ood-farnam.hpc.yale.edu under the \"interactive apps\" drop-down menu item. Setup vncserver on a Cluster Connect to the cluster with X11 forwarding enabled. If on Linux or Mac, ssh -Y netid@cluster , or if on Windows, follow our X11 forwarding guide . Start an interactive job on cluster with the --x11 flag (see Slurm for more information). For this description, we\u2019ll assume you were given node c04n03: srun --pty --x11 -p interactive bash On that node, run the VNCserver. You\u2019ll see something like: c04n03$ vncserver New 'c31n02.grace.hpc.yale.internal:1 (kln26)' desktop is c31n02.grace.hpc.yale.internal:1 Creating default startup script /home/fas/hpcprog/kln26/.vnc/xstartup Starting applications specified in /home/fas/hpcprog/kln26/.vnc/xstartup Log file is /home/fas/hpcprog/kln26/.vnc/c31n02.grace.hpc.yale.internal:1.log The :1 means that your DISPLAY is :1. You\u2019ll need that later, so note it. The first time you run \"vncserver\", you\u2019ll also be asked to select a password for allowing access. On MacOS, if connecting with TurboVNC throws a security exception such as \"javax.net.ssl.SSLHandshakeException\", try adding the SecurityTypes option when starting vncserver on the cluster: vncserver -SecurityTypes VNC,OTP,UnixLogin,None Connect from your local machine (laptop/desktop) macOs/Linux From a shell on your local machine, run the following ssh command: ssh -Y -L7777:c04n03:5901 YourNetID@cluster_login_node This will set up a tunnel from your local port 7777 to port 5901 on c04n03. You will need to customize this command to your situation. The 5901 is for display :1. In general, you should put 5900+DISPLAY. The 7777 is arbitrary; any number above 3000 will likely work. You\u2019ll need the number you chose for the next step. On your local machine, start the vncviewer application. Depending on your local operating system, you may need to install this. We recommend TurboVNC for Mac. When you start the viewer, you\u2019ll need to tell it which host and port to attach to. You want to specify the local end of the tunnel. In the above case, that would be localhost::7777. Exactly how you specify this will depend a bit on which viewer you use. E.g: vncviewer localhost::7777 You should be prompted for the password you set when you started the server. Now you are in a GUI environment and can run IGV or any other rich GUI application. /home/bioinfo/software/IGV/IGV_2.2.0/igv.sh Windows In MobaXterm, create a new Session (available in the menu bar) and then select the VNC session. To fill out the VNC Session setup, click the \"Network settings\" tab and check the box for \"Connect through SSH gateway (jump host). Then fill out the boxes as follows: Remote hostname or IP Address: name of the node running your VNC server (e.g. c01n01) Port: 5900 + the DISPLAY number from above (e.g. 5901 for DISPLAY = 1 ) Gateway SSH server: ssh address of the cluster (e.g. grace.hpc.yale.edu) Port: 22 (should be default) User: netid Use private key: check this box and click to point to your private key file you use to connect to the cluster When you are done, click OK. If promoted for a password for \"localhost\", provide the vncserver password you specified in the previous step. If the VNC server looks very pixelated and your mouse movements seem laggy, try clicking the \"Toggle scaling\" button at the top of the VNC window. Example Configuration: Clean Up When you are all finished, you can kill the vncserver by doing this in the same shell you used to start it (replace :1 by your display number): vncserver -kill :1","title":"VNC"},{"location":"clusters-at-yale/access/vnc/#vnc","text":"As an alternative to X11 Forwarding, using VNC to access the cluster is another way to run graphically intensive applications.","title":"VNC"},{"location":"clusters-at-yale/access/vnc/#open-ondemand","text":"On Grace and Farnam we have web dashboards set up that can run VNC for you as a job and forward your session back to you via your browser. Give them a try at ood-grace.hpc.yale.edu and ood-farnam.hpc.yale.edu under the \"interactive apps\" drop-down menu item.","title":"Open OnDemand"},{"location":"clusters-at-yale/access/vnc/#setup-vncserver-on-a-cluster","text":"Connect to the cluster with X11 forwarding enabled. If on Linux or Mac, ssh -Y netid@cluster , or if on Windows, follow our X11 forwarding guide . Start an interactive job on cluster with the --x11 flag (see Slurm for more information). For this description, we\u2019ll assume you were given node c04n03: srun --pty --x11 -p interactive bash On that node, run the VNCserver. You\u2019ll see something like: c04n03$ vncserver New 'c31n02.grace.hpc.yale.internal:1 (kln26)' desktop is c31n02.grace.hpc.yale.internal:1 Creating default startup script /home/fas/hpcprog/kln26/.vnc/xstartup Starting applications specified in /home/fas/hpcprog/kln26/.vnc/xstartup Log file is /home/fas/hpcprog/kln26/.vnc/c31n02.grace.hpc.yale.internal:1.log The :1 means that your DISPLAY is :1. You\u2019ll need that later, so note it. The first time you run \"vncserver\", you\u2019ll also be asked to select a password for allowing access. On MacOS, if connecting with TurboVNC throws a security exception such as \"javax.net.ssl.SSLHandshakeException\", try adding the SecurityTypes option when starting vncserver on the cluster: vncserver -SecurityTypes VNC,OTP,UnixLogin,None","title":"Setup vncserver on a Cluster"},{"location":"clusters-at-yale/access/vnc/#connect-from-your-local-machine-laptopdesktop","text":"","title":"Connect from your local machine (laptop/desktop)"},{"location":"clusters-at-yale/access/vnc/#macoslinux","text":"From a shell on your local machine, run the following ssh command: ssh -Y -L7777:c04n03:5901 YourNetID@cluster_login_node This will set up a tunnel from your local port 7777 to port 5901 on c04n03. You will need to customize this command to your situation. The 5901 is for display :1. In general, you should put 5900+DISPLAY. The 7777 is arbitrary; any number above 3000 will likely work. You\u2019ll need the number you chose for the next step. On your local machine, start the vncviewer application. Depending on your local operating system, you may need to install this. We recommend TurboVNC for Mac. When you start the viewer, you\u2019ll need to tell it which host and port to attach to. You want to specify the local end of the tunnel. In the above case, that would be localhost::7777. Exactly how you specify this will depend a bit on which viewer you use. E.g: vncviewer localhost::7777 You should be prompted for the password you set when you started the server. Now you are in a GUI environment and can run IGV or any other rich GUI application. /home/bioinfo/software/IGV/IGV_2.2.0/igv.sh","title":"macOs/Linux"},{"location":"clusters-at-yale/access/vnc/#windows","text":"In MobaXterm, create a new Session (available in the menu bar) and then select the VNC session. To fill out the VNC Session setup, click the \"Network settings\" tab and check the box for \"Connect through SSH gateway (jump host). Then fill out the boxes as follows: Remote hostname or IP Address: name of the node running your VNC server (e.g. c01n01) Port: 5900 + the DISPLAY number from above (e.g. 5901 for DISPLAY = 1 ) Gateway SSH server: ssh address of the cluster (e.g. grace.hpc.yale.edu) Port: 22 (should be default) User: netid Use private key: check this box and click to point to your private key file you use to connect to the cluster When you are done, click OK. If promoted for a password for \"localhost\", provide the vncserver password you specified in the previous step. If the VNC server looks very pixelated and your mouse movements seem laggy, try clicking the \"Toggle scaling\" button at the top of the VNC window. Example Configuration:","title":"Windows"},{"location":"clusters-at-yale/access/vnc/#clean-up","text":"When you are all finished, you can kill the vncserver by doing this in the same shell you used to start it (replace :1 by your display number): vncserver -kill :1","title":"Clean Up"},{"location":"clusters-at-yale/access/vpn/","text":"Access from Off Campus (VPN) Yale's clusters can only be accessed on the Yale network. Therefore, in order to access a cluster from off campus, you will need to first connect to Yale's VPN. More information about Yale's VPN can be found on the ITS website . VPN Software Windows and macOS We recommend the Cisco AnyConnect VPN Client, which can be downloaded from the ITS Software Library . Linux On Linux, you can use openconnect to connect to one of Yale's VPNs. If you are using the standard Gnome-based distros, use the commands below to install. Ubuntu/Debian sudo apt install network-manager-openconnect-gnome Fedora/CentOS sudo yum install NetworkManager-openconnect Connect via VPN You will need to connect via the VPN client using the profile \"access.yale.edu\". Multi-factor Authentication (MFA) Authentication for the VPN requires multi-factor authentication via Duo in addition to your normal Yale credentials (netid and password). After you select \"Connect\" in the above dialog box, you will be presented with a new prompt to enter your netid, password and an MFA method. Depending on what you choose you will be prompted to authenticate via a second authentication method. If you type \"push\", simply tap \"Approve\" on your mobile device. If you type \"sms\" you will receive a text message with your passcode. Enter the passcode you received to authenticate. If you type \"phone\", follow the prompts when you are called. Once you successfully authenticate with MFA, you will be connected to the VPN and should be able to log in the clusters via SSH as usual. More information about MFA at Yale can be found on the ITS website .","title":"Access from Off Campus (VPN)"},{"location":"clusters-at-yale/access/vpn/#access-from-off-campus-vpn","text":"Yale's clusters can only be accessed on the Yale network. Therefore, in order to access a cluster from off campus, you will need to first connect to Yale's VPN. More information about Yale's VPN can be found on the ITS website .","title":"Access from Off Campus (VPN)"},{"location":"clusters-at-yale/access/vpn/#vpn-software","text":"","title":"VPN Software"},{"location":"clusters-at-yale/access/vpn/#windows-and-macos","text":"We recommend the Cisco AnyConnect VPN Client, which can be downloaded from the ITS Software Library .","title":"Windows and macOS"},{"location":"clusters-at-yale/access/vpn/#linux","text":"On Linux, you can use openconnect to connect to one of Yale's VPNs. If you are using the standard Gnome-based distros, use the commands below to install. Ubuntu/Debian sudo apt install network-manager-openconnect-gnome Fedora/CentOS sudo yum install NetworkManager-openconnect","title":"Linux"},{"location":"clusters-at-yale/access/vpn/#connect-via-vpn","text":"You will need to connect via the VPN client using the profile \"access.yale.edu\".","title":"Connect via VPN"},{"location":"clusters-at-yale/access/vpn/#multi-factor-authentication-mfa","text":"Authentication for the VPN requires multi-factor authentication via Duo in addition to your normal Yale credentials (netid and password). After you select \"Connect\" in the above dialog box, you will be presented with a new prompt to enter your netid, password and an MFA method. Depending on what you choose you will be prompted to authenticate via a second authentication method. If you type \"push\", simply tap \"Approve\" on your mobile device. If you type \"sms\" you will receive a text message with your passcode. Enter the passcode you received to authenticate. If you type \"phone\", follow the prompts when you are called. Once you successfully authenticate with MFA, you will be connected to the VPN and should be able to log in the clusters via SSH as usual. More information about MFA at Yale can be found on the ITS website .","title":"Multi-factor Authentication (MFA)"},{"location":"clusters-at-yale/access/x11/","text":"Graphical Interfaces (X11) To use a graphical interface on the clusters, your connection needs to be set up for X11 forwarding, which will transmit the graphical window from the cluster back to your local machine. A simple test to see if your setup is working is to run the command xclock . You should see a simple analog clock window pop up. On macOS Download and Install X-Quartz v2.7.8 (newer versions will not work with our clusters). Log out and log back in to your Mac to reset some variables When using ssh to log in to the clusters, use the -Y option to enable X11 forwarding. Example: ssh -Y rdb9@ruddle.hpc.yale.edu On Windows We recommend MobaXterm for connecting to the clusters from Windows. It is configured for X11 forwarding out of the box and should require no additional configuration or software. Quick Test A quick and simple test to check if X11 forwarding is working is to run the command xclock in the session you expect to be forwarding. After a short delay, you should see a window with a simple clock pop up. Submit an X11 enabled Job Once configured, you'll usually want to use X11 forwarding on a compute node to do your work. To allocate a simple interactive session with X11 forwarding: srun --x11 --pty -p interactive bash For more Slurm options, see our Slurm documentation .","title":"Graphical Interfaces (X11)"},{"location":"clusters-at-yale/access/x11/#graphical-interfaces-x11","text":"To use a graphical interface on the clusters, your connection needs to be set up for X11 forwarding, which will transmit the graphical window from the cluster back to your local machine. A simple test to see if your setup is working is to run the command xclock . You should see a simple analog clock window pop up.","title":"Graphical Interfaces (X11)"},{"location":"clusters-at-yale/access/x11/#on-macos","text":"Download and Install X-Quartz v2.7.8 (newer versions will not work with our clusters). Log out and log back in to your Mac to reset some variables When using ssh to log in to the clusters, use the -Y option to enable X11 forwarding. Example: ssh -Y rdb9@ruddle.hpc.yale.edu","title":"On macOS"},{"location":"clusters-at-yale/access/x11/#on-windows","text":"We recommend MobaXterm for connecting to the clusters from Windows. It is configured for X11 forwarding out of the box and should require no additional configuration or software.","title":"On Windows"},{"location":"clusters-at-yale/access/x11/#quick-test","text":"A quick and simple test to check if X11 forwarding is working is to run the command xclock in the session you expect to be forwarding. After a short delay, you should see a window with a simple clock pop up.","title":"Quick Test"},{"location":"clusters-at-yale/access/x11/#submit-an-x11-enabled-job","text":"Once configured, you'll usually want to use X11 forwarding on a compute node to do your work. To allocate a simple interactive session with X11 forwarding: srun --x11 --pty -p interactive bash For more Slurm options, see our Slurm documentation .","title":"Submit an X11 enabled Job"},{"location":"clusters-at-yale/applications/","text":"Overview The YCRC will install and manage commonly used software. These software are available as modules, which allow you to add or remove different combinations and versions of software to your environment as needed. See our module guide for more info. You can run module avail to page through all available software once you log in. You should also feel free to install things for yourself. For Python environments, we recommend using a virtual environment in Anaconda Python . For R enviroments, we recommend using install.packages() to manage your own package needs. For all other software, we encourage users to attempt to install their own software into their directories. Here are instructions for common software procedures. Make Cmake Singularity : create containers and port Docker containers to the clusters We provide guides for certain software packages and languages as well. If you run into issues with your software installations, the YCRC staff can assist at hpc@yale.edu.","title":"Overview"},{"location":"clusters-at-yale/applications/#overview","text":"The YCRC will install and manage commonly used software. These software are available as modules, which allow you to add or remove different combinations and versions of software to your environment as needed. See our module guide for more info. You can run module avail to page through all available software once you log in. You should also feel free to install things for yourself. For Python environments, we recommend using a virtual environment in Anaconda Python . For R enviroments, we recommend using install.packages() to manage your own package needs. For all other software, we encourage users to attempt to install their own software into their directories. Here are instructions for common software procedures. Make Cmake Singularity : create containers and port Docker containers to the clusters We provide guides for certain software packages and languages as well. If you run into issues with your software installations, the YCRC staff can assist at hpc@yale.edu.","title":"Overview"},{"location":"clusters-at-yale/applications/compile/","text":"Build Software How to get software you need up and running on the clusters. caveat emptor We recommend either use existing software modules , Anaconda , Singularity , or pre-compiled software where available. However, there are cases where compiling applications is necessary or desired. This can be because the pre-compiled version isn't readily available/compatible or because compiling applications on the cluster will make an appreciable difference in performance. It is also the case that many R packages are compiled at install time. When compiling applications on the clusters, it is important to consider the ways in which you expect to run the application you are endeavoring to get working. If you want to be able to run jobs calling your application any node on the cluster, you will need to target the oldest hardware available so that newer optimizations are not used that will fail on some nodes. If your application is already quite specialized (e.g. needs GPUs or brand-new CPU instructions), you will want to compile it natively for the subset of compute nodes on which your jobs will run. This decision is often a trade-off between faster individual jobs or jobs that can run on more nodes at once. Each of the cluster pages (see the clusters index for a list) has a \"Compute Node Configurations\" section where nodes are roughly listed from oldest to newest. Illegal Instruction Instructions You may find that software compiled on newer compute nodes will fail with the error Illegal instruction (core dumped) . This includes R/Python libraries that include code that compiles from source. To remedy this issue make sure to always either: Build or install software on the oldest available nodes. You can ensure you are on the oldest hardware by specifying the oldest feature ( --constraint oldest ) in your job submission. Require that your jobs running the software in question request similar hardware to their build environment. If your software needs newer instructions using avx512 as a constraint will probably work, but limit the pool of nodes your job can run on. Either way, you will want to control where your jobs run with job constraints . Conventions Local Install Because you don't have admin/root/sudo privileges on the clusters, you won't be able to use sudo and a package manager like apt , yum , etc.; You will need to adapt install instructions to allow for what is called a local or user install. If you prefer or require this method, you should create a singularity container image (see our Singularity guide ), then run it on the cluster. For things to work smoothly you will need to choose and stick with a prefix, or path to your installed applications and libraries. We recommend this be either in your home or project directory, something like ~/software or /path/to/project/software . Make sure you have created it before continuing. Tip If you choose a project directory prefix, it will be easier to share your applications with lab mates or other cluster users. Just make sure to use the true path (the one returned by mydirectories ). Once you've chosen a prefix you will want to add any directory with executables you want to run to your PATH environment variable, and any directores with libraries that your application(s) link to your LD_LIBRARY_PATH environment variable. Each of these tell your shell where to look when you call your application without specifying an absolute path to it. To set these variables permanently, add the following to the end of your ~/.bashrc file: # local installs export MY_PREFIX = ~/software export PATH = $MY_PREFIX /bin: $PATH export LD_LIBRARY_PATH = $MY_PREFIX /lib: $LD_LIBRARY_PATH For the remainder of the guide we'll use the $MY_PREFIX variable to refer to the prefix. See below or your application's install instructions for exactly how to specify your prefix at build/install time. Dependencies You will need to develop a build strategy that works for you and stay consistent. If you're happy using libraries and toolchains that are already available on the cluster as dependencies (recommended), feel free to create module collections that serve as your environments. If you prefer to completely build your own software tree, that is ok too. Whichever route you choose, try to stick with the same version of dependencies (e.g. MPI, zlib, numpy) and compiler you're using (e.g. GCC, intel). We find that unless absolutely necessary, the newest version of a compiler or library might not be the most compatible with a wide array of scientific software so you may want to step back a few versions or try using what was available at the time your application was being developed. Autotools ( configure / make ) If your application includes instructions to run ./bootstrap , ./autogen.sh , ./configure or make , it is using the GNU Build System . configure If you are instructed to run ./configure to generate a Makefile, specify your prefix with the --prefix option. This creates a file, usually named Makefile that is a recipe for make to use to build your application. export MY_PREFIX = ~/software ./configure --prefix = $MY_PREFIX make install If your configure ran properly, make install should properly place your application in your prefix directory. If there is no install target specified for your application, you can either run make and copy the application to your $MY_PREFIX/bin directory or build it somewhere in $MY_PREFIX and add its relevant paths to your PATH and/or LD_LIBRARY_PATH environment variables in your ~/.bashrc file as shown in the local install section. CMake CMake is a popular cross-platform build system. On a linux system, CMake will create a Makefile in a step analogous to ./configure . It is common to create a build directory then run the cmake and make commands from there. Below is what installing to your $MY_DIRECTORY prefix might look like with CMake. CMake instructions also tend to link together the build process onto on line with && , which tells your shell to only continue to the next command if the previous one exited without error. export MY_PREFIX = ~/software mkdir build && cd build && cmake -DCMAKE_INSTALL_PREFIX = $MY_PREFIX .. && make && make install","title":"Build Software"},{"location":"clusters-at-yale/applications/compile/#build-software","text":"How to get software you need up and running on the clusters.","title":"Build Software"},{"location":"clusters-at-yale/applications/compile/#caveat-emptor","text":"We recommend either use existing software modules , Anaconda , Singularity , or pre-compiled software where available. However, there are cases where compiling applications is necessary or desired. This can be because the pre-compiled version isn't readily available/compatible or because compiling applications on the cluster will make an appreciable difference in performance. It is also the case that many R packages are compiled at install time. When compiling applications on the clusters, it is important to consider the ways in which you expect to run the application you are endeavoring to get working. If you want to be able to run jobs calling your application any node on the cluster, you will need to target the oldest hardware available so that newer optimizations are not used that will fail on some nodes. If your application is already quite specialized (e.g. needs GPUs or brand-new CPU instructions), you will want to compile it natively for the subset of compute nodes on which your jobs will run. This decision is often a trade-off between faster individual jobs or jobs that can run on more nodes at once. Each of the cluster pages (see the clusters index for a list) has a \"Compute Node Configurations\" section where nodes are roughly listed from oldest to newest.","title":"caveat emptor"},{"location":"clusters-at-yale/applications/compile/#illegal-instruction-instructions","text":"You may find that software compiled on newer compute nodes will fail with the error Illegal instruction (core dumped) . This includes R/Python libraries that include code that compiles from source. To remedy this issue make sure to always either: Build or install software on the oldest available nodes. You can ensure you are on the oldest hardware by specifying the oldest feature ( --constraint oldest ) in your job submission. Require that your jobs running the software in question request similar hardware to their build environment. If your software needs newer instructions using avx512 as a constraint will probably work, but limit the pool of nodes your job can run on. Either way, you will want to control where your jobs run with job constraints .","title":"Illegal Instruction Instructions"},{"location":"clusters-at-yale/applications/compile/#conventions","text":"","title":"Conventions"},{"location":"clusters-at-yale/applications/compile/#local-install","text":"Because you don't have admin/root/sudo privileges on the clusters, you won't be able to use sudo and a package manager like apt , yum , etc.; You will need to adapt install instructions to allow for what is called a local or user install. If you prefer or require this method, you should create a singularity container image (see our Singularity guide ), then run it on the cluster. For things to work smoothly you will need to choose and stick with a prefix, or path to your installed applications and libraries. We recommend this be either in your home or project directory, something like ~/software or /path/to/project/software . Make sure you have created it before continuing. Tip If you choose a project directory prefix, it will be easier to share your applications with lab mates or other cluster users. Just make sure to use the true path (the one returned by mydirectories ). Once you've chosen a prefix you will want to add any directory with executables you want to run to your PATH environment variable, and any directores with libraries that your application(s) link to your LD_LIBRARY_PATH environment variable. Each of these tell your shell where to look when you call your application without specifying an absolute path to it. To set these variables permanently, add the following to the end of your ~/.bashrc file: # local installs export MY_PREFIX = ~/software export PATH = $MY_PREFIX /bin: $PATH export LD_LIBRARY_PATH = $MY_PREFIX /lib: $LD_LIBRARY_PATH For the remainder of the guide we'll use the $MY_PREFIX variable to refer to the prefix. See below or your application's install instructions for exactly how to specify your prefix at build/install time.","title":"Local Install"},{"location":"clusters-at-yale/applications/compile/#dependencies","text":"You will need to develop a build strategy that works for you and stay consistent. If you're happy using libraries and toolchains that are already available on the cluster as dependencies (recommended), feel free to create module collections that serve as your environments. If you prefer to completely build your own software tree, that is ok too. Whichever route you choose, try to stick with the same version of dependencies (e.g. MPI, zlib, numpy) and compiler you're using (e.g. GCC, intel). We find that unless absolutely necessary, the newest version of a compiler or library might not be the most compatible with a wide array of scientific software so you may want to step back a few versions or try using what was available at the time your application was being developed.","title":"Dependencies"},{"location":"clusters-at-yale/applications/compile/#autotools-configuremake","text":"If your application includes instructions to run ./bootstrap , ./autogen.sh , ./configure or make , it is using the GNU Build System .","title":"Autotools (configure/make)"},{"location":"clusters-at-yale/applications/compile/#configure","text":"If you are instructed to run ./configure to generate a Makefile, specify your prefix with the --prefix option. This creates a file, usually named Makefile that is a recipe for make to use to build your application. export MY_PREFIX = ~/software ./configure --prefix = $MY_PREFIX","title":"configure"},{"location":"clusters-at-yale/applications/compile/#make-install","text":"If your configure ran properly, make install should properly place your application in your prefix directory. If there is no install target specified for your application, you can either run make and copy the application to your $MY_PREFIX/bin directory or build it somewhere in $MY_PREFIX and add its relevant paths to your PATH and/or LD_LIBRARY_PATH environment variables in your ~/.bashrc file as shown in the local install section.","title":"make install"},{"location":"clusters-at-yale/applications/compile/#cmake","text":"CMake is a popular cross-platform build system. On a linux system, CMake will create a Makefile in a step analogous to ./configure . It is common to create a build directory then run the cmake and make commands from there. Below is what installing to your $MY_DIRECTORY prefix might look like with CMake. CMake instructions also tend to link together the build process onto on line with && , which tells your shell to only continue to the next command if the previous one exited without error. export MY_PREFIX = ~/software mkdir build && cd build && cmake -DCMAKE_INSTALL_PREFIX = $MY_PREFIX .. && make && make install","title":"CMake"},{"location":"clusters-at-yale/applications/easybuild/","text":"Toolchains and EasyBuild We use a build and installation framework called EasyBuild that connects the software we compile and maintain on the clusters with the module system that makes it available to you. Toolchains When we install software, we use pre-defined build environments called toolchains. These are modules that include core compilers and libraries (e.g. GCC , OpenMPI , zlib ). We do this for two main reasons. One is to try to keep our build process simpler. The other is so that you can load two different modules for software built with the same toolchain and expect everything to work (see below for details on Toolchain Trees). The two common toolchains you will interact with are foss and intel . Each of these have module versions corresponding to the year they were built. Toolchain name and version information is appended to the name of a module so it is clear what will be compatible. For example Python/2.7.12-foss-2016b , the software name is Python (version 2.7.12 ) that is built with the foss toolchain version 2016b . The easiest way to see what software a toolchain includes is to load it and then list loaded modules. [ be59@farnam2 ~ ] $ module load foss/2016b [ be59@farnam2 ~ ] $ module list Currently Loaded Modules: 1 ) StdEnv ( S ) 7 ) OpenMPI/1.10.3-GCC-5.4.0-2.26 2 ) GCCcore/5.4.0 8 ) OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 3 ) binutils/2.26-GCCcore-5.4.0 9 ) gompi/2016b 4 ) GCC/5.4.0-2.26 10 ) FFTW/3.3.4-gompi-2016b 5 ) numactl/2.0.11-GCC-5.4.0-2.26 11 ) ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1 6 ) hwloc/1.11.3-GCC-5.4.0-2.26 12 ) foss/2016b Where: S: Module is Sticky, requires --force to unload or purge To summarize, you should try to use modules that use matching foss , iomkl or intel identifiers. Toolchain Trees The one place toolchains can be a little complicated is that there are various levels to a given toolchain depending on how many libraries are included. For example, foss/2018a is a parent toolchain to GCCcore/6.4.0 since foss includes GCC 6.4.0 as well as OpenMPI 2.1.2. Base toolchains: GCCcore - GCC compiler iccifort - Intel compiler (GCCcore is actually a also sub-toolchain of iccifort) Toolchains with MPI: gompi - GCCcore + OpenMPI iimpi - iccifort + Intel MPI iompi - iccifort + OpenMPI Full toolchains (also include math libraries): foss - gompi + OpenBLAS + ScalaPack intel - iimpi + Intel MKL iomkl - iompi + Intel MKL For installations that where you will be loading additional dependencies beyond the toolchain module: 1. Choose a full toolchain and version. 1. Look for modules that contain that toolchain or its sub-toolchains to ensure compatibility. You can run module display foss/2016b to see which versions of the compiler and libraries it includes. Environment Variables To refer to the directory where the software from a module is stored, you can use the environment variable $EBROOTMODULENAME where modulename is the name of the module in all caps with no spaces. This can be useful for finding the executables, libraries, or readme files that are included with the software: [ be59@farnam2 ~ ] $ module load SAMtools/1.9-foss-2016b [ be59@farnam2 ~ ] $ ls $EBROOTSAMTOOLS bin easybuild include lib share [ be59@farnam2 ~ ] $ ls $EBROOTSAMTOOLS /bin ace2sam interpolate_sam.pl md5sum-lite r2plot.lua seq_cache_populate.pl wgsim blast2sam.pl maq2sam-long novo2sam.pl sam2vcf.pl soap2sam.pl wgsim_eval.pl bowtie2sam.pl maq2sam-short plot-bamstats samtools varfilter.py zoom2sam.pl export2sam.pl md5fa psl2sam.pl samtools.pl vcfutils.lua","title":"Toolchains and EasyBuild"},{"location":"clusters-at-yale/applications/easybuild/#toolchains-and-easybuild","text":"We use a build and installation framework called EasyBuild that connects the software we compile and maintain on the clusters with the module system that makes it available to you.","title":"Toolchains and EasyBuild"},{"location":"clusters-at-yale/applications/easybuild/#toolchains","text":"When we install software, we use pre-defined build environments called toolchains. These are modules that include core compilers and libraries (e.g. GCC , OpenMPI , zlib ). We do this for two main reasons. One is to try to keep our build process simpler. The other is so that you can load two different modules for software built with the same toolchain and expect everything to work (see below for details on Toolchain Trees). The two common toolchains you will interact with are foss and intel . Each of these have module versions corresponding to the year they were built. Toolchain name and version information is appended to the name of a module so it is clear what will be compatible. For example Python/2.7.12-foss-2016b , the software name is Python (version 2.7.12 ) that is built with the foss toolchain version 2016b . The easiest way to see what software a toolchain includes is to load it and then list loaded modules. [ be59@farnam2 ~ ] $ module load foss/2016b [ be59@farnam2 ~ ] $ module list Currently Loaded Modules: 1 ) StdEnv ( S ) 7 ) OpenMPI/1.10.3-GCC-5.4.0-2.26 2 ) GCCcore/5.4.0 8 ) OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 3 ) binutils/2.26-GCCcore-5.4.0 9 ) gompi/2016b 4 ) GCC/5.4.0-2.26 10 ) FFTW/3.3.4-gompi-2016b 5 ) numactl/2.0.11-GCC-5.4.0-2.26 11 ) ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1 6 ) hwloc/1.11.3-GCC-5.4.0-2.26 12 ) foss/2016b Where: S: Module is Sticky, requires --force to unload or purge To summarize, you should try to use modules that use matching foss , iomkl or intel identifiers.","title":"Toolchains"},{"location":"clusters-at-yale/applications/easybuild/#toolchain-trees","text":"The one place toolchains can be a little complicated is that there are various levels to a given toolchain depending on how many libraries are included. For example, foss/2018a is a parent toolchain to GCCcore/6.4.0 since foss includes GCC 6.4.0 as well as OpenMPI 2.1.2.","title":"Toolchain Trees"},{"location":"clusters-at-yale/applications/easybuild/#base-toolchains","text":"GCCcore - GCC compiler iccifort - Intel compiler (GCCcore is actually a also sub-toolchain of iccifort)","title":"Base toolchains:"},{"location":"clusters-at-yale/applications/easybuild/#toolchains-with-mpi","text":"gompi - GCCcore + OpenMPI iimpi - iccifort + Intel MPI iompi - iccifort + OpenMPI","title":"Toolchains with MPI:"},{"location":"clusters-at-yale/applications/easybuild/#full-toolchains-also-include-math-libraries","text":"foss - gompi + OpenBLAS + ScalaPack intel - iimpi + Intel MKL iomkl - iompi + Intel MKL For installations that where you will be loading additional dependencies beyond the toolchain module: 1. Choose a full toolchain and version. 1. Look for modules that contain that toolchain or its sub-toolchains to ensure compatibility. You can run module display foss/2016b to see which versions of the compiler and libraries it includes.","title":"Full toolchains (also include math libraries):"},{"location":"clusters-at-yale/applications/easybuild/#environment-variables","text":"To refer to the directory where the software from a module is stored, you can use the environment variable $EBROOTMODULENAME where modulename is the name of the module in all caps with no spaces. This can be useful for finding the executables, libraries, or readme files that are included with the software: [ be59@farnam2 ~ ] $ module load SAMtools/1.9-foss-2016b [ be59@farnam2 ~ ] $ ls $EBROOTSAMTOOLS bin easybuild include lib share [ be59@farnam2 ~ ] $ ls $EBROOTSAMTOOLS /bin ace2sam interpolate_sam.pl md5sum-lite r2plot.lua seq_cache_populate.pl wgsim blast2sam.pl maq2sam-long novo2sam.pl sam2vcf.pl soap2sam.pl wgsim_eval.pl bowtie2sam.pl maq2sam-short plot-bamstats samtools varfilter.py zoom2sam.pl export2sam.pl md5fa psl2sam.pl samtools.pl vcfutils.lua","title":"Environment Variables"},{"location":"clusters-at-yale/applications/modules/","text":"Load Software with Modules There are many software packages installed on the Yale clusters in addition to those installed in the standard system directories. This software has been specifically installed for use on our clusters at the request of our users. In order to manage these additional packages, the clusters use \"module files\". These module files allow you to easily specify which versions of which packages you want use. List All Loaded Modules The module list command displays all of the module files that are currently loaded in your environment: module list You may notice a module named StdEnv loaded by default. This module sets up your cluster environment; purging this module will make the modules we have installed unavailable. Find Available Modules To list all available module files, execute: module avail You can also list all module files whose name contains a specified string. For example, to find all Python module files, use: module avail python Tip You can get a brief description of a module and the url to the software's homepage by running: module help <modulename> If you don't find a commonly used software you require in the list of modules feel free to send us a software installation request to hpc@yale.edu. Otherwise, check out our installation guides to install it for yourself. Load and Unload Modules The module load command is used to modify your environment so you can use a specified software package. For example, if you found and want to load Python version 2.7.13 , execute the command: module load Python/2.7.13-foss-2016b This modifies the PATH environment variable (as well as a few others) so that your default Python interpreter is version 2.7.13 : [ be59@farnam2 ~ ] $ python --version Python 2 .7.13 You don't have to specify the version. For example, if you want the default module version of R, use: module load R You can also unload a module that you've previously loaded: module unload R Warning Mixing and matching certain software can be tricky due to the way we build our software and modules. In short, make sure that the foss or intel in your module names match if they are present. For more information, see our EasyBuild page . Module Collections It can be a pain to have to enter a long list of module load commands every time you log on to the cluster. Module collections allow you to create saved environments that remember a set list of modules to load together. This method is particularly useful if you have two or more module sets that may conflict with one another. To create a saved environment, simply load all of your desired modules and then type module save This will save this set of modules as your default set. To save a set as a non-default, just assign the environment a name: module save environment_name To load all the modules in the set, enter module restore module restore environment_name to load the default or specified environment, respectively. To modify an environment, restore the environment, make the desired changes by loading and/or unloading modules and save it to the same name. To get a list of your environments, run: module savelist More Information You can view documentation while on the cluster using the command: man module There is even more information at the offical lmod website .","title":"Load Software with Modules"},{"location":"clusters-at-yale/applications/modules/#load-software-with-modules","text":"There are many software packages installed on the Yale clusters in addition to those installed in the standard system directories. This software has been specifically installed for use on our clusters at the request of our users. In order to manage these additional packages, the clusters use \"module files\". These module files allow you to easily specify which versions of which packages you want use.","title":"Load Software with Modules"},{"location":"clusters-at-yale/applications/modules/#list-all-loaded-modules","text":"The module list command displays all of the module files that are currently loaded in your environment: module list You may notice a module named StdEnv loaded by default. This module sets up your cluster environment; purging this module will make the modules we have installed unavailable.","title":"List All Loaded Modules"},{"location":"clusters-at-yale/applications/modules/#find-available-modules","text":"To list all available module files, execute: module avail You can also list all module files whose name contains a specified string. For example, to find all Python module files, use: module avail python Tip You can get a brief description of a module and the url to the software's homepage by running: module help <modulename> If you don't find a commonly used software you require in the list of modules feel free to send us a software installation request to hpc@yale.edu. Otherwise, check out our installation guides to install it for yourself.","title":"Find Available Modules"},{"location":"clusters-at-yale/applications/modules/#load-and-unload-modules","text":"The module load command is used to modify your environment so you can use a specified software package. For example, if you found and want to load Python version 2.7.13 , execute the command: module load Python/2.7.13-foss-2016b This modifies the PATH environment variable (as well as a few others) so that your default Python interpreter is version 2.7.13 : [ be59@farnam2 ~ ] $ python --version Python 2 .7.13 You don't have to specify the version. For example, if you want the default module version of R, use: module load R You can also unload a module that you've previously loaded: module unload R Warning Mixing and matching certain software can be tricky due to the way we build our software and modules. In short, make sure that the foss or intel in your module names match if they are present. For more information, see our EasyBuild page .","title":"Load and Unload Modules"},{"location":"clusters-at-yale/applications/modules/#module-collections","text":"It can be a pain to have to enter a long list of module load commands every time you log on to the cluster. Module collections allow you to create saved environments that remember a set list of modules to load together. This method is particularly useful if you have two or more module sets that may conflict with one another. To create a saved environment, simply load all of your desired modules and then type module save This will save this set of modules as your default set. To save a set as a non-default, just assign the environment a name: module save environment_name To load all the modules in the set, enter module restore module restore environment_name to load the default or specified environment, respectively. To modify an environment, restore the environment, make the desired changes by loading and/or unloading modules and save it to the same name. To get a list of your environments, run: module savelist","title":"Module Collections"},{"location":"clusters-at-yale/applications/modules/#more-information","text":"You can view documentation while on the cluster using the command: man module There is even more information at the offical lmod website .","title":"More Information"},{"location":"clusters-at-yale/applications/new-modules-grace/","text":"New Module System During the August 2019 Grace scheduled maintenance, we switched the default module collection. This upgrade makes the cluster more consistent with our other clusters' software environments and allows us to resolve software installation requests more quickly. The old software collection is still available for the time being (see below), but all new software installations will go into the new collection. Key Differences Module Names The new module collection is built using a tool called \"EasyBuild\", which uses a system of \"toolchains\" to more transparently constrain software dependencies and compatibility. See this page for more information on EasyBuild and toolchains. As such, modules are no longer prefixed with a category such as \"Apps\" or \"Langs\", but instead have a suffix describing its toolchain (if applicable). # old: module load Apps/Octave/4.2.1 # new: module load Octave/4.2.1-foss-2016b You will need to update your scripts to load modules using their new names. Note that these are new installations of the same software (not redirects to the old installations), so please let us know if anything isn't working as expected. Any software you have compiled for yourself will likely need to be recompiled with the new modules. Python and R With the growth of users on our clusters, installing and maintaining central versions of Python and R with all the requested packages has become extremely complicated and unsustainable. As such, our recommendation is now to install a personal Python or R environment using conda which will install the packages of your choosing into your directory. If you run into any difficulty with these environments, we are happy to help--just email us at hpc@yale.edu. To Use Old Software To use any of the old software, run the following commands: module use /gpfs/loomis/apps/hpc.rhel6/Modules module use /gpfs/loomis/apps/hpc.rhel7/Modules module save Then you should see all the old software if you run \"module avail\". To then remove the old software collection from your list, run: module unuse /gpfs/loomis/apps/hpc.rhel6/Modules module unuse /gpfs/loomis/apps/hpc.rhel7/Modules module save We will be deprecating the old software at a TBD date, so we encourage you to contact us if anything is missing or not working for you in the new collection. Lmod Warning Some users have experienced warnings at login due to the module change impacting their default module environment. This warning looks like: Last login: Thu Aug 29 14 :53:22 EDT 2019 on pts/61 Lmod Warning: One or more modules in your default collection have changed: \"StdEnv\" . To see the contents of this collection execute: $ module describe default To rebuild the collection, do a module reset, then load the modules you wish, then execute: $ module save default If you no longer want this module collection execute: $ rm ~/.lmod.d/default For more information execute 'module help' or see http://lmod.readthedocs.org/ No change in modules loaded. To resolve this warning, you will need to save a new default module environment. Load all the modules from the new module list that you wish to have loaded at login, for example: module load R module load dSQ Then save that module environment: module save Then when you log out and log back in the warning should be gone.","title":"New Module System"},{"location":"clusters-at-yale/applications/new-modules-grace/#new-module-system","text":"During the August 2019 Grace scheduled maintenance, we switched the default module collection. This upgrade makes the cluster more consistent with our other clusters' software environments and allows us to resolve software installation requests more quickly. The old software collection is still available for the time being (see below), but all new software installations will go into the new collection.","title":"New Module System"},{"location":"clusters-at-yale/applications/new-modules-grace/#key-differences","text":"","title":"Key Differences"},{"location":"clusters-at-yale/applications/new-modules-grace/#module-names","text":"The new module collection is built using a tool called \"EasyBuild\", which uses a system of \"toolchains\" to more transparently constrain software dependencies and compatibility. See this page for more information on EasyBuild and toolchains. As such, modules are no longer prefixed with a category such as \"Apps\" or \"Langs\", but instead have a suffix describing its toolchain (if applicable). # old: module load Apps/Octave/4.2.1 # new: module load Octave/4.2.1-foss-2016b You will need to update your scripts to load modules using their new names. Note that these are new installations of the same software (not redirects to the old installations), so please let us know if anything isn't working as expected. Any software you have compiled for yourself will likely need to be recompiled with the new modules.","title":"Module Names"},{"location":"clusters-at-yale/applications/new-modules-grace/#python-and-r","text":"With the growth of users on our clusters, installing and maintaining central versions of Python and R with all the requested packages has become extremely complicated and unsustainable. As such, our recommendation is now to install a personal Python or R environment using conda which will install the packages of your choosing into your directory. If you run into any difficulty with these environments, we are happy to help--just email us at hpc@yale.edu.","title":"Python and R"},{"location":"clusters-at-yale/applications/new-modules-grace/#to-use-old-software","text":"To use any of the old software, run the following commands: module use /gpfs/loomis/apps/hpc.rhel6/Modules module use /gpfs/loomis/apps/hpc.rhel7/Modules module save Then you should see all the old software if you run \"module avail\". To then remove the old software collection from your list, run: module unuse /gpfs/loomis/apps/hpc.rhel6/Modules module unuse /gpfs/loomis/apps/hpc.rhel7/Modules module save We will be deprecating the old software at a TBD date, so we encourage you to contact us if anything is missing or not working for you in the new collection.","title":"To Use Old Software"},{"location":"clusters-at-yale/applications/new-modules-grace/#lmod-warning","text":"Some users have experienced warnings at login due to the module change impacting their default module environment. This warning looks like: Last login: Thu Aug 29 14 :53:22 EDT 2019 on pts/61 Lmod Warning: One or more modules in your default collection have changed: \"StdEnv\" . To see the contents of this collection execute: $ module describe default To rebuild the collection, do a module reset, then load the modules you wish, then execute: $ module save default If you no longer want this module collection execute: $ rm ~/.lmod.d/default For more information execute 'module help' or see http://lmod.readthedocs.org/ No change in modules loaded. To resolve this warning, you will need to save a new default module environment. Load all the modules from the new module list that you wish to have loaded at login, for example: module load R module load dSQ Then save that module environment: module save Then when you log out and log back in the warning should be gone.","title":"Lmod Warning"},{"location":"clusters-at-yale/applications/new-modules/","text":"New Module System During the December 10-12, 2018 Milgram scheduled maintenance, we will be switching the default module collection. This upgrade will make the cluster more consistent with our other clusters' software environments and allow us to resolve software installation requests more quickly. The old software collection will be available for the time being (see below), but all new software installations will go into the new collection. Key Differences Module Names The new module collection is built using a tool called \"EasyBuild\", which uses a system of \"toolchains\" to more transparently constrain software dependencies and compatibility. See this page for more information on EasyBuild and toolchains. As such, modules will no longer be prefixed with a category such as \"Apps\" or \"Langs\", but instead will have a suffix describing its toolchain (if applicable). # old: module load Apps/Octave/4.2.1 # new: module load Octave/4.2.1-foss-2016b You will need to update your scripts to load modules using their new names. Note that these are new installations of the same software (not redirects to the old installations), so please let us know if anything isn't working as expected. Python and R With the growth of users on our clusters, installing and maintaining central versions of Python and R with all the requested packages has become extremely complicated and unsustainable. As such, our recommendation is now to install a personal Python or R environment using conda which will install the packages of your choosing into your directory. If you run into any difficulty with these environments, we are happy to help--just email us at hpc@yale.edu. To Try and/or Switch to the New Collection To try the new collection and see if all the software you require is available (email us if anything is missing!), run the following command: source /apps/bin/try_new_modules.sh This will switch your module collection (visible via \"module avail\") to the new one for that session. To revert, simply log out and log back in. To switch permanently, run \"module save\" after running the above command. To Use Old Software To use any of the old software, run the following commands on the Milgram login node: module use /gpfs/milgram/apps/hpc/Modules module save Then you should see all the old software if you run \"module avail\". To then remove the old software collection from your list, run: module unuse /gpfs/milgapps/hpc/Modules module save We will be deprecating the old software at a TBD date, so we encourage you to contact us if anything is missing or not working for you in the new collection.","title":"New Module System"},{"location":"clusters-at-yale/applications/new-modules/#new-module-system","text":"During the December 10-12, 2018 Milgram scheduled maintenance, we will be switching the default module collection. This upgrade will make the cluster more consistent with our other clusters' software environments and allow us to resolve software installation requests more quickly. The old software collection will be available for the time being (see below), but all new software installations will go into the new collection.","title":"New Module System"},{"location":"clusters-at-yale/applications/new-modules/#key-differences","text":"","title":"Key Differences"},{"location":"clusters-at-yale/applications/new-modules/#module-names","text":"The new module collection is built using a tool called \"EasyBuild\", which uses a system of \"toolchains\" to more transparently constrain software dependencies and compatibility. See this page for more information on EasyBuild and toolchains. As such, modules will no longer be prefixed with a category such as \"Apps\" or \"Langs\", but instead will have a suffix describing its toolchain (if applicable). # old: module load Apps/Octave/4.2.1 # new: module load Octave/4.2.1-foss-2016b You will need to update your scripts to load modules using their new names. Note that these are new installations of the same software (not redirects to the old installations), so please let us know if anything isn't working as expected.","title":"Module Names"},{"location":"clusters-at-yale/applications/new-modules/#python-and-r","text":"With the growth of users on our clusters, installing and maintaining central versions of Python and R with all the requested packages has become extremely complicated and unsustainable. As such, our recommendation is now to install a personal Python or R environment using conda which will install the packages of your choosing into your directory. If you run into any difficulty with these environments, we are happy to help--just email us at hpc@yale.edu.","title":"Python and R"},{"location":"clusters-at-yale/applications/new-modules/#to-try-andor-switch-to-the-new-collection","text":"To try the new collection and see if all the software you require is available (email us if anything is missing!), run the following command: source /apps/bin/try_new_modules.sh This will switch your module collection (visible via \"module avail\") to the new one for that session. To revert, simply log out and log back in. To switch permanently, run \"module save\" after running the above command.","title":"To Try and/or Switch to the New Collection"},{"location":"clusters-at-yale/applications/new-modules/#to-use-old-software","text":"To use any of the old software, run the following commands on the Milgram login node: module use /gpfs/milgram/apps/hpc/Modules module save Then you should see all the old software if you run \"module avail\". To then remove the old software collection from your list, run: module unuse /gpfs/milgapps/hpc/Modules module save We will be deprecating the old software at a TBD date, so we encourage you to contact us if anything is missing or not working for you in the new collection.","title":"To Use Old Software"},{"location":"clusters-at-yale/clusters/","text":"HPC Resources Compute We maintain and support four compute clusters, with roughly 30,000 cores total. Please click on cluster names for more information. To download a Word document that describes our facilities, equipment, and other resources for HPC and research computing, click here . Cluster Name Approx. Core Count Login Address Monitor Dashboard Purpose Grace 20,000 grace.hpc.yale.edu cluster.ycrc.yale.edu/grace general and highly parallel, tightly coupled Farnam 5,200 farnam.hpc.yale.edu cluster.ycrc.yale.edu/farnam medical/life science Ruddle 3,500 ruddle.hpc.yale.edu cluster.ycrc.yale.edu/ruddle Yale Center for Genome Analysis Milgram 1,600 milgram.hpc.yale.edu cluster.ycrc.yale.edu/milgram HIPAA Storage We maintain several high performance storage systems which amount to about 9 PB total. Listed below are these shared filesystems and the clusters where they are available. We distinguish where clusters store their home directories with an asterisk. The directory /home will always contain the home directory of the cluster you are on. Filesystem Size Mounting Clusters /gpfs/loomis 2.6 PB Grace*, Farnam /gpfs/ysm 1.5 PB Grace, Farnam* /gpfs/slayman 1.0 PB Grace, Farnam /gpfs/ycga 2.0 PB Ruddle* /ycga-ba 1.1 PB Ruddle /gpfs/milgram 1.1 PB Milgram*","title":"HPC Resources"},{"location":"clusters-at-yale/clusters/#hpc-resources","text":"","title":"HPC Resources"},{"location":"clusters-at-yale/clusters/#compute","text":"We maintain and support four compute clusters, with roughly 30,000 cores total. Please click on cluster names for more information. To download a Word document that describes our facilities, equipment, and other resources for HPC and research computing, click here . Cluster Name Approx. Core Count Login Address Monitor Dashboard Purpose Grace 20,000 grace.hpc.yale.edu cluster.ycrc.yale.edu/grace general and highly parallel, tightly coupled Farnam 5,200 farnam.hpc.yale.edu cluster.ycrc.yale.edu/farnam medical/life science Ruddle 3,500 ruddle.hpc.yale.edu cluster.ycrc.yale.edu/ruddle Yale Center for Genome Analysis Milgram 1,600 milgram.hpc.yale.edu cluster.ycrc.yale.edu/milgram HIPAA","title":"Compute"},{"location":"clusters-at-yale/clusters/#storage","text":"We maintain several high performance storage systems which amount to about 9 PB total. Listed below are these shared filesystems and the clusters where they are available. We distinguish where clusters store their home directories with an asterisk. The directory /home will always contain the home directory of the cluster you are on. Filesystem Size Mounting Clusters /gpfs/loomis 2.6 PB Grace*, Farnam /gpfs/ysm 1.5 PB Grace, Farnam* /gpfs/slayman 1.0 PB Grace, Farnam /gpfs/ycga 2.0 PB Ruddle* /ycga-ba 1.1 PB Ruddle /gpfs/milgram 1.1 PB Milgram*","title":"Storage"},{"location":"clusters-at-yale/clusters/farnam/","text":"Farnam The Farnam Cluster is named for Louise Whitman Farnam , the first woman to graduate from the Yale School of Medicine, class of 1916. Farnam is a shared-use resource for the Yale School of Medicine (YSM). It consists of a variety of compute nodes networked over ethernet and mounts several shared filesystems. Hardware Farnam is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. GPUs listed can be requested with the --gres flag, e.g. --gres=gpu:gtx1080ti:2 would request 2 GeForce GTX 1080Ti GPUs per node. See the Request Compute Resources page for more info. Warning Care should be taken when scheduling your job if you are running programs/libraries optimized for specific hardware. You can narrow which nodes can run your job by requesting the features from the Node Configurations table as constraints (slurm --constraint flag) to your job. See the Request Compute Resources page and the Build Software page for further guidance. Compute Node Configurations Count CPU CPU Cores RAM GPU vRAM/GPU Features 117 2x E5-2660_v3 20 121G haswell, avx2, E5-2660_v3, nogpu, standard, oldest 5 2x E5-2660 v3 20 121G 2x k80 (2GPUs/k80) 12G haswell, avx2, E5-2660_v3, doubleprecision 2 4x E7-4809_v3 32 1507G haswell, avx2, E7-4809_v3, nogpu 1 2x E5-2623 v4 8 59G 4x gtx1080ti 11G broadwell, avx2, E5-2623_v4, singleprecision 1 2x E5-2637 v4 8 121G 4x titanv 12G broadwell, avx2, E5-2637_v4, doubleprecision 21 2x E5-2637 v4 8 121G 4x gtx1080ti 11G broadwell, avx2, E5-2637_v4, singleprecision 3 2x E5-2660 v4 28 247G 2x p100 16G broadwell, avx2, E5-2660_v4, doubleprecision 38 2x E5-2680_v4 28 247G broadwell, avx2, E5-2680_v4, nogpu, standard 1 4x E7-4820_v4 40 1507G broadwell, avx2, E7-4820_v4, nogpu 2 2x 5122 8 183G 4x rtx2080 8G skylake, avx2, avx512, 5122, singleprecision 1 2x 6132 28 751G skylake, avx2, avx512, 6132, nogpu 2 2x 6132 28 183G skylake, avx2, avx512, 6132, nogpu, standard 4 2x 6240 36 750G cascadelake, avx2, avx512, 6240, nogpu 4 2x 6240 36 372G cascadelake, avx2, avx512, 6240, nogpu 24 2x 6240 36 183G cascadelake, avx2, avx512, 6240, nogpu, standard 3 2x 6240 36 1507G cascadelake, avx2, avx512, 6240, nogpu 9 2x 5222 8 183G 4x rtx5000 16G cascadelake, avx2, avx512, 5222, doubleprecision 1 2x 6242 32 1001G 2x rtx8000 48G cascadelake, avx2, avx512, 6242, doubleprecision Slurm Partitions Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The default resource requests for all jobs is 1 core and 5G of memory per core. Public Partitions The general partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The bigmem partition contains our largest memory nodes--only jobs that cannot be satisfied by general should run here. The gpu_devel partition is a single node meant for testing or compiling GPU accelerated code, and the gpu partition is where normal GPU jobs should run. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime Default/Max Node Type (count) general* 400 CPUs, 2560 G RAM 200 CPUs, 1280 G RAM 1d/30d E5-2660_v3 (93), 6240 (19) interactive 2 jobs, 20 CPUs, 256 G RAM 6hr/1d E5-2660_v3 (93) bigmem 2 jobs, 32 CPUs, 1532 G RAM 1d/3d E7-4809_v3 (2), 6240 (3) gpu_devel 1 job 10min/2hr E5-2623_v4 gtx1080ti (1) gpu 32 CPUs, 256 G RAM 1d/2d E5-2660_v3 k80 (2), E5-2637_v4 gtx1080ti (10) scavenge 800 CPUs, 5120 G RAM 1d/7d all scavenge_gpu 32 CPUs, 256 G RAM 1d/2d all nodes with GPUs (see Compute Node table) * default Private Partitions Private partitions contain nodes acquired by specific research groups. Full access to these partitions is granted at the discretion of the owner. Contact us if your group would like to purchase nodes. Partition Walltime Default/Max User Limits Node Type (count) pi_breaker 1d / 14d E5-2680_v4 (24) pi_cryoem 1d / 365d 2 jobs, 12 GPUs E5-2637_v4 gtx1080ti (10) pi_deng 1d / 14d E5-2680_v4 p100 (1) pi_dunn 1d / 14d 6240 (1) pi_edwards 1d / 14d 6240 (1) pi_gerstein 1d / 14d E7-4820_v4 (1), E5-2680_v4 (11), 6132 751G (1), 6132 (2) pi_gerstein_gpu 1d / 14d E5-2660_v3 k80 (3), E5-2680_v4 p100 (2), E5-2637_v4 titanv (1) pi_gruen 1d / 14d E5-2680_v4 (1) pi_jadi 1d / 365d E5-2680_v4 (2) pi_jetz 1d / 14d 6240 372G (4), 6240 750G (4) pi_kleinstein 1d / 14d 6240 (1), E5-2660_v3 (3) pi_krauthammer 1d / 14d E5-2660_v3 (1) pi_ma 1d / 14d E5-2660_v3 (2) pi_ohern 1d / 14d E5-2660_v3 (5) pi_reinisch 1d / 14d 5122 rtx2080 (2) pi_sigworth 1d / 14d E5-2660_v3 (1) pi_sindelar 1d / 14d E5-2637_v4 gtx1080ti (1), E5-2660_v3 (1) pi_tomography 1d / 4d 2 jobs, 12 GPUs 5222 rtx5000 (9), 6242 rtx8000 (1) pi_townsend 1d / 14d E5-2660_v3 (5) pi_zhao 1d / 14d 6240 (2) Public Datasets We host datasets of general interest in a loosely organized directory tree in /gpfs/ysm/datasets : \u251c\u2500\u2500 cryoem \u251c\u2500\u2500 db \u2502 \u251c\u2500\u2500 annovar \u2502 \u2514\u2500\u2500 blast \u2514\u2500\u2500 genomes \u251c\u2500\u2500 1000Genomes \u251c\u2500\u2500 10xgenomics \u251c\u2500\u2500 Aedes_aegypti \u251c\u2500\u2500 Chelonoidis_nigra \u251c\u2500\u2500 Danio_rerio \u251c\u2500\u2500 hisat2 \u251c\u2500\u2500 Homo_sapiens \u251c\u2500\u2500 Mus_musculus \u251c\u2500\u2500 PhiX \u2514\u2500\u2500 Saccharomyces_cerevisiae If you would like us to host a dataset or questions about what is currently available, please email hpc@yale.edu. Storage Farnam has access to a number of GPFS filesystems. /gpfs/ysm is Farnam's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/ysm/home 125G/user 500,000 Yes project /gpfs/ysm/project 4T/group 5,000,000 No scratch60 /gpfs/ysm/scratch60 10T/group 5,000,000 No","title":"Farnam"},{"location":"clusters-at-yale/clusters/farnam/#farnam","text":"The Farnam Cluster is named for Louise Whitman Farnam , the first woman to graduate from the Yale School of Medicine, class of 1916. Farnam is a shared-use resource for the Yale School of Medicine (YSM). It consists of a variety of compute nodes networked over ethernet and mounts several shared filesystems.","title":"Farnam"},{"location":"clusters-at-yale/clusters/farnam/#hardware","text":"Farnam is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. GPUs listed can be requested with the --gres flag, e.g. --gres=gpu:gtx1080ti:2 would request 2 GeForce GTX 1080Ti GPUs per node. See the Request Compute Resources page for more info. Warning Care should be taken when scheduling your job if you are running programs/libraries optimized for specific hardware. You can narrow which nodes can run your job by requesting the features from the Node Configurations table as constraints (slurm --constraint flag) to your job. See the Request Compute Resources page and the Build Software page for further guidance.","title":"Hardware"},{"location":"clusters-at-yale/clusters/farnam/#compute-node-configurations","text":"Count CPU CPU Cores RAM GPU vRAM/GPU Features 117 2x E5-2660_v3 20 121G haswell, avx2, E5-2660_v3, nogpu, standard, oldest 5 2x E5-2660 v3 20 121G 2x k80 (2GPUs/k80) 12G haswell, avx2, E5-2660_v3, doubleprecision 2 4x E7-4809_v3 32 1507G haswell, avx2, E7-4809_v3, nogpu 1 2x E5-2623 v4 8 59G 4x gtx1080ti 11G broadwell, avx2, E5-2623_v4, singleprecision 1 2x E5-2637 v4 8 121G 4x titanv 12G broadwell, avx2, E5-2637_v4, doubleprecision 21 2x E5-2637 v4 8 121G 4x gtx1080ti 11G broadwell, avx2, E5-2637_v4, singleprecision 3 2x E5-2660 v4 28 247G 2x p100 16G broadwell, avx2, E5-2660_v4, doubleprecision 38 2x E5-2680_v4 28 247G broadwell, avx2, E5-2680_v4, nogpu, standard 1 4x E7-4820_v4 40 1507G broadwell, avx2, E7-4820_v4, nogpu 2 2x 5122 8 183G 4x rtx2080 8G skylake, avx2, avx512, 5122, singleprecision 1 2x 6132 28 751G skylake, avx2, avx512, 6132, nogpu 2 2x 6132 28 183G skylake, avx2, avx512, 6132, nogpu, standard 4 2x 6240 36 750G cascadelake, avx2, avx512, 6240, nogpu 4 2x 6240 36 372G cascadelake, avx2, avx512, 6240, nogpu 24 2x 6240 36 183G cascadelake, avx2, avx512, 6240, nogpu, standard 3 2x 6240 36 1507G cascadelake, avx2, avx512, 6240, nogpu 9 2x 5222 8 183G 4x rtx5000 16G cascadelake, avx2, avx512, 5222, doubleprecision 1 2x 6242 32 1001G 2x rtx8000 48G cascadelake, avx2, avx512, 6242, doubleprecision","title":"Compute Node Configurations"},{"location":"clusters-at-yale/clusters/farnam/#slurm-partitions","text":"Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The default resource requests for all jobs is 1 core and 5G of memory per core.","title":"Slurm Partitions"},{"location":"clusters-at-yale/clusters/farnam/#public-partitions","text":"The general partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The bigmem partition contains our largest memory nodes--only jobs that cannot be satisfied by general should run here. The gpu_devel partition is a single node meant for testing or compiling GPU accelerated code, and the gpu partition is where normal GPU jobs should run. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime Default/Max Node Type (count) general* 400 CPUs, 2560 G RAM 200 CPUs, 1280 G RAM 1d/30d E5-2660_v3 (93), 6240 (19) interactive 2 jobs, 20 CPUs, 256 G RAM 6hr/1d E5-2660_v3 (93) bigmem 2 jobs, 32 CPUs, 1532 G RAM 1d/3d E7-4809_v3 (2), 6240 (3) gpu_devel 1 job 10min/2hr E5-2623_v4 gtx1080ti (1) gpu 32 CPUs, 256 G RAM 1d/2d E5-2660_v3 k80 (2), E5-2637_v4 gtx1080ti (10) scavenge 800 CPUs, 5120 G RAM 1d/7d all scavenge_gpu 32 CPUs, 256 G RAM 1d/2d all nodes with GPUs (see Compute Node table) * default","title":"Public Partitions"},{"location":"clusters-at-yale/clusters/farnam/#private-partitions","text":"Private partitions contain nodes acquired by specific research groups. Full access to these partitions is granted at the discretion of the owner. Contact us if your group would like to purchase nodes. Partition Walltime Default/Max User Limits Node Type (count) pi_breaker 1d / 14d E5-2680_v4 (24) pi_cryoem 1d / 365d 2 jobs, 12 GPUs E5-2637_v4 gtx1080ti (10) pi_deng 1d / 14d E5-2680_v4 p100 (1) pi_dunn 1d / 14d 6240 (1) pi_edwards 1d / 14d 6240 (1) pi_gerstein 1d / 14d E7-4820_v4 (1), E5-2680_v4 (11), 6132 751G (1), 6132 (2) pi_gerstein_gpu 1d / 14d E5-2660_v3 k80 (3), E5-2680_v4 p100 (2), E5-2637_v4 titanv (1) pi_gruen 1d / 14d E5-2680_v4 (1) pi_jadi 1d / 365d E5-2680_v4 (2) pi_jetz 1d / 14d 6240 372G (4), 6240 750G (4) pi_kleinstein 1d / 14d 6240 (1), E5-2660_v3 (3) pi_krauthammer 1d / 14d E5-2660_v3 (1) pi_ma 1d / 14d E5-2660_v3 (2) pi_ohern 1d / 14d E5-2660_v3 (5) pi_reinisch 1d / 14d 5122 rtx2080 (2) pi_sigworth 1d / 14d E5-2660_v3 (1) pi_sindelar 1d / 14d E5-2637_v4 gtx1080ti (1), E5-2660_v3 (1) pi_tomography 1d / 4d 2 jobs, 12 GPUs 5222 rtx5000 (9), 6242 rtx8000 (1) pi_townsend 1d / 14d E5-2660_v3 (5) pi_zhao 1d / 14d 6240 (2)","title":"Private Partitions"},{"location":"clusters-at-yale/clusters/farnam/#public-datasets","text":"We host datasets of general interest in a loosely organized directory tree in /gpfs/ysm/datasets : \u251c\u2500\u2500 cryoem \u251c\u2500\u2500 db \u2502 \u251c\u2500\u2500 annovar \u2502 \u2514\u2500\u2500 blast \u2514\u2500\u2500 genomes \u251c\u2500\u2500 1000Genomes \u251c\u2500\u2500 10xgenomics \u251c\u2500\u2500 Aedes_aegypti \u251c\u2500\u2500 Chelonoidis_nigra \u251c\u2500\u2500 Danio_rerio \u251c\u2500\u2500 hisat2 \u251c\u2500\u2500 Homo_sapiens \u251c\u2500\u2500 Mus_musculus \u251c\u2500\u2500 PhiX \u2514\u2500\u2500 Saccharomyces_cerevisiae If you would like us to host a dataset or questions about what is currently available, please email hpc@yale.edu.","title":"Public Datasets"},{"location":"clusters-at-yale/clusters/farnam/#storage","text":"Farnam has access to a number of GPFS filesystems. /gpfs/ysm is Farnam's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/ysm/home 125G/user 500,000 Yes project /gpfs/ysm/project 4T/group 5,000,000 No scratch60 /gpfs/ysm/scratch60 10T/group 5,000,000 No","title":"Storage"},{"location":"clusters-at-yale/clusters/grace/","text":"Grace The Grace cluster is is named for the computer scientist and United States Navy Rear Admiral Grace Murray Hopper , who received her Ph.D. in Mathematics from Yale in 1934. Grace is a shared-use resource for the Faculty of Arts and Sciences (FAS). It consists of a variety of compute nodes networked over low-latency InfiniBand and mounts several shared filesystems. Hardware Grace is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. GPUs listed can be requested with the --gres flag, e.g. --gres=gpu:p100:1 would request one Tesla P100 GPU per node. See the Request Compute Resources page for more info. Warning Care should be taken when scheduling your job if you are running programs/libraries optimized for specific hardware. You can narrow which nodes can run your job by requesting the features from the Node Configurations table as constraints (slurm --constraint flag) to your job. See the Request Compute Resources page and the Build Software page for further guidance. Compute Node Configurations Count CPU Cores RAM GPU Features 80 2x E5-2660_v2 20 121G ivybridge,E5-2660_v2,nogpu,standard,oldest 1 2x E7-4820_v2 32 1003G ivybridge,E7-4820_v2,nogpu 136 2x E5-2660_v3 20 121G haswell,avx2,E5-2660_v3,nogpu,standard 20 2x E5-2660_v3 20 247G haswell,avx2,E5-2660_v3,nogpu,standard 8 2x E5-2660_v3 20 247G 2x k80 haswell,avx2,E5-2660_v3,doubleprecision 6 2x E5-2660_v3 20 121G 4x k80 haswell,avx2,E5-2660_v3,doubleprecision 1 2x E7-4809_v3 32 2011G haswell,avx2,E7-4809_v3,nogpu 161 2x E5-2660_v4 28 247G broadwell,avx2,E5-2660_v4,nogpu,standard 7 2x E5-2660_v4 28 247G 1x p100 broadwell,avx2,E5-2660_v4,doubleprecision 4 2x E7-4820_v4 40 1507G broadwell,avx2,E7-4820_v4,nogpu 1 2x E5-2637_v4 8 121G 4x gtx1080ti broadwell,avx2,E5-2637_v4,singleprecision 136 2x 6136 24 90G hdr,skylake,avx2,avx512,6136,nogpu,standard 16 2x 6136 24 90G edr,skylake,avx2,avx512,6136,nogpu,standard 9 2x 6136 24 183G 4x p100 skylake,avx2,avx512,6136,doubleprecision 3 2x 6142 32 183G skylake,avx2,avx512,6142,nogpu,standard 2 2x 6136 24 90G 2x v100 skylake,avx2,avx512,6136,doubleprecision 2 2x 5122 8 183G 4x rtx2080 skylake,avx2,avx512,5122,singleprecision 1 2x 6126 24 176G skylake,avx2,avx512,6126,nogpu,standard 1 2x 6136 24 751G skylake,avx2,avx512,6136,nogpu 131 2x 6240 36 183G cascadelake,avx2,avx512,6240,nogpu,standard 76 2x 6240 36 181G cascadelake,avx2,avx512,6240,nogpu,standard 20 2x 8260 96 183G cascadelake,avx2,avx512,8260,nogpu 8 2x 6240 36 371G cascadelake,avx2,avx512,6240,nogpu,standard 5 2x 6240 36 183G 4x rtx2080ti cascadelake,avx2,avx512,6240,singleprecision 5 2x 6240 36 372G 4x v100 cascadelake,avx2,avx512,6240,doubleprecision 2 2x 6240 36 1506G cascadelake,avx2,avx512,6240,nogpu 1 2x 6240 36 1503G cascadelake,avx2,avx512,6240,nogpu,standard 1 2x 6254 36 1506G 4x rtx4000 2x rtx8000 2x v100 cascadelake,avx2,avx512,6254 Slurm Partitions Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The default resource requests for all jobs is 1 core and 5GB of memory, unless otherwise specified. Public Partitions The day partition is where most batch jobs should run, and is the default if you don't specify a partition. The week partition is smaller, but allows for longer jobs. The interactive partition should only be used for testing or compiling software. The bigmem partition contains our largest memory node; only jobs that cannot be satisfied by general should run here. The gpu_devel partition is a single node meant for testing or compiling GPU accelerated code, and the gpu partition is where normal GPU jobs should run. The mpi partition is reserved for tightly-coupled parallel and access is by special request only. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime (Default/Max) Node Type (count) bigmem CPUs: 40, RAM: 1500G 1h/1d E7-4820_v4 1507G (2), 6240 1503G (1), 6240 1506G (2) covid 1h/1d E5-2660_v4 (72), 6240 (67), E5-2660_v2 (35), E5-2660_v3 (44), 6240 (68) day* CPUs: 900 CPUs: 640 1h/1d E5-2660_v4 (72), 6240 (67), E5-2660_v2 (35), E5-2660_v3 (44), 6240 (68) gpu GPUs: 24 1h/1d 6240 rtx2080ti:4 (1), 6136 v100:2 (2), 6240 v100:4 (5), 6240 rtx2080ti:4 (4), E5-2660_v3 k80:4 (3), E5-2660_v4 p100:1 (6) gpu_devel CPUs: 10 1h/4h E5-2660_v3 k80:4 (1) interactive CPUs: 4, RAM: 32G 1h/6h E5-2660_v2 (2), 6126 (1) mpi** Nodes: 48 Nodes: 32 1h/1d 6136 (120) scavenge CPUs: 10000 1h/1d all scavenge_gpu GPUs: 30 1h/1d all with GPUs (see Nodes table) week CPUs: 250 CPUs: 100 1h/7d E5-2660_v3 (36), E5-2660_v4 (7) * default ** The mpi partition is reserved for tightly-coupled parallel programs that make efficient use of multiple nodes. See our MPI documentation if your workload fits this description. The default memory request on the mpi partition in 3.75GB per core. Private Partitions Private partitions contain nodes acquired by specific research groups. Full access to these partitions is granted at the discretion of the owner. Contact us if your group would like to purchase nodes. Partition Walltime Default/Max Node Type (count) pi_altonji 1h/28d E5-2660_v3 (2) pi_anticevic 1h/100d E5-2660_v3 (16), E5-2660_v4 (20) pi_anticevic_bigmem 1h/100d E7-4809_v3 2011G (1) pi_anticevic_gpu 1h/100d E5-2660_v3 k80:2 (8) pi_anticevic_z 1h/100d E5-2660_v3 (3) pi_balou 1h/28d E5-2660_v4 (30), 6240 (9) pi_berry 1h/28d E5-2660_v3 (1), 6240 (1) pi_cowles 1h/28d E5-2660_v3 (14) pi_cowles_nopreempt 1h/28d E5-2660_v3 (10) pi_econ_io 1h/28d 6240 (6) pi_econ_lp 1h/28d 6240 (5) pi_esi 1h/28d 6240 (36) pi_fedorov* 1h/28d 6136 (12) pi_gelernter 1h/28d E5-2660_v4 (1), 6240 (1) pi_gerstein 1h/28d E7-4820_v2 1003G (1), E5-2660_v3 (32) pi_glahn 1h/100d E5-2660_v3 (1) pi_hammes_schiffer* 1h/28d E5-2637_v4 gtx1080ti:4 (1), 6136 751G (1), 6136 (16), 5122 rtx2080:4 (2) pi_hodgson 1h/28d 6240 (1) pi_holland 1h/28d E5-2660_v2 (2), E5-2660_v3 (2), 6240 (8) pi_howard 1h/28d 6240 (1) pi_jetz 1h/28d E5-2660_v4 (2) pi_kaminski 1h/28d E5-2660_v3 (8) pi_lederman 1h/28d 6254 1506G rtx4000:4,rtx8000:2,v100:2 (1) pi_levine 1h/28d 8260 (20) pi_lora* 1h/28d 6136 (4) pi_mak 1h/28d E5-2660_v4 (3) pi_manohar 1h/180d E7-4820_v4 1507G (2), E5-2660_v4 (8), E5-2660_v4 p100:1 (1), 6240 (4) pi_ohern 1h/28d E5-2660_v2 (16), E5-2660_v4 (3), 6136 p100:4 (9), 6240 (2) pi_owen_miller 1h/28d E5-2660_v4 (5) pi_poland 1h/28d E5-2660_v4 (10), 6240 (8) pi_polimanti 1h/28d 6240 (1) pi_seto 1h/28d 6142 (3) pi_tsmith 1h/28d E5-2660_v3 (1) * The default memory request on this partition is 3.75GB per core. Storage Grace has access to a number of GPFS filesystems. /gpfs/loomis is Grace's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/loomis/home.grace 100G/user 200,000 Yes project /gpfs/loomis/project 1T/group 5,000,000 No scratch60 /gpfs/loomis/scratch60 20T/group 5,000,000 No","title":"Grace"},{"location":"clusters-at-yale/clusters/grace/#grace","text":"The Grace cluster is is named for the computer scientist and United States Navy Rear Admiral Grace Murray Hopper , who received her Ph.D. in Mathematics from Yale in 1934. Grace is a shared-use resource for the Faculty of Arts and Sciences (FAS). It consists of a variety of compute nodes networked over low-latency InfiniBand and mounts several shared filesystems.","title":"Grace"},{"location":"clusters-at-yale/clusters/grace/#hardware","text":"Grace is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. GPUs listed can be requested with the --gres flag, e.g. --gres=gpu:p100:1 would request one Tesla P100 GPU per node. See the Request Compute Resources page for more info. Warning Care should be taken when scheduling your job if you are running programs/libraries optimized for specific hardware. You can narrow which nodes can run your job by requesting the features from the Node Configurations table as constraints (slurm --constraint flag) to your job. See the Request Compute Resources page and the Build Software page for further guidance.","title":"Hardware"},{"location":"clusters-at-yale/clusters/grace/#compute-node-configurations","text":"Count CPU Cores RAM GPU Features 80 2x E5-2660_v2 20 121G ivybridge,E5-2660_v2,nogpu,standard,oldest 1 2x E7-4820_v2 32 1003G ivybridge,E7-4820_v2,nogpu 136 2x E5-2660_v3 20 121G haswell,avx2,E5-2660_v3,nogpu,standard 20 2x E5-2660_v3 20 247G haswell,avx2,E5-2660_v3,nogpu,standard 8 2x E5-2660_v3 20 247G 2x k80 haswell,avx2,E5-2660_v3,doubleprecision 6 2x E5-2660_v3 20 121G 4x k80 haswell,avx2,E5-2660_v3,doubleprecision 1 2x E7-4809_v3 32 2011G haswell,avx2,E7-4809_v3,nogpu 161 2x E5-2660_v4 28 247G broadwell,avx2,E5-2660_v4,nogpu,standard 7 2x E5-2660_v4 28 247G 1x p100 broadwell,avx2,E5-2660_v4,doubleprecision 4 2x E7-4820_v4 40 1507G broadwell,avx2,E7-4820_v4,nogpu 1 2x E5-2637_v4 8 121G 4x gtx1080ti broadwell,avx2,E5-2637_v4,singleprecision 136 2x 6136 24 90G hdr,skylake,avx2,avx512,6136,nogpu,standard 16 2x 6136 24 90G edr,skylake,avx2,avx512,6136,nogpu,standard 9 2x 6136 24 183G 4x p100 skylake,avx2,avx512,6136,doubleprecision 3 2x 6142 32 183G skylake,avx2,avx512,6142,nogpu,standard 2 2x 6136 24 90G 2x v100 skylake,avx2,avx512,6136,doubleprecision 2 2x 5122 8 183G 4x rtx2080 skylake,avx2,avx512,5122,singleprecision 1 2x 6126 24 176G skylake,avx2,avx512,6126,nogpu,standard 1 2x 6136 24 751G skylake,avx2,avx512,6136,nogpu 131 2x 6240 36 183G cascadelake,avx2,avx512,6240,nogpu,standard 76 2x 6240 36 181G cascadelake,avx2,avx512,6240,nogpu,standard 20 2x 8260 96 183G cascadelake,avx2,avx512,8260,nogpu 8 2x 6240 36 371G cascadelake,avx2,avx512,6240,nogpu,standard 5 2x 6240 36 183G 4x rtx2080ti cascadelake,avx2,avx512,6240,singleprecision 5 2x 6240 36 372G 4x v100 cascadelake,avx2,avx512,6240,doubleprecision 2 2x 6240 36 1506G cascadelake,avx2,avx512,6240,nogpu 1 2x 6240 36 1503G cascadelake,avx2,avx512,6240,nogpu,standard 1 2x 6254 36 1506G 4x rtx4000 2x rtx8000 2x v100 cascadelake,avx2,avx512,6254","title":"Compute Node Configurations"},{"location":"clusters-at-yale/clusters/grace/#slurm-partitions","text":"Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The default resource requests for all jobs is 1 core and 5GB of memory, unless otherwise specified.","title":"Slurm Partitions"},{"location":"clusters-at-yale/clusters/grace/#public-partitions","text":"The day partition is where most batch jobs should run, and is the default if you don't specify a partition. The week partition is smaller, but allows for longer jobs. The interactive partition should only be used for testing or compiling software. The bigmem partition contains our largest memory node; only jobs that cannot be satisfied by general should run here. The gpu_devel partition is a single node meant for testing or compiling GPU accelerated code, and the gpu partition is where normal GPU jobs should run. The mpi partition is reserved for tightly-coupled parallel and access is by special request only. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime (Default/Max) Node Type (count) bigmem CPUs: 40, RAM: 1500G 1h/1d E7-4820_v4 1507G (2), 6240 1503G (1), 6240 1506G (2) covid 1h/1d E5-2660_v4 (72), 6240 (67), E5-2660_v2 (35), E5-2660_v3 (44), 6240 (68) day* CPUs: 900 CPUs: 640 1h/1d E5-2660_v4 (72), 6240 (67), E5-2660_v2 (35), E5-2660_v3 (44), 6240 (68) gpu GPUs: 24 1h/1d 6240 rtx2080ti:4 (1), 6136 v100:2 (2), 6240 v100:4 (5), 6240 rtx2080ti:4 (4), E5-2660_v3 k80:4 (3), E5-2660_v4 p100:1 (6) gpu_devel CPUs: 10 1h/4h E5-2660_v3 k80:4 (1) interactive CPUs: 4, RAM: 32G 1h/6h E5-2660_v2 (2), 6126 (1) mpi** Nodes: 48 Nodes: 32 1h/1d 6136 (120) scavenge CPUs: 10000 1h/1d all scavenge_gpu GPUs: 30 1h/1d all with GPUs (see Nodes table) week CPUs: 250 CPUs: 100 1h/7d E5-2660_v3 (36), E5-2660_v4 (7) * default ** The mpi partition is reserved for tightly-coupled parallel programs that make efficient use of multiple nodes. See our MPI documentation if your workload fits this description. The default memory request on the mpi partition in 3.75GB per core.","title":"Public Partitions"},{"location":"clusters-at-yale/clusters/grace/#private-partitions","text":"Private partitions contain nodes acquired by specific research groups. Full access to these partitions is granted at the discretion of the owner. Contact us if your group would like to purchase nodes. Partition Walltime Default/Max Node Type (count) pi_altonji 1h/28d E5-2660_v3 (2) pi_anticevic 1h/100d E5-2660_v3 (16), E5-2660_v4 (20) pi_anticevic_bigmem 1h/100d E7-4809_v3 2011G (1) pi_anticevic_gpu 1h/100d E5-2660_v3 k80:2 (8) pi_anticevic_z 1h/100d E5-2660_v3 (3) pi_balou 1h/28d E5-2660_v4 (30), 6240 (9) pi_berry 1h/28d E5-2660_v3 (1), 6240 (1) pi_cowles 1h/28d E5-2660_v3 (14) pi_cowles_nopreempt 1h/28d E5-2660_v3 (10) pi_econ_io 1h/28d 6240 (6) pi_econ_lp 1h/28d 6240 (5) pi_esi 1h/28d 6240 (36) pi_fedorov* 1h/28d 6136 (12) pi_gelernter 1h/28d E5-2660_v4 (1), 6240 (1) pi_gerstein 1h/28d E7-4820_v2 1003G (1), E5-2660_v3 (32) pi_glahn 1h/100d E5-2660_v3 (1) pi_hammes_schiffer* 1h/28d E5-2637_v4 gtx1080ti:4 (1), 6136 751G (1), 6136 (16), 5122 rtx2080:4 (2) pi_hodgson 1h/28d 6240 (1) pi_holland 1h/28d E5-2660_v2 (2), E5-2660_v3 (2), 6240 (8) pi_howard 1h/28d 6240 (1) pi_jetz 1h/28d E5-2660_v4 (2) pi_kaminski 1h/28d E5-2660_v3 (8) pi_lederman 1h/28d 6254 1506G rtx4000:4,rtx8000:2,v100:2 (1) pi_levine 1h/28d 8260 (20) pi_lora* 1h/28d 6136 (4) pi_mak 1h/28d E5-2660_v4 (3) pi_manohar 1h/180d E7-4820_v4 1507G (2), E5-2660_v4 (8), E5-2660_v4 p100:1 (1), 6240 (4) pi_ohern 1h/28d E5-2660_v2 (16), E5-2660_v4 (3), 6136 p100:4 (9), 6240 (2) pi_owen_miller 1h/28d E5-2660_v4 (5) pi_poland 1h/28d E5-2660_v4 (10), 6240 (8) pi_polimanti 1h/28d 6240 (1) pi_seto 1h/28d 6142 (3) pi_tsmith 1h/28d E5-2660_v3 (1) * The default memory request on this partition is 3.75GB per core.","title":"Private Partitions"},{"location":"clusters-at-yale/clusters/grace/#storage","text":"Grace has access to a number of GPFS filesystems. /gpfs/loomis is Grace's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/loomis/home.grace 100G/user 200,000 Yes project /gpfs/loomis/project 1T/group 5,000,000 No scratch60 /gpfs/loomis/scratch60 20T/group 5,000,000 No","title":"Storage"},{"location":"clusters-at-yale/clusters/milgram-dev/","text":"Milgram Milgram is named for Dr. Stanley Milgram, a psychologist who researched the behavioral motivations behind social awareness in individuals and obedience to authority figures. He conducted several famous experiments during his professorship at Yale University including the lost-letter experiment, the small-world experiment, and the Milgram experiment on obedience to authority figures. Milgram is a HIPAA aligned Department of Psychology cluster intended for use on projects that may involve sensitive data. This applies to both storage and computation. If you have any questions about this policy, please contact us . Note Connections to Milgram can only be made from the HIPAA VPN ( access.yale.edu/hipaa ). See our VPN page for setup instructions. If your group has a workstation (see list ), you can connect using one of those. Hardware Milgram is made up of a couple kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. Warning Care should be taken if when scheduling your job if you are running programs/libraries optimized for specific hardware. See the guide on how to compile software for specific guidance. Compute Node Configurations Count CPU CPU Cores RAM Features 12 2x E5-2660 v3 20 121G haswell, E5-2660_v3, oldest 48 2x E5-2660 v4 28 250G broadwell, E5-2660_v4 Slurm Partitions Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The default resource requests for all jobs is 1 core and 5GB of memory per core. The short partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The long and verylong partitions are meant for jobs with projected walltimes that are too long to run in short. For courses using the cluster we set aside the education partition. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime Default/Max Node Type (count) short* 1158 CPUs, 10176 G RAM 772 CPUs, 6784 G RAM 1h/6h E5-2660_v3 (9), E5-2660_v4 (48) interactive 1 job, 4 CPUs, 20 G RAM 1h/6h E5-2660_v3 (1) long 1188 CPUs, 5940 G RAM 1h/2d E5-2660_v3 (9), E5-2660_v4 (48) verylong 792 CPUs, 3960 G RAM 1h/7d E5-2660_v3 (9), E5-2660_v4 (48) education 1h/6h E5-2660_v3 (2) scavenge none E5-2660_v3 (9), E5-2660_v4 (48) * default Storage /gpfs/milgram is Milgram's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/milgram/home 20G/user 500,000 Yes project /gpfs/milgram/project varies varies No scratch60 /gpfs/milgram/scratch60 varies 5,000,000 No Data Fileset There is a data fileset on /gpfs/milgram designed to host shared, public datasets to prevent duplication of data across the machine. Each dataset in the data fileset has one curator who is in charge of populating the dataset, managing permissions and requesting access to that dataset. See below for a list of datasets. If you have questions about or would like read access to a specific dataset, please contact the listed curator. Dataset Directory Owner Curator ABCD /gpfs/milgram/data/ABCD TBD TBD bold5000 /gpfs/milgram/data/bold500 TBD TBD brainiak_tutorials /gpfs/milgram/data/brainiak_tutorials TBD TBD bsnip1 /gpfs/milgram/data/bsnip1 TBD TBD GSP /gpfs/milgram/data/GSP TBD TBD HCP /gpfs/milgram/data/HCP TBD TBD HealthyBrain /gpfs/milgram/data/HealthyBrain TBD TBD NKI /gpfs/milgram/data/NKI TBD TBD PartlyCloudy /gpfs/milgram/data/PartlyCloudy TBD TBD PING /gpfs/milgram/data/PING TBD TBD UKB /gpfs/milgram/data/UKB TBD TBD yale_smart /gpfs/milgram/data/yale_smart TBD TBD For Dataset Curators The appointed curators for each dataset has the following responsibilities. Manage Datasets and Reset Permissions The curator for a specific fileset is the only user with write access to that directory. It is their responsibility to manage the data in that directory. In order to add new data, the curator will need to temporarily grant themselves read permission to the directory where they will be populating the data: cd /gpfs/milgram/data/ [ destination_for_data ] chmod u+w . To move or clean out data, the curator will also need to temporarily grant themselves write permission for the relevant data using chmod u+w [file] or chmod -R u+w [directory] . After data management is complete, the permissions must be reset to read-only to prevent accidental contamination of the dataset. To reset the permissions, run the following command at the highest level directory where the data was manipulated: cd /gpfs/milgram/data/ [ destination_for_data ] chmod -R ugo-w . Request Access for New User The curator is responsible for fielding access requests to their directory. Once a new user has been given permission to access the directory, the curator themselves will submit a permission change request to hpc@yale.edu. Appropriate Data Only The data fileset should only be used for raw datasets. Approval must be requested from ????? to add data products generated at Yale to the fileset. Intermediate data products and temporary files (such as slurm logs) should never be placed in the data directory.","title":"Milgram"},{"location":"clusters-at-yale/clusters/milgram-dev/#milgram","text":"Milgram is named for Dr. Stanley Milgram, a psychologist who researched the behavioral motivations behind social awareness in individuals and obedience to authority figures. He conducted several famous experiments during his professorship at Yale University including the lost-letter experiment, the small-world experiment, and the Milgram experiment on obedience to authority figures. Milgram is a HIPAA aligned Department of Psychology cluster intended for use on projects that may involve sensitive data. This applies to both storage and computation. If you have any questions about this policy, please contact us . Note Connections to Milgram can only be made from the HIPAA VPN ( access.yale.edu/hipaa ). See our VPN page for setup instructions. If your group has a workstation (see list ), you can connect using one of those.","title":"Milgram"},{"location":"clusters-at-yale/clusters/milgram-dev/#hardware","text":"Milgram is made up of a couple kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. Warning Care should be taken if when scheduling your job if you are running programs/libraries optimized for specific hardware. See the guide on how to compile software for specific guidance.","title":"Hardware"},{"location":"clusters-at-yale/clusters/milgram-dev/#compute-node-configurations","text":"Count CPU CPU Cores RAM Features 12 2x E5-2660 v3 20 121G haswell, E5-2660_v3, oldest 48 2x E5-2660 v4 28 250G broadwell, E5-2660_v4","title":"Compute Node Configurations"},{"location":"clusters-at-yale/clusters/milgram-dev/#slurm-partitions","text":"Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The default resource requests for all jobs is 1 core and 5GB of memory per core. The short partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The long and verylong partitions are meant for jobs with projected walltimes that are too long to run in short. For courses using the cluster we set aside the education partition. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime Default/Max Node Type (count) short* 1158 CPUs, 10176 G RAM 772 CPUs, 6784 G RAM 1h/6h E5-2660_v3 (9), E5-2660_v4 (48) interactive 1 job, 4 CPUs, 20 G RAM 1h/6h E5-2660_v3 (1) long 1188 CPUs, 5940 G RAM 1h/2d E5-2660_v3 (9), E5-2660_v4 (48) verylong 792 CPUs, 3960 G RAM 1h/7d E5-2660_v3 (9), E5-2660_v4 (48) education 1h/6h E5-2660_v3 (2) scavenge none E5-2660_v3 (9), E5-2660_v4 (48) * default","title":"Slurm Partitions"},{"location":"clusters-at-yale/clusters/milgram-dev/#storage","text":"/gpfs/milgram is Milgram's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/milgram/home 20G/user 500,000 Yes project /gpfs/milgram/project varies varies No scratch60 /gpfs/milgram/scratch60 varies 5,000,000 No","title":"Storage"},{"location":"clusters-at-yale/clusters/milgram-dev/#data-fileset","text":"There is a data fileset on /gpfs/milgram designed to host shared, public datasets to prevent duplication of data across the machine. Each dataset in the data fileset has one curator who is in charge of populating the dataset, managing permissions and requesting access to that dataset. See below for a list of datasets. If you have questions about or would like read access to a specific dataset, please contact the listed curator. Dataset Directory Owner Curator ABCD /gpfs/milgram/data/ABCD TBD TBD bold5000 /gpfs/milgram/data/bold500 TBD TBD brainiak_tutorials /gpfs/milgram/data/brainiak_tutorials TBD TBD bsnip1 /gpfs/milgram/data/bsnip1 TBD TBD GSP /gpfs/milgram/data/GSP TBD TBD HCP /gpfs/milgram/data/HCP TBD TBD HealthyBrain /gpfs/milgram/data/HealthyBrain TBD TBD NKI /gpfs/milgram/data/NKI TBD TBD PartlyCloudy /gpfs/milgram/data/PartlyCloudy TBD TBD PING /gpfs/milgram/data/PING TBD TBD UKB /gpfs/milgram/data/UKB TBD TBD yale_smart /gpfs/milgram/data/yale_smart TBD TBD","title":"Data Fileset"},{"location":"clusters-at-yale/clusters/milgram-dev/#for-dataset-curators","text":"The appointed curators for each dataset has the following responsibilities.","title":"For Dataset Curators"},{"location":"clusters-at-yale/clusters/milgram-dev/#manage-datasets-and-reset-permissions","text":"The curator for a specific fileset is the only user with write access to that directory. It is their responsibility to manage the data in that directory. In order to add new data, the curator will need to temporarily grant themselves read permission to the directory where they will be populating the data: cd /gpfs/milgram/data/ [ destination_for_data ] chmod u+w . To move or clean out data, the curator will also need to temporarily grant themselves write permission for the relevant data using chmod u+w [file] or chmod -R u+w [directory] . After data management is complete, the permissions must be reset to read-only to prevent accidental contamination of the dataset. To reset the permissions, run the following command at the highest level directory where the data was manipulated: cd /gpfs/milgram/data/ [ destination_for_data ] chmod -R ugo-w .","title":"Manage Datasets and Reset Permissions"},{"location":"clusters-at-yale/clusters/milgram-dev/#request-access-for-new-user","text":"The curator is responsible for fielding access requests to their directory. Once a new user has been given permission to access the directory, the curator themselves will submit a permission change request to hpc@yale.edu.","title":"Request Access for New User"},{"location":"clusters-at-yale/clusters/milgram-dev/#appropriate-data-only","text":"The data fileset should only be used for raw datasets. Approval must be requested from ????? to add data products generated at Yale to the fileset. Intermediate data products and temporary files (such as slurm logs) should never be placed in the data directory.","title":"Appropriate Data Only"},{"location":"clusters-at-yale/clusters/milgram-workstations/","text":"Milgram Workstations Host Name Lab Location cannon1.milgram.hpc.yale.internal Cannon SSS Hall cannon2.milgram.hpc.yale.internal Cannon SSS Hall casey1.milgram.hpc.yale.internal Casey SSS Hall chang1.milgram.hpc.yale.internal Chang Dunham Lab cl1.milgram.hpc.yale.internal Chun SSS Hall cl2.milgram.hpc.yale.internal Chun SSS Hall cl3.milgram.hpc.yale.internal Chun SSS Hall crockett1.milgram.hpc.yale.internal Crockett Dunham Lab gee1.milgram.hpc.yale.internal Gee Kirtland Hall gee2.milgram.hpc.yale.internal Gee Kirtland Hall hl1.milgram.hpc.yale.internal Holmes SSS Hall hl2.milgram.hpc.yale.internal Holmes SSS Hall joormann1.milgram.hpc.yale.internal Joorman Kirtland Hall","title":"Milgram Workstations"},{"location":"clusters-at-yale/clusters/milgram-workstations/#milgram-workstations","text":"Host Name Lab Location cannon1.milgram.hpc.yale.internal Cannon SSS Hall cannon2.milgram.hpc.yale.internal Cannon SSS Hall casey1.milgram.hpc.yale.internal Casey SSS Hall chang1.milgram.hpc.yale.internal Chang Dunham Lab cl1.milgram.hpc.yale.internal Chun SSS Hall cl2.milgram.hpc.yale.internal Chun SSS Hall cl3.milgram.hpc.yale.internal Chun SSS Hall crockett1.milgram.hpc.yale.internal Crockett Dunham Lab gee1.milgram.hpc.yale.internal Gee Kirtland Hall gee2.milgram.hpc.yale.internal Gee Kirtland Hall hl1.milgram.hpc.yale.internal Holmes SSS Hall hl2.milgram.hpc.yale.internal Holmes SSS Hall joormann1.milgram.hpc.yale.internal Joorman Kirtland Hall","title":"Milgram Workstations"},{"location":"clusters-at-yale/clusters/milgram/","text":"Milgram Milgram is named for Dr. Stanley Milgram, a psychologist who researched the behavioral motivations behind social awareness in individuals and obedience to authority figures. He conducted several famous experiments during his professorship at Yale University including the lost-letter experiment, the small-world experiment, and the Milgram experiment on obedience to authority figures. Milgram is a HIPAA aligned Department of Psychology cluster intended for use on projects that may involve sensitive data. This applies to both storage and computation. If you have any questions about this policy, please contact us . Note Connections to Milgram can only be made from the Yale VPN ( access.yale.edu )--even if you are already on campus (YaleSecure or ethernet). See our VPN page for setup instructions. If your group has a workstation (see list ), you can connect using one of those. Hardware Milgram is made up of a couple kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. Warning Care should be taken when scheduling your job if you are running programs/libraries optimized for specific hardware. You can narrow which nodes can run your job by requesting the features from the Node Configurations table as constraints (slurm --constraint flag) to your job. See the Request Compute Resources page and the Build Software page for further guidance. Compute Node Configurations Count CPU CPU Cores RAM GPU vRAM/GPU Features 12 2x E5-2660 v3 20 121G haswell, E5-2660_v3, nogpu, oldest 48 2x E5-2660 v4 28 250G broadwell, E5-2660_v4, nogpu 5 2x 6240 36 372G 4x rtx2080ti 8G cascadelake, avx512, 6240, nogpu, standard Slurm Partitions Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The default resource requests for all jobs is 1 core and 5GB of memory per core. The short partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The long and verylong partitions are meant for jobs with projected walltimes that are too long to run in short. For courses using the cluster we set aside the education partition. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime Default/Max Node Type (count) interactive 1 job, 4 CPUs, 20 G RAM 1h/6h E5-2660_v3 (2) short* 1158 CPUs, 10176 G RAM 772 CPUs, 6784 G RAM 1h/6h E5-2660_v3 (9), E5-2660_v4 (48) long 1188 CPUs, 5940 G RAM 1h/2d E5-2660_v3 (9), E5-2660_v4 (48) verylong 792 CPUs, 3960 G RAM 1h/7d E5-2660_v3 (9), E5-2660_v4 (48) gpu 1h/7d 6240 w/ rtx2080ti (5) education 1h/6h E5-2660_v3 (2) scavenge none E5-2660_v3 (9), E5-2660_v4 (48), 6240 (5) * default Storage /gpfs/milgram is Milgram's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/milgram/home 20G/user 500,000 Yes project /gpfs/milgram/project varies varies No scratch60 /gpfs/milgram/scratch60 varies 5,000,000 No","title":"Milgram"},{"location":"clusters-at-yale/clusters/milgram/#milgram","text":"Milgram is named for Dr. Stanley Milgram, a psychologist who researched the behavioral motivations behind social awareness in individuals and obedience to authority figures. He conducted several famous experiments during his professorship at Yale University including the lost-letter experiment, the small-world experiment, and the Milgram experiment on obedience to authority figures. Milgram is a HIPAA aligned Department of Psychology cluster intended for use on projects that may involve sensitive data. This applies to both storage and computation. If you have any questions about this policy, please contact us . Note Connections to Milgram can only be made from the Yale VPN ( access.yale.edu )--even if you are already on campus (YaleSecure or ethernet). See our VPN page for setup instructions. If your group has a workstation (see list ), you can connect using one of those.","title":"Milgram"},{"location":"clusters-at-yale/clusters/milgram/#hardware","text":"Milgram is made up of a couple kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. Warning Care should be taken when scheduling your job if you are running programs/libraries optimized for specific hardware. You can narrow which nodes can run your job by requesting the features from the Node Configurations table as constraints (slurm --constraint flag) to your job. See the Request Compute Resources page and the Build Software page for further guidance.","title":"Hardware"},{"location":"clusters-at-yale/clusters/milgram/#compute-node-configurations","text":"Count CPU CPU Cores RAM GPU vRAM/GPU Features 12 2x E5-2660 v3 20 121G haswell, E5-2660_v3, nogpu, oldest 48 2x E5-2660 v4 28 250G broadwell, E5-2660_v4, nogpu 5 2x 6240 36 372G 4x rtx2080ti 8G cascadelake, avx512, 6240, nogpu, standard","title":"Compute Node Configurations"},{"location":"clusters-at-yale/clusters/milgram/#slurm-partitions","text":"Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The default resource requests for all jobs is 1 core and 5GB of memory per core. The short partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The long and verylong partitions are meant for jobs with projected walltimes that are too long to run in short. For courses using the cluster we set aside the education partition. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime Default/Max Node Type (count) interactive 1 job, 4 CPUs, 20 G RAM 1h/6h E5-2660_v3 (2) short* 1158 CPUs, 10176 G RAM 772 CPUs, 6784 G RAM 1h/6h E5-2660_v3 (9), E5-2660_v4 (48) long 1188 CPUs, 5940 G RAM 1h/2d E5-2660_v3 (9), E5-2660_v4 (48) verylong 792 CPUs, 3960 G RAM 1h/7d E5-2660_v3 (9), E5-2660_v4 (48) gpu 1h/7d 6240 w/ rtx2080ti (5) education 1h/6h E5-2660_v3 (2) scavenge none E5-2660_v3 (9), E5-2660_v4 (48), 6240 (5) * default","title":"Slurm Partitions"},{"location":"clusters-at-yale/clusters/milgram/#storage","text":"/gpfs/milgram is Milgram's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/milgram/home 20G/user 500,000 Yes project /gpfs/milgram/project varies varies No scratch60 /gpfs/milgram/scratch60 varies 5,000,000 No","title":"Storage"},{"location":"clusters-at-yale/clusters/omega/","text":"Omega Warning Omega has been permanently decommissioned as of Dec 6, 2019. Omega's home directories will purged on Jan 31, 2020. Omega's scratch space was permanently deleted on Feb 1, 2019. Omega has now served Yale\u2019s research community well for more than 2 years past the normal end-of-life for similar clusters. Most of its components are no longer under vendor warranty, and parts are sometimes difficult to obtain, so we are forced to support it on a best-effort basis. Last year, we developed a multi-year plan to replace Omega, which began with moving Omega\u2019s shared resources to our Grace cluster, for which we acquired new commons nodes. We plan to continue to support groups with dedicated node allocations and other users running tightly-coupled parallel jobs on Omega until Mid 2019. The mpi partition on Grace contains the replacement nodes the remainder of Omega. Please test your workload on those nodes are your convenience. We will provide ample warning before the final Omega decommission. Hardware The cluster is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. Compute Node Configurations Count CPU CPU Cores RAM Features 668 X5560 8 32G nehalem,sse4_2,X5560 32 X5560 8 44G nehalem,sse4_2,X5560,extramem 4 E5-2650 16 121G sandybridge,sse4_2,avx,E5-2650 60 E5-2620 12 59G sandybridge,sse4_2,avx,E5-2620 Slurm Partitions Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The default resource requests for all jobs is 1 core and 4GB of memory. Public Partitions The day partition is where most batch jobs should run, and is the default if you don't specify a partition. The week partition is smaller, but allows for longer jobs. The interactive partition should only be used for testing or compiling software. The bigmem partition contains our largest memory node; only jobs that cannot be satisfied by day should run here. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition User Limits Walltime default/max Node Type (count) day* 128 nodes 1h/1d X5560 (218) week 64 nodes 1h/7d X5560 (46), X5560 44G (16) interactive 1 job, 8 CPUs, 1 node 1h/4h X5560 (2) shared** 1h/1d X5560 (2) bigmem 1h/1d E5-2650 (4) scavenge 1h/7d all * default partition ** The shared partition is for jobs that require less than a full node of cores Private Partitions Private partitions contain nodes acquired by specific research groups. Full access to these partitions is granted at the discretion of the owner. Contact us if your group would like to purchase nodes. Partition Walltime default/max Node Type (count) astro 1h/28d X5560 (112), X5560 44G (16) geo 1h/7d X5560 (207) hep 1h/7d X5560 (47) esi 1h/28d E5-2620 (60) Storage /gpfs/loomis is Omega's primary filesystem where home, and scratch60 directories are located. You can also access Grace's project space (if you have a Grace account) from Omega. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/loomis/home.omega 300G/group 500,000 Yes scratch60 /gpfs/loomis/scratch60 20T/group 5,000,000 No","title":"Omega"},{"location":"clusters-at-yale/clusters/omega/#omega","text":"Warning Omega has been permanently decommissioned as of Dec 6, 2019. Omega's home directories will purged on Jan 31, 2020. Omega's scratch space was permanently deleted on Feb 1, 2019. Omega has now served Yale\u2019s research community well for more than 2 years past the normal end-of-life for similar clusters. Most of its components are no longer under vendor warranty, and parts are sometimes difficult to obtain, so we are forced to support it on a best-effort basis. Last year, we developed a multi-year plan to replace Omega, which began with moving Omega\u2019s shared resources to our Grace cluster, for which we acquired new commons nodes. We plan to continue to support groups with dedicated node allocations and other users running tightly-coupled parallel jobs on Omega until Mid 2019. The mpi partition on Grace contains the replacement nodes the remainder of Omega. Please test your workload on those nodes are your convenience. We will provide ample warning before the final Omega decommission.","title":"Omega"},{"location":"clusters-at-yale/clusters/omega/#hardware","text":"The cluster is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs.","title":"Hardware"},{"location":"clusters-at-yale/clusters/omega/#compute-node-configurations","text":"Count CPU CPU Cores RAM Features 668 X5560 8 32G nehalem,sse4_2,X5560 32 X5560 8 44G nehalem,sse4_2,X5560,extramem 4 E5-2650 16 121G sandybridge,sse4_2,avx,E5-2650 60 E5-2620 12 59G sandybridge,sse4_2,avx,E5-2620","title":"Compute Node Configurations"},{"location":"clusters-at-yale/clusters/omega/#slurm-partitions","text":"Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The default resource requests for all jobs is 1 core and 4GB of memory.","title":"Slurm Partitions"},{"location":"clusters-at-yale/clusters/omega/#public-partitions","text":"The day partition is where most batch jobs should run, and is the default if you don't specify a partition. The week partition is smaller, but allows for longer jobs. The interactive partition should only be used for testing or compiling software. The bigmem partition contains our largest memory node; only jobs that cannot be satisfied by day should run here. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition User Limits Walltime default/max Node Type (count) day* 128 nodes 1h/1d X5560 (218) week 64 nodes 1h/7d X5560 (46), X5560 44G (16) interactive 1 job, 8 CPUs, 1 node 1h/4h X5560 (2) shared** 1h/1d X5560 (2) bigmem 1h/1d E5-2650 (4) scavenge 1h/7d all * default partition ** The shared partition is for jobs that require less than a full node of cores","title":"Public Partitions"},{"location":"clusters-at-yale/clusters/omega/#private-partitions","text":"Private partitions contain nodes acquired by specific research groups. Full access to these partitions is granted at the discretion of the owner. Contact us if your group would like to purchase nodes. Partition Walltime default/max Node Type (count) astro 1h/28d X5560 (112), X5560 44G (16) geo 1h/7d X5560 (207) hep 1h/7d X5560 (47) esi 1h/28d E5-2620 (60)","title":"Private Partitions"},{"location":"clusters-at-yale/clusters/omega/#storage","text":"/gpfs/loomis is Omega's primary filesystem where home, and scratch60 directories are located. You can also access Grace's project space (if you have a Grace account) from Omega. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/loomis/home.omega 300G/group 500,000 Yes scratch60 /gpfs/loomis/scratch60 20T/group 5,000,000 No","title":"Storage"},{"location":"clusters-at-yale/clusters/ruddle/","text":"Ruddle Ruddle is named for Frank Ruddle , a Yale geneticist who was a pioneer in genetic engineering and the study of developmental genetics. Ruddle is intended for use only on projects related to the Yale Center for Genome Analysis ; Please do not use this cluster for other projects. If you have any questions about this policy, please contact us . Hardware Ruddle is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. Warning Care should be taken when scheduling your job if you are running programs/libraries optimized for specific hardware. You can narrow which nodes can run your job by requesting the features from the Node Configurations table as constraints (slurm --constraint flag) to your job. See the Request Compute Resources page and the Build Software page for further guidance. Compute Node Configurations Count CPU CPU Cores RAM Features 155 2x E5-2660 v3 20 121G haswell, avx2, E5-2660_v3, oldest 2 4x E7-4809 v3 32 1507G haswell, avx2, E7-4809_v3 Slurm Partitions Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The default resource requests for all jobs is 1 core and 5GB of memory per core. The general partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The bigmem partition contains our largest memory nodes; only jobs that cannot be satisfied by general should run here. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime Default/Max Node Type (count) general* 400 CPUs, 2000 G RAM 300 CPUs, 1800 G RAM 7d/30d E5-2660_v3 (155) interactive 20 CPUs, 256 G RAM 1d/2d E5-2660_v3 (155) bigmem 32 CPUs, 1507 G RAM 1d/7d E7-4809_v3 1507G (2) scavenge 800 CPUs, 5120 G RAM 1d/7d all * default Access Sequencing Data To avoid duplication of data and to save space that counts against your quotas, we suggest that you make soft links to your sequencing data rather than copying them. Normally, YCGA will send you an email informing you that your data is ready, and will include a url that looks like: http://fcb.ycga.yale.edu:3010/ randomstring /sample_dir_001 You can use that link to download your data in a browser, but if you plan to process the data on Ruddle, it is better to make a soft link to the data, rather than copying it. You can use the ycgaFastq tool to do that: $ /home/bioinfo/software/knightlab/bin_Mar2018/ycgaFastq fcb.ycga.yale.edu:3010/randomstring/sample_dir_001 If you would like to know the true location of the data on Ruddle, do this: $ cd /ycga-gpfs/project/fas/lsprog/tools/external/data/randomstring/sample_dir_001 $ ls -l Tip Original sequence data are archived pursuant to the YCGA retention policy. For long-running projects we recommend you keep a personal backup of your sequence files. If you need to retrieve archived sequencing data, please see our guide on how to do so . If you have a very old link from YCGA that doesn't use the random string, you can find the location by decoding the url as shown below: fullPath Starts With Root Path on Ruddle gpfs_illumina/sequencer /gpfs/ycga/illumina/sequencer ba_sequencers /ycga-ba/ba_sequencers sequencers /gpfs/ycga/sequencers/panfs/sequencers For example, if the sample link you received is: http://sysg1.cs.yale.edu:2011/gen?fullPath=sequencers2/sequencerV/runs/131107_D00306_0096... etc The path on the cluster to the data is: /gpfs/ycga/sequencers/panfs/sequencers2/sequencerV/runs/131107_D00306_0096... etc Public Datasets We host datasets of general interest in a loosely organized directory tree in /gpfs/ycga/datasets : \u251c\u2500\u2500 annovar \u2502 \u2514\u2500\u2500 humandb \u251c\u2500\u2500 db \u2502 \u2514\u2500\u2500 blast \u251c\u2500\u2500 genomes \u2502 \u251c\u2500\u2500 Aedes_aegypti \u2502 \u251c\u2500\u2500 Bos_taurus \u2502 \u251c\u2500\u2500 Chelonoidis_nigra \u2502 \u251c\u2500\u2500 Danio_rerio \u2502 \u251c\u2500\u2500 Gallus_gallus \u2502 \u251c\u2500\u2500 hisat2 \u2502 \u251c\u2500\u2500 Homo_sapiens \u2502 \u251c\u2500\u2500 Macaca_mulatta \u2502 \u251c\u2500\u2500 Monodelphis_domestica \u2502 \u251c\u2500\u2500 Mus_musculus \u2502 \u251c\u2500\u2500 PhiX \u2502 \u2514\u2500\u2500 tmp \u2514\u2500\u2500 hisat2 \u2514\u2500\u2500 mouse If you would like us to host a dataset or questions about what is currently available, please email hpc@yale.edu. Storage Ruddle's filesystem, /gpfs/ycga , is where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. Ruddle's legacy filesystem, /ycga-ba , was retired. See here for details. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily.. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/ycga/home 125G/user 500,000 Yes project /gpfs/ycga/project 4T/group 5,000,000 No scratch60 /gpfs/ycga/scratch60 10T/group 5,000,000 No","title":"Ruddle"},{"location":"clusters-at-yale/clusters/ruddle/#ruddle","text":"Ruddle is named for Frank Ruddle , a Yale geneticist who was a pioneer in genetic engineering and the study of developmental genetics. Ruddle is intended for use only on projects related to the Yale Center for Genome Analysis ; Please do not use this cluster for other projects. If you have any questions about this policy, please contact us .","title":"Ruddle"},{"location":"clusters-at-yale/clusters/ruddle/#hardware","text":"Ruddle is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. Warning Care should be taken when scheduling your job if you are running programs/libraries optimized for specific hardware. You can narrow which nodes can run your job by requesting the features from the Node Configurations table as constraints (slurm --constraint flag) to your job. See the Request Compute Resources page and the Build Software page for further guidance.","title":"Hardware"},{"location":"clusters-at-yale/clusters/ruddle/#compute-node-configurations","text":"Count CPU CPU Cores RAM Features 155 2x E5-2660 v3 20 121G haswell, avx2, E5-2660_v3, oldest 2 4x E7-4809 v3 32 1507G haswell, avx2, E7-4809_v3","title":"Compute Node Configurations"},{"location":"clusters-at-yale/clusters/ruddle/#slurm-partitions","text":"Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The default resource requests for all jobs is 1 core and 5GB of memory per core. The general partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The bigmem partition contains our largest memory nodes; only jobs that cannot be satisfied by general should run here. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime Default/Max Node Type (count) general* 400 CPUs, 2000 G RAM 300 CPUs, 1800 G RAM 7d/30d E5-2660_v3 (155) interactive 20 CPUs, 256 G RAM 1d/2d E5-2660_v3 (155) bigmem 32 CPUs, 1507 G RAM 1d/7d E7-4809_v3 1507G (2) scavenge 800 CPUs, 5120 G RAM 1d/7d all * default","title":"Slurm Partitions"},{"location":"clusters-at-yale/clusters/ruddle/#access-sequencing-data","text":"To avoid duplication of data and to save space that counts against your quotas, we suggest that you make soft links to your sequencing data rather than copying them. Normally, YCGA will send you an email informing you that your data is ready, and will include a url that looks like: http://fcb.ycga.yale.edu:3010/ randomstring /sample_dir_001 You can use that link to download your data in a browser, but if you plan to process the data on Ruddle, it is better to make a soft link to the data, rather than copying it. You can use the ycgaFastq tool to do that: $ /home/bioinfo/software/knightlab/bin_Mar2018/ycgaFastq fcb.ycga.yale.edu:3010/randomstring/sample_dir_001 If you would like to know the true location of the data on Ruddle, do this: $ cd /ycga-gpfs/project/fas/lsprog/tools/external/data/randomstring/sample_dir_001 $ ls -l Tip Original sequence data are archived pursuant to the YCGA retention policy. For long-running projects we recommend you keep a personal backup of your sequence files. If you need to retrieve archived sequencing data, please see our guide on how to do so . If you have a very old link from YCGA that doesn't use the random string, you can find the location by decoding the url as shown below: fullPath Starts With Root Path on Ruddle gpfs_illumina/sequencer /gpfs/ycga/illumina/sequencer ba_sequencers /ycga-ba/ba_sequencers sequencers /gpfs/ycga/sequencers/panfs/sequencers For example, if the sample link you received is: http://sysg1.cs.yale.edu:2011/gen?fullPath=sequencers2/sequencerV/runs/131107_D00306_0096... etc The path on the cluster to the data is: /gpfs/ycga/sequencers/panfs/sequencers2/sequencerV/runs/131107_D00306_0096... etc","title":"Access Sequencing Data"},{"location":"clusters-at-yale/clusters/ruddle/#public-datasets","text":"We host datasets of general interest in a loosely organized directory tree in /gpfs/ycga/datasets : \u251c\u2500\u2500 annovar \u2502 \u2514\u2500\u2500 humandb \u251c\u2500\u2500 db \u2502 \u2514\u2500\u2500 blast \u251c\u2500\u2500 genomes \u2502 \u251c\u2500\u2500 Aedes_aegypti \u2502 \u251c\u2500\u2500 Bos_taurus \u2502 \u251c\u2500\u2500 Chelonoidis_nigra \u2502 \u251c\u2500\u2500 Danio_rerio \u2502 \u251c\u2500\u2500 Gallus_gallus \u2502 \u251c\u2500\u2500 hisat2 \u2502 \u251c\u2500\u2500 Homo_sapiens \u2502 \u251c\u2500\u2500 Macaca_mulatta \u2502 \u251c\u2500\u2500 Monodelphis_domestica \u2502 \u251c\u2500\u2500 Mus_musculus \u2502 \u251c\u2500\u2500 PhiX \u2502 \u2514\u2500\u2500 tmp \u2514\u2500\u2500 hisat2 \u2514\u2500\u2500 mouse If you would like us to host a dataset or questions about what is currently available, please email hpc@yale.edu.","title":"Public Datasets"},{"location":"clusters-at-yale/clusters/ruddle/#storage","text":"Ruddle's filesystem, /gpfs/ycga , is where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. Ruddle's legacy filesystem, /ycga-ba , was retired. See here for details. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily.. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/ycga/home 125G/user 500,000 Yes project /gpfs/ycga/project 4T/group 5,000,000 No scratch60 /gpfs/ycga/scratch60 10T/group 5,000,000 No","title":"Storage"},{"location":"clusters-at-yale/clusters/ycga-ba/","text":"Notice of decommissioning of ycga-ba filesystem Dear Ruddle Users, We retired the old storage system mounted at /ycga-ba on September 16, 2019. All YCGA-generated primary sequence data on /ycga-ba were already archived to tape and removed. Please note that no other data were migrated to any other storage system. This shutdown impacted all locations on /ycga-ba, including: /ycga-ba/sequencers[123]/scratch /ycga-ba/labs /ycga-ba/lifton /ycga-ba/state /ycga-ba/state-bio /ycga-ba/data1 /ycga-ba/data2","title":"Notice of decommissioning of ycga-ba filesystem"},{"location":"clusters-at-yale/clusters/ycga-ba/#notice-of-decommissioning-of-ycga-ba-filesystem","text":"Dear Ruddle Users, We retired the old storage system mounted at /ycga-ba on September 16, 2019. All YCGA-generated primary sequence data on /ycga-ba were already archived to tape and removed. Please note that no other data were migrated to any other storage system. This shutdown impacted all locations on /ycga-ba, including: /ycga-ba/sequencers[123]/scratch /ycga-ba/labs /ycga-ba/lifton /ycga-ba/state /ycga-ba/state-bio /ycga-ba/data1 /ycga-ba/data2","title":"Notice of decommissioning of ycga-ba filesystem"},{"location":"clusters-at-yale/data/","text":"Overview Each cluster has a small amount of space dedicated to home directories, meant for notes and scripts. There are also project and scratch spaces for larger and more numerous files. Each group is given a free allocation in each of these storage locations (see the individual cluster pages for details). Contact us at hpc@yale.edu about purchasing additional cluster storage if your needs exceed your free allocation. Other Storage Options If you or your group finds the HPC storage do not accommodate your needs, please see the off-cluster research data storage page for other options.","title":"Overview"},{"location":"clusters-at-yale/data/#overview","text":"Each cluster has a small amount of space dedicated to home directories, meant for notes and scripts. There are also project and scratch spaces for larger and more numerous files. Each group is given a free allocation in each of these storage locations (see the individual cluster pages for details). Contact us at hpc@yale.edu about purchasing additional cluster storage if your needs exceed your free allocation.","title":"Overview"},{"location":"clusters-at-yale/data/#other-storage-options","text":"If you or your group finds the HPC storage do not accommodate your needs, please see the off-cluster research data storage page for other options.","title":"Other Storage Options"},{"location":"clusters-at-yale/data/archived-sequencing/","text":"Ruddle Archived Sequence Data Retrieve Data from the Archive In the sequencing archive on Ruddle , a directory exists for each run, holding one or more tar files. There is a main tar file, plus a tar file for each project directory. Most users only need the project tar file corresponding to their data. Although the archive actually exists on tape, you can treat it as a regular directory tree. Many operations such as ls , cd , etc. are very fast, since directory structures and file metadata are on a disk cache. However, when you actually read the contents of files the tape is mounted and the file is read into a disk cache. Archived runs are stored in the following locations. Original location Archive location /panfs/sequencers /SAY/archive/YCGA-729009-YCGA/archive/panfs/sequencers /ycga-ba/ba_sequencers /SAY/archive/YCGA-729009-YCGA/archive/ycga-ba/ba_sequencers /gpfs/ycga/sequencers/illumina/sequencers /SAY/archive/YCGA-729009-YCGA/archive/ycga-gpfs/sequencers/illumina/sequencers You can directly copy or untar the project tarfile into a scratch directory. cd ~/scratch60/somedir tar \u2013xvf /SAY/archive/YCGA-729009-YCGA/archive/path/to/file.tar Inside the project tar files are the fastq files, which have been compressed using quip . If your pipeline cannot read quip files directly, you will need to uncompress them before using them. module load Quip quip \u2013d M20_ACAGTG_L008_R1_009.fastq.qp For your convenience, we have a tool, restore , that will download a tar file, untar it, and uncompress all quip files. module load ycga-public restore \u2013t /SAY/archive/YCGA-729009-YCGA/archive/path/to/file.tar If you have trouble locating your files, you can use the utility locateRun , using any substring of the original run name. locateRun is in the same module as restore. locateRun C9374AN Restore spends most of the time running quip. You can parallelize and thereby speed up that process using the -n flag. restore \u2013n 20 ... Tip When retrieving data, run untar/unquip as a job on a compute node, not a login node and make sure to allocate sufficient resources to your job, e.g. \u2013c 20 --mem=100G .","title":"Ruddle Archived Sequence Data"},{"location":"clusters-at-yale/data/archived-sequencing/#ruddle-archived-sequence-data","text":"","title":"Ruddle Archived Sequence Data"},{"location":"clusters-at-yale/data/archived-sequencing/#retrieve-data-from-the-archive","text":"In the sequencing archive on Ruddle , a directory exists for each run, holding one or more tar files. There is a main tar file, plus a tar file for each project directory. Most users only need the project tar file corresponding to their data. Although the archive actually exists on tape, you can treat it as a regular directory tree. Many operations such as ls , cd , etc. are very fast, since directory structures and file metadata are on a disk cache. However, when you actually read the contents of files the tape is mounted and the file is read into a disk cache. Archived runs are stored in the following locations. Original location Archive location /panfs/sequencers /SAY/archive/YCGA-729009-YCGA/archive/panfs/sequencers /ycga-ba/ba_sequencers /SAY/archive/YCGA-729009-YCGA/archive/ycga-ba/ba_sequencers /gpfs/ycga/sequencers/illumina/sequencers /SAY/archive/YCGA-729009-YCGA/archive/ycga-gpfs/sequencers/illumina/sequencers You can directly copy or untar the project tarfile into a scratch directory. cd ~/scratch60/somedir tar \u2013xvf /SAY/archive/YCGA-729009-YCGA/archive/path/to/file.tar Inside the project tar files are the fastq files, which have been compressed using quip . If your pipeline cannot read quip files directly, you will need to uncompress them before using them. module load Quip quip \u2013d M20_ACAGTG_L008_R1_009.fastq.qp For your convenience, we have a tool, restore , that will download a tar file, untar it, and uncompress all quip files. module load ycga-public restore \u2013t /SAY/archive/YCGA-729009-YCGA/archive/path/to/file.tar If you have trouble locating your files, you can use the utility locateRun , using any substring of the original run name. locateRun is in the same module as restore. locateRun C9374AN Restore spends most of the time running quip. You can parallelize and thereby speed up that process using the -n flag. restore \u2013n 20 ... Tip When retrieving data, run untar/unquip as a job on a compute node, not a login node and make sure to allocate sufficient resources to your job, e.g. \u2013c 20 --mem=100G .","title":"Retrieve Data from the Archive"},{"location":"clusters-at-yale/data/cluster-storage/","text":"Cluster Storage Each research group is provided with storage space for research data. The storage is separated into three tiers: home, project, and scratch. You can monitor your storage usage by running the getquota command on a cluster. Except for on the Milgram cluster, no sensitive data can be stored on any cluster storage. Warning The only storage backed up on every cluster is home . Please see our HPC Policies page for additional information about backups HPC Storage Locations Home Home storage (backed-up daily) is a small amount of personal space to store your own scripts, notes, final products (e.g. figures), etc. Your home storage is for you only: please do not share this storage with other people. Home storage is backed up daily. When running the getquota command, the usage and file count values for home are for your own usage. Project Project storage (not backed-up!), intended to be the primary storage location for HPC research data in active use, is shared by your entire research group. Project space is available to all the members of our group via the project link in the home directories, or via the absolute path: /gpfs/<filesystem>/project/<group>/<netid> 60-Day Scratch ( scratch60 ) Scratch storage (not backed-up!), intended to be the storage location for temporary data, is shared by your entire research group. Scratch space is best used for intermediate files that can be regenerated/reconstituted if necessary. Files older than 60 days will be deleted automatically , ..and you may be asked to delete files younger than 60 days old if this space fills up. Scratch space is available via the scratch60 link in your home directory, or via the absolute path: /gpfs/<filesystem>/scratch60/<group>/<netid> . HPC Storage Best Practices Staging Data Large datasets are often stored off-site, either on departmental servers or Storage@Yale. Efficient processing of these data can be achieved through temporary staging on the high-performance parallel filesystem. The permanent copy of the data remains on the off-site storage, while a working copy is placed in scrach60 , for example. These data can then be processed efficiently and the output files transmitted back to the off-site storage. A sample workflow would be: # copy data to temporary cluster storage [ netID@cluster ~ ] $ rsync -avP netID@department_server:/path/to/data $HOME /scratch60/ # process data on cluster [ netID@cluster ~ ] $ sbatch data_processing.sh # return results to permanent storage for safe-keeping [ netID@cluster ~ ] $ rsync -avP $HOME /scratch60/output_data netID@department_server:/path/to/outputs/ The working copy of the data can then be removed manually or left to be deleted when it reaches the 60-day limit. Prevent Large Numbers of Small Files Parallel filesystems, like the ones attached to our clusters, perform poorly with very large numbers of small files. For this reason, there are file count limits on all accounts to provide a safety net against excessive file creation. However, we expect users to manage their own file counts by altering workflows to reduce file creation, deleting unneeded files, and compressing (using tar ) collections of files no longer in use. Backups and Snapshots Retrieve Data from Home Backups Contact us at hpc@yale.edu with your netid and the list of files/directories you would like restored. Retrieve Data from Snapshots (Milgram) Milgram runs snapshots nightly on portions of the filesystem so that you can retrieve mistakenly modified or removed files for yourself. As long as your files existed in the form you want them in before the most recent midnight, they can probably be recovered. Snapshot directory structure mirrors the files that are being tracked with a prefix, listed in the table below. File set Snapshot Prefix /gpfs/slayman/pi/gerstein /gpfs/slayman/pi/gerstein/.snapshots /gpfs/milgram/home /gpfs/milgram/home/.snapshots /gpfs/milgram/project /gpfs/milgram/project/groupname/.snapshots For example, if you wanted to recover the file /gpfs/milgram/project/bjornson/rdb9/doit.sh (a file in the bjornson group's project directory owned by rdb9) it would be found at /gpfs/milgram/project/bjornson/.snapshots/$(date +%Y%m%d-0000)/rdb9/doit.sh . Info Because of the way snapshots are stored, sizes will not be correctly reported until you copy your files/directories back out of the .snapshots directory.","title":"Cluster Storage"},{"location":"clusters-at-yale/data/cluster-storage/#cluster-storage","text":"Each research group is provided with storage space for research data. The storage is separated into three tiers: home, project, and scratch. You can monitor your storage usage by running the getquota command on a cluster. Except for on the Milgram cluster, no sensitive data can be stored on any cluster storage. Warning The only storage backed up on every cluster is home . Please see our HPC Policies page for additional information about backups","title":"Cluster Storage"},{"location":"clusters-at-yale/data/cluster-storage/#hpc-storage-locations","text":"","title":"HPC Storage Locations"},{"location":"clusters-at-yale/data/cluster-storage/#home","text":"Home storage (backed-up daily) is a small amount of personal space to store your own scripts, notes, final products (e.g. figures), etc. Your home storage is for you only: please do not share this storage with other people. Home storage is backed up daily. When running the getquota command, the usage and file count values for home are for your own usage.","title":"Home"},{"location":"clusters-at-yale/data/cluster-storage/#project","text":"Project storage (not backed-up!), intended to be the primary storage location for HPC research data in active use, is shared by your entire research group. Project space is available to all the members of our group via the project link in the home directories, or via the absolute path: /gpfs/<filesystem>/project/<group>/<netid>","title":"Project"},{"location":"clusters-at-yale/data/cluster-storage/#60-day-scratch-scratch60","text":"Scratch storage (not backed-up!), intended to be the storage location for temporary data, is shared by your entire research group. Scratch space is best used for intermediate files that can be regenerated/reconstituted if necessary. Files older than 60 days will be deleted automatically , ..and you may be asked to delete files younger than 60 days old if this space fills up. Scratch space is available via the scratch60 link in your home directory, or via the absolute path: /gpfs/<filesystem>/scratch60/<group>/<netid> .","title":"60-Day Scratch (scratch60)"},{"location":"clusters-at-yale/data/cluster-storage/#hpc-storage-best-practices","text":"","title":"HPC Storage Best Practices"},{"location":"clusters-at-yale/data/cluster-storage/#staging-data","text":"Large datasets are often stored off-site, either on departmental servers or Storage@Yale. Efficient processing of these data can be achieved through temporary staging on the high-performance parallel filesystem. The permanent copy of the data remains on the off-site storage, while a working copy is placed in scrach60 , for example. These data can then be processed efficiently and the output files transmitted back to the off-site storage. A sample workflow would be: # copy data to temporary cluster storage [ netID@cluster ~ ] $ rsync -avP netID@department_server:/path/to/data $HOME /scratch60/ # process data on cluster [ netID@cluster ~ ] $ sbatch data_processing.sh # return results to permanent storage for safe-keeping [ netID@cluster ~ ] $ rsync -avP $HOME /scratch60/output_data netID@department_server:/path/to/outputs/ The working copy of the data can then be removed manually or left to be deleted when it reaches the 60-day limit.","title":"Staging Data"},{"location":"clusters-at-yale/data/cluster-storage/#prevent-large-numbers-of-small-files","text":"Parallel filesystems, like the ones attached to our clusters, perform poorly with very large numbers of small files. For this reason, there are file count limits on all accounts to provide a safety net against excessive file creation. However, we expect users to manage their own file counts by altering workflows to reduce file creation, deleting unneeded files, and compressing (using tar ) collections of files no longer in use.","title":"Prevent Large Numbers of Small Files"},{"location":"clusters-at-yale/data/cluster-storage/#backups-and-snapshots","text":"","title":"Backups and Snapshots"},{"location":"clusters-at-yale/data/cluster-storage/#retrieve-data-from-home-backups","text":"Contact us at hpc@yale.edu with your netid and the list of files/directories you would like restored.","title":"Retrieve Data from Home Backups"},{"location":"clusters-at-yale/data/cluster-storage/#retrieve-data-from-snapshots-milgram","text":"Milgram runs snapshots nightly on portions of the filesystem so that you can retrieve mistakenly modified or removed files for yourself. As long as your files existed in the form you want them in before the most recent midnight, they can probably be recovered. Snapshot directory structure mirrors the files that are being tracked with a prefix, listed in the table below. File set Snapshot Prefix /gpfs/slayman/pi/gerstein /gpfs/slayman/pi/gerstein/.snapshots /gpfs/milgram/home /gpfs/milgram/home/.snapshots /gpfs/milgram/project /gpfs/milgram/project/groupname/.snapshots For example, if you wanted to recover the file /gpfs/milgram/project/bjornson/rdb9/doit.sh (a file in the bjornson group's project directory owned by rdb9) it would be found at /gpfs/milgram/project/bjornson/.snapshots/$(date +%Y%m%d-0000)/rdb9/doit.sh . Info Because of the way snapshots are stored, sizes will not be correctly reported until you copy your files/directories back out of the .snapshots directory.","title":"Retrieve Data from Snapshots (Milgram)"},{"location":"clusters-at-yale/data/globus/","text":"Large Transfers with Globus For large data transfers both within Yale and to external collaborators, we recommend using Globus. Globus is a file transfer service that is efficient and easy to use. It has several advantages: Robust and fast transfers of large files and/or large collections of files. Files can be transferred between your computer and the clusters. Files can be transferred between Yale and other sites. A web and command-line interface for starting and monitoring transfers. Access to specific files or directories granted to external collaborators in a secure way. Globus transfers data between computers set up as \"endpoints\". The official YCRC endpoints are listed below. Transfers can be to and from these endpoints or those you have defined for yourself with Globus Connect . Cluster Endpoints We currently support endpoints for the following clusters. Cluster Globus Endpoint Farnam yale#farnam Grace yale#grace Ruddle yale#ruddle These endpoints provide access to all files you normally have access to, except sequencing data on Ruddle. Get Started with Globus In a browser, go to app.globus.org . Use the pull-down menu to select Yale and click \"Continue\". If you are not already logged into CAS, you will be prompted to log in. [First login only] Do not associate with another account yet unless you are familiar with doing this [First login only] Select \"non-profit research or educational purposes\" [First login only] Click on \"Allow\" for allowing Globus Web App From the file manager interface enter the name of the endpoint you would like to browse in the collection field (e.g. yale#farnam) Click on the right-hand side menu option \"Transfer or Sync to...\" Enter the second endpoint name in the right search box (e.g. another cluster or your personal endpoint) Select one or more files you would like to transfer and click the appropriate start button on the bottom. Sharing Data Using Globus From the file manager interface enter the name of the endpoint you would like to share from in the collection field (e.g. yale#grace) Click the Share button on the right Click on \"Add a Shared Endpoint\" Next to Path, click \"Browse\" to find and select the directory you want to share Add other details as desired and click on \"Create Share\" Click on \"Add Permissions -- Share With\" Under \"Username or Email\" enter the e-mail address of the person that you want to share the data with, then click on \"Save\", then click on \"Add Permission\" Do not select \"write\" unless you want the person you are sharing the data with to be able to write to the share. For more information, please see the official Globus Documentation . Setup an Endpoint on Your Computer You can set up your own endpoint for transferring data to and from your own computer with Globus Connect . Usually you will want to use Globus Connect Personal. Setup a Google Drive Endpoint See our Google Drive Documentation for instructions for using Globus to transfer data to EliApps Google Drive.","title":"Large Transfers with Globus"},{"location":"clusters-at-yale/data/globus/#large-transfers-with-globus","text":"For large data transfers both within Yale and to external collaborators, we recommend using Globus. Globus is a file transfer service that is efficient and easy to use. It has several advantages: Robust and fast transfers of large files and/or large collections of files. Files can be transferred between your computer and the clusters. Files can be transferred between Yale and other sites. A web and command-line interface for starting and monitoring transfers. Access to specific files or directories granted to external collaborators in a secure way. Globus transfers data between computers set up as \"endpoints\". The official YCRC endpoints are listed below. Transfers can be to and from these endpoints or those you have defined for yourself with Globus Connect .","title":"Large Transfers with Globus"},{"location":"clusters-at-yale/data/globus/#cluster-endpoints","text":"We currently support endpoints for the following clusters. Cluster Globus Endpoint Farnam yale#farnam Grace yale#grace Ruddle yale#ruddle These endpoints provide access to all files you normally have access to, except sequencing data on Ruddle.","title":"Cluster Endpoints"},{"location":"clusters-at-yale/data/globus/#get-started-with-globus","text":"In a browser, go to app.globus.org . Use the pull-down menu to select Yale and click \"Continue\". If you are not already logged into CAS, you will be prompted to log in. [First login only] Do not associate with another account yet unless you are familiar with doing this [First login only] Select \"non-profit research or educational purposes\" [First login only] Click on \"Allow\" for allowing Globus Web App From the file manager interface enter the name of the endpoint you would like to browse in the collection field (e.g. yale#farnam) Click on the right-hand side menu option \"Transfer or Sync to...\" Enter the second endpoint name in the right search box (e.g. another cluster or your personal endpoint) Select one or more files you would like to transfer and click the appropriate start button on the bottom.","title":"Get Started with Globus"},{"location":"clusters-at-yale/data/globus/#sharing-data-using-globus","text":"From the file manager interface enter the name of the endpoint you would like to share from in the collection field (e.g. yale#grace) Click the Share button on the right Click on \"Add a Shared Endpoint\" Next to Path, click \"Browse\" to find and select the directory you want to share Add other details as desired and click on \"Create Share\" Click on \"Add Permissions -- Share With\" Under \"Username or Email\" enter the e-mail address of the person that you want to share the data with, then click on \"Save\", then click on \"Add Permission\" Do not select \"write\" unless you want the person you are sharing the data with to be able to write to the share. For more information, please see the official Globus Documentation .","title":"Sharing Data Using Globus"},{"location":"clusters-at-yale/data/globus/#setup-an-endpoint-on-your-computer","text":"You can set up your own endpoint for transferring data to and from your own computer with Globus Connect . Usually you will want to use Globus Connect Personal.","title":"Setup an Endpoint on Your Computer"},{"location":"clusters-at-yale/data/globus/#setup-a-google-drive-endpoint","text":"See our Google Drive Documentation for instructions for using Globus to transfer data to EliApps Google Drive.","title":"Setup a Google Drive Endpoint"},{"location":"clusters-at-yale/data/permissions/","text":"Manage Permissions for Sharing Home Directories Do not give your home directory group write permissions. This will break your ability to log into the cluster. If you need to share something in your home directory, either move it your project directory or ask hpc@yale.edu for assistance. Shared Group Directories Upon request we can setup directories for sharing scripts or data across your research group. These directories can either have read-only permissions for the group (so no one accidentally modifies something) or read and write permissions for shared data directories. If interested, email us at hpc@yale.edu to request such a directory. Share With Specific Users or Other Groups It can be very useful to create shared directories that can be read and written by multiple users, or all members of a group. The linux command setfacl is useful for this, but can be complicated to use. We recommend that you create a shared directory somewhere in your project or scratch60 directories, rather than home . When sharing a sub-directory in your project or scratch60 , you need first share your project or scratch60 , and then share the sub-directory. Here are some simple scenarios. Warning Your ~/project and ~/scratch60 directories are actually symlinks (shortcuts) to elsewhere on the filesystem. Either run mydirectories or readlink - f dirname (replace dirname with the one you are interested in) to get their true paths. Otherwise you will receive errors related to read permissions for your home-space. Share a Directory with All Members of a Group To share a new directory called shared in your project directory with group othergroup : setfacl -m \"g:othergroup:rx\" $(readlink -f ~/project) cd ~/project mkdir shared setfacl -m \"g:othergroup:rwx\" shared Share a Directory with a Particular Person To share a new directory called shared with a person with netid aa111 : setfacl -m \"u:aa111:rx\" $(readlink -f ~/project) cd ~/project mkdir shared setfacl -m \"u:aa111:rwx\" shared Remove Sharing of a Directory To remove a group othergroup from sharing of a directory called shared : setfacl -R -x \"g:othergroup\" shared To remove a person with netid aa111 from sharing of a directory called shared : setfacl -R -x \"u:aa111\" shared","title":"Manage Permissions for Sharing"},{"location":"clusters-at-yale/data/permissions/#manage-permissions-for-sharing","text":"","title":"Manage Permissions for Sharing"},{"location":"clusters-at-yale/data/permissions/#home-directories","text":"Do not give your home directory group write permissions. This will break your ability to log into the cluster. If you need to share something in your home directory, either move it your project directory or ask hpc@yale.edu for assistance.","title":"Home Directories"},{"location":"clusters-at-yale/data/permissions/#shared-group-directories","text":"Upon request we can setup directories for sharing scripts or data across your research group. These directories can either have read-only permissions for the group (so no one accidentally modifies something) or read and write permissions for shared data directories. If interested, email us at hpc@yale.edu to request such a directory.","title":"Shared Group Directories"},{"location":"clusters-at-yale/data/permissions/#share-with-specific-users-or-other-groups","text":"It can be very useful to create shared directories that can be read and written by multiple users, or all members of a group. The linux command setfacl is useful for this, but can be complicated to use. We recommend that you create a shared directory somewhere in your project or scratch60 directories, rather than home . When sharing a sub-directory in your project or scratch60 , you need first share your project or scratch60 , and then share the sub-directory. Here are some simple scenarios. Warning Your ~/project and ~/scratch60 directories are actually symlinks (shortcuts) to elsewhere on the filesystem. Either run mydirectories or readlink - f dirname (replace dirname with the one you are interested in) to get their true paths. Otherwise you will receive errors related to read permissions for your home-space.","title":"Share With Specific Users or Other Groups"},{"location":"clusters-at-yale/data/permissions/#share-a-directory-with-all-members-of-a-group","text":"To share a new directory called shared in your project directory with group othergroup : setfacl -m \"g:othergroup:rx\" $(readlink -f ~/project) cd ~/project mkdir shared setfacl -m \"g:othergroup:rwx\" shared","title":"Share a Directory with All Members of a Group"},{"location":"clusters-at-yale/data/permissions/#share-a-directory-with-a-particular-person","text":"To share a new directory called shared with a person with netid aa111 : setfacl -m \"u:aa111:rx\" $(readlink -f ~/project) cd ~/project mkdir shared setfacl -m \"u:aa111:rwx\" shared","title":"Share a Directory with a Particular Person"},{"location":"clusters-at-yale/data/permissions/#remove-sharing-of-a-directory","text":"To remove a group othergroup from sharing of a directory called shared : setfacl -R -x \"g:othergroup\" shared To remove a person with netid aa111 from sharing of a directory called shared : setfacl -R -x \"u:aa111\" shared","title":"Remove Sharing of a Directory"},{"location":"clusters-at-yale/data/transfer/","text":"Transfer Data For all transfer methods you should already have set up your account on the cluster(s) you want to tranfer data to/from. Data Transfer Nodes Each cluster (except Milgram) has a dedicated node specially networked for high speed transfers both on and off-campus using the Yale Science Network. These transfer nodes host of our Globus service , enabling resilient transfers at scale. After logging in to a cluster, you can ssh to its transfer node with: ssh transfer Tip If you are running a large transfer without Globus , run it inside a tmux session on the transfer node. This protects your transfer from network interruptions between your computer and the transfer node. You may also use the transfer node to transfer data from your local machine using one of the below methods. From off-cluster, the nodes are accessible at the following hostnames. You must still be on-campus or on the VPN to access the transfer nodes. Cluster Transfer Node Grace transfer-grace.hpc.yale.edu Farnam transfer-farnam.hpc.yale.edu Ruddle transfer-ruddle.hpc.yale.edu Milgram use login node, milgram.hpc.yale.edu Graphical Transfer Tools OOD Web Transfers On Farnam , Grace , and Ruddle you can use their respective Open OnDemand sites to transfer files. This works best for small numbers of relatively small files. You can also directly edit scripts through this interface, alleviating the need to transfer scripts to your computer to edit. MobaXterm (Windows) MobaXterm is an all-in-one graphical client for Windows that includes a transfer pane for each cluster you connect to. Once you have established a connection to the cluster, click on the \"Sftp\" tab in the left sidebar to see your files on the cluster. You can drag-and-drop data into and out of the SFTP pane to upload and download, respectively. Cyberduck You can also transfer files between your local computer and a cluster using an FTP client, such as Cyberduck (OSX/Windows) . You will need to configure the client with your netid as the username, the cluster transfer node as the hostname and your private key as the authentication method. An example configuration of Cyberduck is shown below. Cyberduck on Ruddle Ruddle requires Multi-Factor Authentication so there are a couple additional configuration steps. Under Cyberduck > Preferences > Transfers > General change the setting to \"Use browser connection\" instead of \"Open multiple connections\". When you connect type one of the following when prompted with a \"Partial authentication success\" window. \"push\" to receive a push notification to your smart phone (requires the Duo mobile app) \"sms\" to receive a verification passcode via text message \"phone\" to receive a phone call Large Transfers (Globus) Globus is a web-enabled GridFTP service that transfers large datasets fast, securely, and reliably between computers configured to be endpoints. We have configured endpoints for most of the Yale clusters and you can configure your computer to transfer files as well. Please see our Globus page for Yale-specific documentation and their official docs to get started. Command-Line Transfer Tools scp and rsync (macOS/Linux/Linux on Windows) Linux and macOS users can use scp or rsync . Use the hostname of the cluster transfer node (see above) to transfer files. Note that you must have your ssh keys properly setup to use the commands outlined below. See the Cluster Access documentation for more info. scp and sftp are both used from a Terminal window. The basic syntax of scp is scp [ from ] [ to ] The from and to can each be a filename or a directory/folder on the computer you are typing the command on or a remote host (e.g. the transfer node). Transfer a File from Your Computer to a Cluster Using the example netid abc123 , following is run on your computer's local terminal. scp myfile.txt abc123@transfer-grace.hpc.yale.edu:/home/fas/admins/abc123/test In this example, myfile.txt is copied to the directory /home/fas/admins/abc123/test: on Grace. This example assumes that myfile.txt is in your current directory. You may also specify the full path of myfile.txt . scp /home/xyz/myfile.txt abc123@transfer-grace.hpc.yale.edu:/home/fas/admins/abc123/test Transfer a Directory to a Cluster scp -r mydirectory abc123@transfer-grace.hpc.yale.edu:/home/fas/admins/abc123/test In this example, the contents of mydirectory are transferred. The -r indicates that the copy is recursive. Transfer Files from the Cluster to Your Computer Assuming you would like the files copied to your current directory: scp abc123@transfer-grace.hpc.yale.edu:/home/fas/admins/abc123/myfile.txt . Note that . represents your current working directory. To specify the destination, simply replace the . with the full path: scp abc123@transfer-grace.hpc.yale.edu:/home/fas/admins/abc123/myfile.txt /path/myfolder Rclone To move data to and from cloud storage (Box, Dropbox, Wasabi, AWS S3, or Google Cloud Storage, etc.) , we recommend using Rclone . It is installed as a module on all of the clusters and can be installed on your computer. You will need to configure it for each kind of storage you would like to transfer to with: rclone configure You'll be prompted for a name for the connection (e.g mys3), and then details about the connection. Once you've saved that configuration, you can use that connection name to copy files with similar syntax to scp and rsync : rclone copy localpath/myfile mys3:bucketname/ rclone sync localpath/mydir mys3:bucketname/remotedir We recommend that you protect your configurations with a password. You'll see that as an option when you run rclone config. For all the Rclone documentaion please refer to the official site .","title":"Transfer Data"},{"location":"clusters-at-yale/data/transfer/#transfer-data","text":"For all transfer methods you should already have set up your account on the cluster(s) you want to tranfer data to/from.","title":"Transfer Data"},{"location":"clusters-at-yale/data/transfer/#data-transfer-nodes","text":"Each cluster (except Milgram) has a dedicated node specially networked for high speed transfers both on and off-campus using the Yale Science Network. These transfer nodes host of our Globus service , enabling resilient transfers at scale. After logging in to a cluster, you can ssh to its transfer node with: ssh transfer Tip If you are running a large transfer without Globus , run it inside a tmux session on the transfer node. This protects your transfer from network interruptions between your computer and the transfer node. You may also use the transfer node to transfer data from your local machine using one of the below methods. From off-cluster, the nodes are accessible at the following hostnames. You must still be on-campus or on the VPN to access the transfer nodes. Cluster Transfer Node Grace transfer-grace.hpc.yale.edu Farnam transfer-farnam.hpc.yale.edu Ruddle transfer-ruddle.hpc.yale.edu Milgram use login node, milgram.hpc.yale.edu","title":"Data Transfer Nodes"},{"location":"clusters-at-yale/data/transfer/#graphical-transfer-tools","text":"","title":"Graphical Transfer Tools"},{"location":"clusters-at-yale/data/transfer/#ood-web-transfers","text":"On Farnam , Grace , and Ruddle you can use their respective Open OnDemand sites to transfer files. This works best for small numbers of relatively small files. You can also directly edit scripts through this interface, alleviating the need to transfer scripts to your computer to edit.","title":"OOD Web Transfers"},{"location":"clusters-at-yale/data/transfer/#mobaxterm-windows","text":"MobaXterm is an all-in-one graphical client for Windows that includes a transfer pane for each cluster you connect to. Once you have established a connection to the cluster, click on the \"Sftp\" tab in the left sidebar to see your files on the cluster. You can drag-and-drop data into and out of the SFTP pane to upload and download, respectively.","title":"MobaXterm (Windows)"},{"location":"clusters-at-yale/data/transfer/#cyberduck","text":"You can also transfer files between your local computer and a cluster using an FTP client, such as Cyberduck (OSX/Windows) . You will need to configure the client with your netid as the username, the cluster transfer node as the hostname and your private key as the authentication method. An example configuration of Cyberduck is shown below.","title":"Cyberduck"},{"location":"clusters-at-yale/data/transfer/#cyberduck-on-ruddle","text":"Ruddle requires Multi-Factor Authentication so there are a couple additional configuration steps. Under Cyberduck > Preferences > Transfers > General change the setting to \"Use browser connection\" instead of \"Open multiple connections\". When you connect type one of the following when prompted with a \"Partial authentication success\" window. \"push\" to receive a push notification to your smart phone (requires the Duo mobile app) \"sms\" to receive a verification passcode via text message \"phone\" to receive a phone call","title":"Cyberduck on Ruddle"},{"location":"clusters-at-yale/data/transfer/#large-transfers-globus","text":"Globus is a web-enabled GridFTP service that transfers large datasets fast, securely, and reliably between computers configured to be endpoints. We have configured endpoints for most of the Yale clusters and you can configure your computer to transfer files as well. Please see our Globus page for Yale-specific documentation and their official docs to get started.","title":"Large Transfers (Globus)"},{"location":"clusters-at-yale/data/transfer/#command-line-transfer-tools","text":"","title":"Command-Line Transfer Tools"},{"location":"clusters-at-yale/data/transfer/#scp-and-rsync-macoslinuxlinux-on-windows","text":"Linux and macOS users can use scp or rsync . Use the hostname of the cluster transfer node (see above) to transfer files. Note that you must have your ssh keys properly setup to use the commands outlined below. See the Cluster Access documentation for more info. scp and sftp are both used from a Terminal window. The basic syntax of scp is scp [ from ] [ to ] The from and to can each be a filename or a directory/folder on the computer you are typing the command on or a remote host (e.g. the transfer node).","title":"scp and rsync (macOS/Linux/Linux on Windows)"},{"location":"clusters-at-yale/data/transfer/#transfer-a-file-from-your-computer-to-a-cluster","text":"Using the example netid abc123 , following is run on your computer's local terminal. scp myfile.txt abc123@transfer-grace.hpc.yale.edu:/home/fas/admins/abc123/test In this example, myfile.txt is copied to the directory /home/fas/admins/abc123/test: on Grace. This example assumes that myfile.txt is in your current directory. You may also specify the full path of myfile.txt . scp /home/xyz/myfile.txt abc123@transfer-grace.hpc.yale.edu:/home/fas/admins/abc123/test","title":"Transfer a File from Your Computer to a Cluster"},{"location":"clusters-at-yale/data/transfer/#transfer-a-directory-to-a-cluster","text":"scp -r mydirectory abc123@transfer-grace.hpc.yale.edu:/home/fas/admins/abc123/test In this example, the contents of mydirectory are transferred. The -r indicates that the copy is recursive.","title":"Transfer a Directory to a Cluster"},{"location":"clusters-at-yale/data/transfer/#transfer-files-from-the-cluster-to-your-computer","text":"Assuming you would like the files copied to your current directory: scp abc123@transfer-grace.hpc.yale.edu:/home/fas/admins/abc123/myfile.txt . Note that . represents your current working directory. To specify the destination, simply replace the . with the full path: scp abc123@transfer-grace.hpc.yale.edu:/home/fas/admins/abc123/myfile.txt /path/myfolder","title":"Transfer Files from the Cluster to Your Computer"},{"location":"clusters-at-yale/data/transfer/#rclone","text":"To move data to and from cloud storage (Box, Dropbox, Wasabi, AWS S3, or Google Cloud Storage, etc.) , we recommend using Rclone . It is installed as a module on all of the clusters and can be installed on your computer. You will need to configure it for each kind of storage you would like to transfer to with: rclone configure You'll be prompted for a name for the connection (e.g mys3), and then details about the connection. Once you've saved that configuration, you can use that connection name to copy files with similar syntax to scp and rsync : rclone copy localpath/myfile mys3:bucketname/ rclone sync localpath/mydir mys3:bucketname/remotedir We recommend that you protect your configurations with a password. You'll see that as an option when you run rclone config. For all the Rclone documentaion please refer to the official site .","title":"Rclone"},{"location":"clusters-at-yale/guides/","text":"Overview We provide guides for running certain software packages on our clusters. If you have tips for running a commonly used software and would like to contribute them to our Software Guides, email us at hpc@yale.edu . Also checkout our list of recommended online tutorials on Python, R, unix commands and more .","title":"Overview"},{"location":"clusters-at-yale/guides/#overview","text":"We provide guides for running certain software packages on our clusters. If you have tips for running a commonly used software and would like to contribute them to our Software Guides, email us at hpc@yale.edu . Also checkout our list of recommended online tutorials on Python, R, unix commands and more .","title":"Overview"},{"location":"clusters-at-yale/guides/cesm/","text":"CESM/CAM This is a quick start guide for CESM at Yale. You will still need to read the CESM User Guide and work with your fellow research group members to design and run your simulations, but this guide covers the basics that are specific to running CESM at Yale. CESM User Guides CESM1.0.4 User\u2019s Guide CESM1.1.z User\u2019s Guide CESM User\u2019s Guide (CESM1.2 Release Series User\u2019s Guide) (PDF) Modules CESM 1.0.4, 1.2.2, 2.x are available on Grace. For CESM 2.1.0, load the following modules module load CESM/2.1.0-iomkl-2018a For older versions of CESM, you will need to use the old modules. These old version of CESM do not work with the new modules module use /apps/hpc/Modules module use /apps/hpc.rhel7/Modules module avail CESM Once you have located your module, run module load <module-name> with the module name from above. With either module, the module will configure your environment with the Intel compiler, OpenMPI and NetCDF libraries as well as set the location of the Yale\u2019s repository of CESM input data. If you will be primarily using CESM, you can avoid rerunning the module load command every time you login by saving it to your default environment: module load <module-name> module save Input Data To reduce the amount of data duplication on the cluster, we keep one centralized repository of CESM input data. The YCRC staff are only people who can add to that directory, so if your build fails due to missing inputdata, send your create_newcase line to the YCRC ( hpc@yale.edu ) and they will download that data for you. Run CESM CESM needs to be rebuilt separately for each run. As a result, running CESM is more complicated than a standard piece of software where you would just run the executable. Create Your Case Each simulation is called a \u201ccase\u201d. Loading a CESM module will put the create_newcase script in your path, so you can call it as follows. This will create a directory with your case name, that we will refer to as $CASE through out the rest of the guide. create_newcase -case $CASE -compset = <compset> -res = <resolution> -mach = <machine> cd $CASE The mach parameters for Grace is yalegrace for CESM 1.x and gracempi for CESM 2.x , respectively. For example create_newcase --case $CASE --compset = B1850 --res = f09_g17 --mach = gracempi cd $CASE Setup Your Case If you are making any changes to the namelist files (such as increasing the duration of the simulation), do those before running the setup scripts below. CESM 1.0.X ./configure -case CESM 1.1.X and CESM 1.2.X ./cesm_setup CESM 2.X ./case.setup Build Your Case After you run the setup script, there will be a set of the scripts in your case directory that start with your case name. To compile your simulation executable, first move to an interactive job and then run the build script corresponding to your case. # CESM 1.x srun --pty -c 4 -p interactive bash module load <module-name> # <module-name> = the appropriate module for your CESM version ./ $CASE . $mach .build # CESM 2.x srun --pty -c 4 -p interactive bash module load <module-name> # <module-name> = the appropriate module for your CESM version ./case.build For more details on interactive jobs, see our Slurm documentation . During the build, CESM will create a corresponding directory in your scratch60 or project directory at ls ~/scratch60/CESM/$CASE This directory will contain all the outputs from your simulation as well as logs and the cesm.exe executable. Common Build Issues Make sure you compile on an interactive node as described above. If you build fails, it will direct you to look in a bldlog file. If that log complains that it can\u2019t find mpirun, NetCDF or another library or executable, make sure you have the correct CESM module loaded. It can helpful to run module purge before the module load to ensure a reproducible environment. Submit Your Case Once the build is complete, which can take 5-15 minutes, you can submit your case with the submit script. # CESM 1.x ./ $CASE . $mach .submit # CESM 2.x ./case.submit For more details on monitoring your submitted jobs, see our Slurm documentation . Troubleshoot Your Run If your run doesn\u2019t complete, there are a few places to look to identify the error. CESM writes to multiple log files for the different components and you will likely have to look in a few to find the root cause of your error. Slurm Log In your case directory, there will be a file that looks like slurm-<job_id>.log . Check that file first to make sure the job started up properly. If the last few lines in the file redirect you to look at cpl.log.<some_number> file in your scratch directory, see below. If there is another error, try to address it and resubmit. CESM Run Logs If the last few lines of the slurm log direct you to look at cpl.log.<some_number> file, change directory to your case \u201crun\u201d directory in your scratch directory: cd ~/scratch60/CESM/ $CASE /run The pointer to the cpl file is often misleading as I have found the error is usually located in one of the other logs. Instead look in the cesm.log.xxxxxx file. Towards the end there may be an error or it may signify which component was running. Then look in the log corresponding to that component to track down the issue. One shortcut to finding the relevant logs is to sort the log files by the time to see which ones were last updated: ls -ltr *log* Look at the end of the last couple logs listed and look for an indication of the error. Resolve Errors Once you have identified the lines in the logs corresponding to your error: If your log says something like Disk quota exceeded , your group is out of space in the fileset you are writing to. You can run the getquota script to get details on your disk usage. Your group will need to reduce their usage before you will be able to run successfully. If it looks like a model error and you don\u2019t know how to fix it, we strongly recommend Googling your error and/or looking in the CESM forums . If you are still experiencing issues, you can email hpc@yale.edu . Alternative Submission Parameters By default, the submission script will submit to the \"mpi\" partition for 1 day. To change this, edit your case\u2019s run script and change the partition and time. The maximum walltime in the mpi partition is 24 hours. The maximum walltime in scavenge is 24 hours on Grace. For example: ## scavenge partition #SBATCH --partition=scavenge #SBATCH --time=1- ## day partition #SBATCH --partition=pi_fedorov #SBATCH --time=7- Then you can submit by running the submit script ./ $CASE . $mach .submit Further Reading We recommend referencing the User Guides listed at the top of this page. CESM User Forum Our Slurm Documentation CESM is a very widely used package, you can often find answers by simply using Google. Just make sure that the solutions you find correspond to the approximate version of CESM you are using. CESM changes in subtle but significant ways between versions.","title":"CESM/CAM"},{"location":"clusters-at-yale/guides/cesm/#cesmcam","text":"This is a quick start guide for CESM at Yale. You will still need to read the CESM User Guide and work with your fellow research group members to design and run your simulations, but this guide covers the basics that are specific to running CESM at Yale.","title":"CESM/CAM"},{"location":"clusters-at-yale/guides/cesm/#cesm-user-guides","text":"CESM1.0.4 User\u2019s Guide CESM1.1.z User\u2019s Guide CESM User\u2019s Guide (CESM1.2 Release Series User\u2019s Guide) (PDF)","title":"CESM User Guides"},{"location":"clusters-at-yale/guides/cesm/#modules","text":"CESM 1.0.4, 1.2.2, 2.x are available on Grace. For CESM 2.1.0, load the following modules module load CESM/2.1.0-iomkl-2018a For older versions of CESM, you will need to use the old modules. These old version of CESM do not work with the new modules module use /apps/hpc/Modules module use /apps/hpc.rhel7/Modules module avail CESM Once you have located your module, run module load <module-name> with the module name from above. With either module, the module will configure your environment with the Intel compiler, OpenMPI and NetCDF libraries as well as set the location of the Yale\u2019s repository of CESM input data. If you will be primarily using CESM, you can avoid rerunning the module load command every time you login by saving it to your default environment: module load <module-name> module save","title":"Modules"},{"location":"clusters-at-yale/guides/cesm/#input-data","text":"To reduce the amount of data duplication on the cluster, we keep one centralized repository of CESM input data. The YCRC staff are only people who can add to that directory, so if your build fails due to missing inputdata, send your create_newcase line to the YCRC ( hpc@yale.edu ) and they will download that data for you.","title":"Input Data"},{"location":"clusters-at-yale/guides/cesm/#run-cesm","text":"CESM needs to be rebuilt separately for each run. As a result, running CESM is more complicated than a standard piece of software where you would just run the executable.","title":"Run CESM"},{"location":"clusters-at-yale/guides/cesm/#create-your-case","text":"Each simulation is called a \u201ccase\u201d. Loading a CESM module will put the create_newcase script in your path, so you can call it as follows. This will create a directory with your case name, that we will refer to as $CASE through out the rest of the guide. create_newcase -case $CASE -compset = <compset> -res = <resolution> -mach = <machine> cd $CASE The mach parameters for Grace is yalegrace for CESM 1.x and gracempi for CESM 2.x , respectively. For example create_newcase --case $CASE --compset = B1850 --res = f09_g17 --mach = gracempi cd $CASE","title":"Create Your Case"},{"location":"clusters-at-yale/guides/cesm/#setup-your-case","text":"If you are making any changes to the namelist files (such as increasing the duration of the simulation), do those before running the setup scripts below.","title":"Setup Your Case"},{"location":"clusters-at-yale/guides/cesm/#cesm-10x","text":"./configure -case","title":"CESM 1.0.X"},{"location":"clusters-at-yale/guides/cesm/#cesm-11x-and-cesm-12x","text":"./cesm_setup","title":"CESM 1.1.X and CESM 1.2.X"},{"location":"clusters-at-yale/guides/cesm/#cesm-2x","text":"./case.setup","title":"CESM 2.X"},{"location":"clusters-at-yale/guides/cesm/#build-your-case","text":"After you run the setup script, there will be a set of the scripts in your case directory that start with your case name. To compile your simulation executable, first move to an interactive job and then run the build script corresponding to your case. # CESM 1.x srun --pty -c 4 -p interactive bash module load <module-name> # <module-name> = the appropriate module for your CESM version ./ $CASE . $mach .build # CESM 2.x srun --pty -c 4 -p interactive bash module load <module-name> # <module-name> = the appropriate module for your CESM version ./case.build For more details on interactive jobs, see our Slurm documentation . During the build, CESM will create a corresponding directory in your scratch60 or project directory at ls ~/scratch60/CESM/$CASE This directory will contain all the outputs from your simulation as well as logs and the cesm.exe executable.","title":"Build Your Case"},{"location":"clusters-at-yale/guides/cesm/#common-build-issues","text":"Make sure you compile on an interactive node as described above. If you build fails, it will direct you to look in a bldlog file. If that log complains that it can\u2019t find mpirun, NetCDF or another library or executable, make sure you have the correct CESM module loaded. It can helpful to run module purge before the module load to ensure a reproducible environment.","title":"Common Build Issues"},{"location":"clusters-at-yale/guides/cesm/#submit-your-case","text":"Once the build is complete, which can take 5-15 minutes, you can submit your case with the submit script. # CESM 1.x ./ $CASE . $mach .submit # CESM 2.x ./case.submit For more details on monitoring your submitted jobs, see our Slurm documentation .","title":"Submit Your Case"},{"location":"clusters-at-yale/guides/cesm/#troubleshoot-your-run","text":"If your run doesn\u2019t complete, there are a few places to look to identify the error. CESM writes to multiple log files for the different components and you will likely have to look in a few to find the root cause of your error.","title":"Troubleshoot Your Run"},{"location":"clusters-at-yale/guides/cesm/#slurm-log","text":"In your case directory, there will be a file that looks like slurm-<job_id>.log . Check that file first to make sure the job started up properly. If the last few lines in the file redirect you to look at cpl.log.<some_number> file in your scratch directory, see below. If there is another error, try to address it and resubmit.","title":"Slurm Log"},{"location":"clusters-at-yale/guides/cesm/#cesm-run-logs","text":"If the last few lines of the slurm log direct you to look at cpl.log.<some_number> file, change directory to your case \u201crun\u201d directory in your scratch directory: cd ~/scratch60/CESM/ $CASE /run The pointer to the cpl file is often misleading as I have found the error is usually located in one of the other logs. Instead look in the cesm.log.xxxxxx file. Towards the end there may be an error or it may signify which component was running. Then look in the log corresponding to that component to track down the issue. One shortcut to finding the relevant logs is to sort the log files by the time to see which ones were last updated: ls -ltr *log* Look at the end of the last couple logs listed and look for an indication of the error.","title":"CESM Run Logs"},{"location":"clusters-at-yale/guides/cesm/#resolve-errors","text":"Once you have identified the lines in the logs corresponding to your error: If your log says something like Disk quota exceeded , your group is out of space in the fileset you are writing to. You can run the getquota script to get details on your disk usage. Your group will need to reduce their usage before you will be able to run successfully. If it looks like a model error and you don\u2019t know how to fix it, we strongly recommend Googling your error and/or looking in the CESM forums . If you are still experiencing issues, you can email hpc@yale.edu .","title":"Resolve Errors"},{"location":"clusters-at-yale/guides/cesm/#alternative-submission-parameters","text":"By default, the submission script will submit to the \"mpi\" partition for 1 day. To change this, edit your case\u2019s run script and change the partition and time. The maximum walltime in the mpi partition is 24 hours. The maximum walltime in scavenge is 24 hours on Grace. For example: ## scavenge partition #SBATCH --partition=scavenge #SBATCH --time=1- ## day partition #SBATCH --partition=pi_fedorov #SBATCH --time=7- Then you can submit by running the submit script ./ $CASE . $mach .submit","title":"Alternative Submission Parameters"},{"location":"clusters-at-yale/guides/cesm/#further-reading","text":"We recommend referencing the User Guides listed at the top of this page. CESM User Forum Our Slurm Documentation CESM is a very widely used package, you can often find answers by simply using Google. Just make sure that the solutions you find correspond to the approximate version of CESM you are using. CESM changes in subtle but significant ways between versions.","title":"Further Reading"},{"location":"clusters-at-yale/guides/clustershell/","text":"ClusterShell ClusterShell is a useful Python package for executing arbitrary commands across multiple hosts. On the Yale clusters it provides a relatively simple way for you to run commands on nodes your jobs are running on, and collect the results. The two most useful commands provided are nodeset , which can show and manipulate node lists and clush , which can run commands on multiple nodes at once. Configuration To set up ClusterShell, make sure you have a .config directory and a copy our groups.conf file there. For more info about ClusterShell configuration for Slurm, see the official docs . mkdir -p ~/.config/clustershell wget https://docs.ycrc.yale.edu/files/clustershell_groups.conf -O ~/.config/clustershell/groups.conf We provide ClusterShell as a module, but you can also install it with conda . Module module load ClusterShell Conda module load miniconda conda create -yn clustershell python pip source activate clustershell pip install ClusterShell Examples nodeset The nodeset command uses sinfo underneath but has slightly different syntax. You can use it to ask about node states and nodes your job is running on. The nice difference is you can ask for folded (e.g. c[01-02]n[12,15,18] ) or expanded (e.g. c01n01 c01n02 ... ) node lists. The groups useful to you that we have configured are @user , @job and @state . User group List expanded node names where user abc123 has jobs running # similar to squeue -h -u abc123 -o \"%N\" nodeset -e @user:abc123 Job group List folded nodes where job 1234567 is running # similar to squeue -h -j 1234567 -o \"%N\" nodeset -f @job:1234567 State group List expanded node names that are idle according to slurm # similar to sinfo -t IDLE -o \"%N\" nodeset -e @state:idle clush The clush command uses the node grouping syntax from nodeset to allow you to run commands on those nodes. clush uses ssh to connect to each of these nodes. You can use the -b option to gather output from nodes with same output into the same lines. Leaving this out will report on each node separately. Info You can only ssh to, and therefore run clush on, nodes where you have active jobs. Local storage Get a list of files in /tmp/abs on all nodes where job 654321 is running. clush -bw @job:654321 ls /tmp/abc123 # don't gather identical output clush -w @job:654321 ls /tmp/abc123 CPU usage Show %cpu, memory usage, and command for all nodes running any jobs owned by user abc123 . clush -bw @user:abc123 ps -uabc123 -o%cpu,rss,cmd GPU usage Show what's running on all the GPUs on the nodes associated with your job 654321 . clush -bw @job:654321 nvidia-smi --format = csv --query-compute-apps = process_name,used_gpu_memory","title":"ClusterShell"},{"location":"clusters-at-yale/guides/clustershell/#clustershell","text":"ClusterShell is a useful Python package for executing arbitrary commands across multiple hosts. On the Yale clusters it provides a relatively simple way for you to run commands on nodes your jobs are running on, and collect the results. The two most useful commands provided are nodeset , which can show and manipulate node lists and clush , which can run commands on multiple nodes at once.","title":"ClusterShell"},{"location":"clusters-at-yale/guides/clustershell/#configuration","text":"To set up ClusterShell, make sure you have a .config directory and a copy our groups.conf file there. For more info about ClusterShell configuration for Slurm, see the official docs . mkdir -p ~/.config/clustershell wget https://docs.ycrc.yale.edu/files/clustershell_groups.conf -O ~/.config/clustershell/groups.conf We provide ClusterShell as a module, but you can also install it with conda .","title":"Configuration"},{"location":"clusters-at-yale/guides/clustershell/#module","text":"module load ClusterShell","title":"Module"},{"location":"clusters-at-yale/guides/clustershell/#conda","text":"module load miniconda conda create -yn clustershell python pip source activate clustershell pip install ClusterShell","title":"Conda"},{"location":"clusters-at-yale/guides/clustershell/#examples","text":"","title":"Examples"},{"location":"clusters-at-yale/guides/clustershell/#nodeset","text":"The nodeset command uses sinfo underneath but has slightly different syntax. You can use it to ask about node states and nodes your job is running on. The nice difference is you can ask for folded (e.g. c[01-02]n[12,15,18] ) or expanded (e.g. c01n01 c01n02 ... ) node lists. The groups useful to you that we have configured are @user , @job and @state .","title":"nodeset"},{"location":"clusters-at-yale/guides/clustershell/#user-group","text":"List expanded node names where user abc123 has jobs running # similar to squeue -h -u abc123 -o \"%N\" nodeset -e @user:abc123","title":"User group"},{"location":"clusters-at-yale/guides/clustershell/#job-group","text":"List folded nodes where job 1234567 is running # similar to squeue -h -j 1234567 -o \"%N\" nodeset -f @job:1234567","title":"Job group"},{"location":"clusters-at-yale/guides/clustershell/#state-group","text":"List expanded node names that are idle according to slurm # similar to sinfo -t IDLE -o \"%N\" nodeset -e @state:idle","title":"State group"},{"location":"clusters-at-yale/guides/clustershell/#clush","text":"The clush command uses the node grouping syntax from nodeset to allow you to run commands on those nodes. clush uses ssh to connect to each of these nodes. You can use the -b option to gather output from nodes with same output into the same lines. Leaving this out will report on each node separately. Info You can only ssh to, and therefore run clush on, nodes where you have active jobs.","title":"clush"},{"location":"clusters-at-yale/guides/clustershell/#local-storage","text":"Get a list of files in /tmp/abs on all nodes where job 654321 is running. clush -bw @job:654321 ls /tmp/abc123 # don't gather identical output clush -w @job:654321 ls /tmp/abc123","title":"Local storage"},{"location":"clusters-at-yale/guides/clustershell/#cpu-usage","text":"Show %cpu, memory usage, and command for all nodes running any jobs owned by user abc123 . clush -bw @user:abc123 ps -uabc123 -o%cpu,rss,cmd","title":"CPU usage"},{"location":"clusters-at-yale/guides/clustershell/#gpu-usage","text":"Show what's running on all the GPUs on the nodes associated with your job 654321 . clush -bw @job:654321 nvidia-smi --format = csv --query-compute-apps = process_name,used_gpu_memory","title":"GPU usage"},{"location":"clusters-at-yale/guides/conda/","text":"Python with Conda For researchers who have Python (or R package--see bottom) requirements beyond the most common packages (e.g. Numpy, Scipy, Pandas), we recommend using Anaconda . Using Anaconda's Conda package manager, you can create and manage packages and environments. These allow you to easily switch between versions of Python libraries and applications for different projects. Many other software applications have also started to use Conda as a package manager. It has become a popular choice for managing pipelines that involve several tools, especially with multiple languages. The Miniconda Module For your convenience, we provide a relatively recent version of Miniconda (a minimal set of Anaconda libraries) as a module. It serves to bootstrap your personal environments. By using this module, you do not need to download your own copy of Conda, which will prevent unnecessary file and storage usage in your directories. Note: If you are on Milgram and run out of space in your home directory for Conda, you can either reinstall your environment in your project space (see below) or contact us at hpc@yale.edu for help with your home quota. Setup Your Environment Load the Miniconda Module module load miniconda You can save this to your default module collection by using module save . See our module documentation for more details. Default Install Locations By default on all clusters, we set the CONDA_ENVS_PATH and CONDA_PKGS_DIRS environment variables to conda_envs and conda_pkgs in your project directory where there is more quota available. Conda will install to and search in these directories for environments and cached packages. Create a conda Environment To create an environment (saved to the first location in $CONDA_ENVS_PATH or to ~/.conda/envs ) use the conda create command. You should give your environments names that are meaningful to you, so you can more easily keep track of which serves which project or purpose. You can also use environments manage groups of packages that have conflicting prerequisites. Because dependency resolution is hard and messy, we find specifying as many packages as possible at environment creation time can help minimize broken dependencies. Although often unavoidable for Python, we also recommend against heavily mixing the use of conda and pip to install applications. If needed, try to get as much installed with conda , then use pip to get the rest of the way to your desired environment. Tip For added reproducibility and control, specify versions of packages to be installed using conda with packagename=version syntax. E.g. numpy=1.14 For example, if you have a legacy application that needs Python 2 and OpenBLAS: conda create -n legacy_application python = 2 .7 openblas If you want a good starting point for interactive development of scientific Python scripts: conda create -n py37_dev python = 3 .7 numpy scipy pandas matplotlib ipython jupyter Conda Channels There are also community-lead collections of unofficial packages that you can use with conda called channels. A few popular examples are Conda Forge and Bioconda . See the conda docs for more info about managing channels. You could use the Conda Forge channel to install Brian2 conda create -n brian2 --channel conda-forge brian2 Bioconda provides recent versions of various bioinformatics tools, for example: conda create -n bioinfo --channel conda-forge --channel bioconda biopython bedtools bowtie2 repeatmasker Channel priority decreases from left to right - the first argument is higher priority than the second. Using Your Environment To use the applications in your environment, make sure you have the miniconda module loaded then run the following: source activate env_name Warning We do not recommend putting source activate commands in your .bashrc file. This can lead to issues in interactive or batch jobs. If you do have issues with an environment in an interactive or batch job, trying re-entering the environment by calling source deactivate before rerunning source activate env_name . Interactive Your conda environments will not follow you into job allocations. Make sure to activate them after your interactive job begins. In a Job Script To make sure that you are running in your project environment in a submission script, make sure to include the following lines in your submission script before running any other commands or scripts (but after your Slurm directives ): #!/bin/bash #SBATCH --partition=general #SBATCH --job-name=my_conda_job #SBATCH --cpus-per-task 4 #SBATCH --mem-per-cpu=6000 module load miniconda source activate env_name python analyses.py Find and Install Additional Packages You can search Anaconda Cloud for any packages you would like to install. Once in your conda environment, you can install any additional packages using conda install : Python conda install numpy Troubleshoot \"Permission Denied\" If you get a permission denied error while trying to conda or pip install a package, make sure you have created an environment and activated it or activated an existing one first. \"-bash: activate: No such file or directory\" If you get the above error, it is likely that you don't have the necessary module file loaded. Try loading the minconda module and rerunning your source activate env_name command. \"could not find environment:\" This error means that the version of Anaconda/Miniconda you have loaded doesn't recognize the environment name you have supplied. Make sure you have the miniconda module loaded (and not a different Python module) and have previously created this environment. You can see a list of previously created environments by running: conda info --envs Additional Conda Commands List Installed Packages source activate env_name conda list Delete a Conda Environment conda env remove --name env_name Share your Conda Environment If you want to share or back up a conda environment, you can export it to a file. To do so you need to run the following, replacing env_name with the desired environment. source activate env_name conda env export > env_name_environment.yml # on another machine or account, run conda env create -f env_name_environment.yml Conda for R Conda can also be used under certain circumstances to install and manage R packages. All R packages are prepended with r- . Create new environment: conda create -n r_env r-essentials r-base Install additional packages: conda install r-ggplot2 A list of officially supported R packages can be found here , but many additional packages are also available (e.g. via the conda-forge channel) and can be found using search on Anaconda Cloud .","title":"Python with Conda"},{"location":"clusters-at-yale/guides/conda/#python-with-conda","text":"For researchers who have Python (or R package--see bottom) requirements beyond the most common packages (e.g. Numpy, Scipy, Pandas), we recommend using Anaconda . Using Anaconda's Conda package manager, you can create and manage packages and environments. These allow you to easily switch between versions of Python libraries and applications for different projects. Many other software applications have also started to use Conda as a package manager. It has become a popular choice for managing pipelines that involve several tools, especially with multiple languages.","title":"Python with Conda"},{"location":"clusters-at-yale/guides/conda/#the-miniconda-module","text":"For your convenience, we provide a relatively recent version of Miniconda (a minimal set of Anaconda libraries) as a module. It serves to bootstrap your personal environments. By using this module, you do not need to download your own copy of Conda, which will prevent unnecessary file and storage usage in your directories. Note: If you are on Milgram and run out of space in your home directory for Conda, you can either reinstall your environment in your project space (see below) or contact us at hpc@yale.edu for help with your home quota.","title":"The Miniconda Module"},{"location":"clusters-at-yale/guides/conda/#setup-your-environment","text":"","title":"Setup Your Environment"},{"location":"clusters-at-yale/guides/conda/#load-the-miniconda-module","text":"module load miniconda You can save this to your default module collection by using module save . See our module documentation for more details.","title":"Load the Miniconda Module"},{"location":"clusters-at-yale/guides/conda/#default-install-locations","text":"By default on all clusters, we set the CONDA_ENVS_PATH and CONDA_PKGS_DIRS environment variables to conda_envs and conda_pkgs in your project directory where there is more quota available. Conda will install to and search in these directories for environments and cached packages.","title":"Default Install Locations"},{"location":"clusters-at-yale/guides/conda/#create-a-conda-environment","text":"To create an environment (saved to the first location in $CONDA_ENVS_PATH or to ~/.conda/envs ) use the conda create command. You should give your environments names that are meaningful to you, so you can more easily keep track of which serves which project or purpose. You can also use environments manage groups of packages that have conflicting prerequisites. Because dependency resolution is hard and messy, we find specifying as many packages as possible at environment creation time can help minimize broken dependencies. Although often unavoidable for Python, we also recommend against heavily mixing the use of conda and pip to install applications. If needed, try to get as much installed with conda , then use pip to get the rest of the way to your desired environment. Tip For added reproducibility and control, specify versions of packages to be installed using conda with packagename=version syntax. E.g. numpy=1.14 For example, if you have a legacy application that needs Python 2 and OpenBLAS: conda create -n legacy_application python = 2 .7 openblas If you want a good starting point for interactive development of scientific Python scripts: conda create -n py37_dev python = 3 .7 numpy scipy pandas matplotlib ipython jupyter","title":"Create a conda Environment"},{"location":"clusters-at-yale/guides/conda/#conda-channels","text":"There are also community-lead collections of unofficial packages that you can use with conda called channels. A few popular examples are Conda Forge and Bioconda . See the conda docs for more info about managing channels. You could use the Conda Forge channel to install Brian2 conda create -n brian2 --channel conda-forge brian2 Bioconda provides recent versions of various bioinformatics tools, for example: conda create -n bioinfo --channel conda-forge --channel bioconda biopython bedtools bowtie2 repeatmasker Channel priority decreases from left to right - the first argument is higher priority than the second.","title":"Conda Channels"},{"location":"clusters-at-yale/guides/conda/#using-your-environment","text":"To use the applications in your environment, make sure you have the miniconda module loaded then run the following: source activate env_name Warning We do not recommend putting source activate commands in your .bashrc file. This can lead to issues in interactive or batch jobs. If you do have issues with an environment in an interactive or batch job, trying re-entering the environment by calling source deactivate before rerunning source activate env_name .","title":"Using Your Environment"},{"location":"clusters-at-yale/guides/conda/#interactive","text":"Your conda environments will not follow you into job allocations. Make sure to activate them after your interactive job begins.","title":"Interactive"},{"location":"clusters-at-yale/guides/conda/#in-a-job-script","text":"To make sure that you are running in your project environment in a submission script, make sure to include the following lines in your submission script before running any other commands or scripts (but after your Slurm directives ): #!/bin/bash #SBATCH --partition=general #SBATCH --job-name=my_conda_job #SBATCH --cpus-per-task 4 #SBATCH --mem-per-cpu=6000 module load miniconda source activate env_name python analyses.py","title":"In a Job Script"},{"location":"clusters-at-yale/guides/conda/#find-and-install-additional-packages","text":"You can search Anaconda Cloud for any packages you would like to install. Once in your conda environment, you can install any additional packages using conda install :","title":"Find and Install Additional Packages"},{"location":"clusters-at-yale/guides/conda/#python","text":"conda install numpy","title":"Python"},{"location":"clusters-at-yale/guides/conda/#troubleshoot","text":"","title":"Troubleshoot"},{"location":"clusters-at-yale/guides/conda/#permission-denied","text":"If you get a permission denied error while trying to conda or pip install a package, make sure you have created an environment and activated it or activated an existing one first.","title":"\"Permission Denied\""},{"location":"clusters-at-yale/guides/conda/#-bash-activate-no-such-file-or-directory","text":"If you get the above error, it is likely that you don't have the necessary module file loaded. Try loading the minconda module and rerunning your source activate env_name command.","title":"\"-bash: activate: No such file or directory\""},{"location":"clusters-at-yale/guides/conda/#could-not-find-environment","text":"This error means that the version of Anaconda/Miniconda you have loaded doesn't recognize the environment name you have supplied. Make sure you have the miniconda module loaded (and not a different Python module) and have previously created this environment. You can see a list of previously created environments by running: conda info --envs","title":"\"could not find environment:\""},{"location":"clusters-at-yale/guides/conda/#additional-conda-commands","text":"","title":"Additional Conda Commands"},{"location":"clusters-at-yale/guides/conda/#list-installed-packages","text":"source activate env_name conda list","title":"List Installed Packages"},{"location":"clusters-at-yale/guides/conda/#delete-a-conda-environment","text":"conda env remove --name env_name","title":"Delete a Conda Environment"},{"location":"clusters-at-yale/guides/conda/#share-your-conda-environment","text":"If you want to share or back up a conda environment, you can export it to a file. To do so you need to run the following, replacing env_name with the desired environment. source activate env_name conda env export > env_name_environment.yml # on another machine or account, run conda env create -f env_name_environment.yml","title":"Share your Conda Environment"},{"location":"clusters-at-yale/guides/conda/#conda-for-r","text":"Conda can also be used under certain circumstances to install and manage R packages. All R packages are prepended with r- . Create new environment: conda create -n r_env r-essentials r-base Install additional packages: conda install r-ggplot2 A list of officially supported R packages can be found here , but many additional packages are also available (e.g. via the conda-forge channel) and can be found using search on Anaconda Cloud .","title":"Conda for R"},{"location":"clusters-at-yale/guides/cryoem/","text":"Cryo-Electron Microscopy on Farnam Below is a work in progress collection of general hints, tips and tricks for running your work on Farnam . As always, if anything below is unclear or could use updating, please let us know during office hours, via email or through our web ticketing system . Storage Be wary of you and your group's storage quotas. Run getquota from time to time to make sure there isn't usage you aren't expecting. We strongly recommend that you archive raw data off-cluster, as only home directories are backed up . Let us know if you need extra space and we can work with you to find a solution that is right for your project and your group. On all nodes that have gtx1080ti GPUs there is a fast SSD mounted at /tmp . You can use this as a fast local cache if your program can take advantage of it. Schedule Jobs Many Cryo-EM applications can make use of GPUs as co-processors. In order to use a GPU on Farnam you must allocate a job on a partition with GPUs available and explicitly request GPU(s). Make sure to familiarize yourself with our documentation on scheduling jobs and requesting specific resources . There are four partitions that give you access to GPUs. The gpu, gpu_devel and savenge_gpu partitions are all for general use. The use of the pi_cryoem partition is limited to users of the Cryo-EM resources on campus. Please coordinate with the staff from West Campus and CCMI ( See here for contact info ) for access. Software Many Cryo-EM applications are meant to be viewed and interacted with in real-time. This mode of working is not ideal for the way most HPC clusters are set up, so where possible try to prototype a job you would like to run with a smaller dataset or subset of your data. Then develop a script to submit with sbatch . RELION The RELION pipeline operates in two modes. You can use it as a more familiar and beginner-friendly graphical interface, or call the programs involved directly. Once you are comfortable, using the commands directly in scripts submitted with sbatch will allow you to get the most work done the fastest. The authors provide up-to-date hints about performance on their https://www3.mrc-lmb.cam.ac.uk/relion/index.php/Benchmarks_&_computer_hardware page. If you need technical help (jobs submit fine but having other issues) you should search and submit to their mailing list . Module We have GPU-enabled versions of RELION available on Farnam via modules . To check witch versions are available, run module avail relion . To see specific notes about a particular install, you can use module help , e.g. module help RELION/3.0.5-fosscuda-2018b . Example Job Parameters Because RELION reserves one worker (slurm task) for orchestrating an MPI-based job, which they call the \"master\". This can lead to inefficient jobs where there are tasks that could be using a GPU but are stuck being the master process. To use a more plentiful resource as your master worker/task you can free up CPUs on a GPU node for computation. You can request a better layout for your job with a heterogenous job . Here is an example 3D refinement job: #!/bin/bash #SBATCH --partition=general --ntasks 1 -c2 -C avx2 --job-name=class3D_hetero_01 --mem=10G --output=\"class3D_hetero_01-%j.out\" #SBATCH hetjob #SBATCH --partition=gpu --ntasks 4 -c2 -N1 --mem=120G --gres=gpu:4 module load RELION srun --pack-group = 0 ,1 relion_refine_mpi --o hetero/refine3D/job0001 ... --dont_combine_weights_via_disc --j ${ SLURM_CPUS_PER_TASK } --gpu This job submission request will result in Relion using a single task/worker on a general purpose CPU node, and efficiently use a GPU node. Each GPU node task will have a dedicated GPU and two CPU cores. EMAN2 EMAN2 has always been a bit of a struggle for us to install properly on the clusters. Below are a few options Conda Install The EMAN2 authors offer some instructions on how to get EMAN2 running in a cluster environment on their install page . Singularity Container At present, we have a mostly working singularity container for EMAN2.3 available here: /gpfs/ysm/datasets/cryoem/eman2.3_ubuntu18.04.sif To run a program from EMAN2 using this container you would use a command like: singularity exec /gpfs/ysm/datasets/cryoem/eman2.3_ubuntu18.04.sif e2projectmanager.py Cryosparc We have a whole separate page about this one, it is a bit involved.","title":"Cryo-Electron Microscopy on Farnam"},{"location":"clusters-at-yale/guides/cryoem/#cryo-electron-microscopy-on-farnam","text":"Below is a work in progress collection of general hints, tips and tricks for running your work on Farnam . As always, if anything below is unclear or could use updating, please let us know during office hours, via email or through our web ticketing system .","title":"Cryo-Electron Microscopy on Farnam"},{"location":"clusters-at-yale/guides/cryoem/#storage","text":"Be wary of you and your group's storage quotas. Run getquota from time to time to make sure there isn't usage you aren't expecting. We strongly recommend that you archive raw data off-cluster, as only home directories are backed up . Let us know if you need extra space and we can work with you to find a solution that is right for your project and your group. On all nodes that have gtx1080ti GPUs there is a fast SSD mounted at /tmp . You can use this as a fast local cache if your program can take advantage of it.","title":"Storage"},{"location":"clusters-at-yale/guides/cryoem/#schedule-jobs","text":"Many Cryo-EM applications can make use of GPUs as co-processors. In order to use a GPU on Farnam you must allocate a job on a partition with GPUs available and explicitly request GPU(s). Make sure to familiarize yourself with our documentation on scheduling jobs and requesting specific resources . There are four partitions that give you access to GPUs. The gpu, gpu_devel and savenge_gpu partitions are all for general use. The use of the pi_cryoem partition is limited to users of the Cryo-EM resources on campus. Please coordinate with the staff from West Campus and CCMI ( See here for contact info ) for access.","title":"Schedule Jobs"},{"location":"clusters-at-yale/guides/cryoem/#software","text":"Many Cryo-EM applications are meant to be viewed and interacted with in real-time. This mode of working is not ideal for the way most HPC clusters are set up, so where possible try to prototype a job you would like to run with a smaller dataset or subset of your data. Then develop a script to submit with sbatch .","title":"Software"},{"location":"clusters-at-yale/guides/cryoem/#relion","text":"The RELION pipeline operates in two modes. You can use it as a more familiar and beginner-friendly graphical interface, or call the programs involved directly. Once you are comfortable, using the commands directly in scripts submitted with sbatch will allow you to get the most work done the fastest. The authors provide up-to-date hints about performance on their https://www3.mrc-lmb.cam.ac.uk/relion/index.php/Benchmarks_&_computer_hardware page. If you need technical help (jobs submit fine but having other issues) you should search and submit to their mailing list .","title":"RELION"},{"location":"clusters-at-yale/guides/cryoem/#module","text":"We have GPU-enabled versions of RELION available on Farnam via modules . To check witch versions are available, run module avail relion . To see specific notes about a particular install, you can use module help , e.g. module help RELION/3.0.5-fosscuda-2018b .","title":"Module"},{"location":"clusters-at-yale/guides/cryoem/#example-job-parameters","text":"Because RELION reserves one worker (slurm task) for orchestrating an MPI-based job, which they call the \"master\". This can lead to inefficient jobs where there are tasks that could be using a GPU but are stuck being the master process. To use a more plentiful resource as your master worker/task you can free up CPUs on a GPU node for computation. You can request a better layout for your job with a heterogenous job . Here is an example 3D refinement job: #!/bin/bash #SBATCH --partition=general --ntasks 1 -c2 -C avx2 --job-name=class3D_hetero_01 --mem=10G --output=\"class3D_hetero_01-%j.out\" #SBATCH hetjob #SBATCH --partition=gpu --ntasks 4 -c2 -N1 --mem=120G --gres=gpu:4 module load RELION srun --pack-group = 0 ,1 relion_refine_mpi --o hetero/refine3D/job0001 ... --dont_combine_weights_via_disc --j ${ SLURM_CPUS_PER_TASK } --gpu This job submission request will result in Relion using a single task/worker on a general purpose CPU node, and efficiently use a GPU node. Each GPU node task will have a dedicated GPU and two CPU cores.","title":"Example Job Parameters"},{"location":"clusters-at-yale/guides/cryoem/#eman2","text":"EMAN2 has always been a bit of a struggle for us to install properly on the clusters. Below are a few options","title":"EMAN2"},{"location":"clusters-at-yale/guides/cryoem/#conda-install","text":"The EMAN2 authors offer some instructions on how to get EMAN2 running in a cluster environment on their install page .","title":"Conda Install"},{"location":"clusters-at-yale/guides/cryoem/#singularity-container","text":"At present, we have a mostly working singularity container for EMAN2.3 available here: /gpfs/ysm/datasets/cryoem/eman2.3_ubuntu18.04.sif To run a program from EMAN2 using this container you would use a command like: singularity exec /gpfs/ysm/datasets/cryoem/eman2.3_ubuntu18.04.sif e2projectmanager.py","title":"Singularity Container"},{"location":"clusters-at-yale/guides/cryoem/#cryosparc","text":"We have a whole separate page about this one, it is a bit involved.","title":"Cryosparc"},{"location":"clusters-at-yale/guides/cryosparc/","text":"cryoSPARCv2 on Farnam Getting cryoSPARC set up and running on the YCRC clusters is something of a task. This guide is meant for intermediate/advanced users. If enought people can convince Structura bio ( see ticket here ) to make cryoSPARC more cluster-friendly we could have a single instance running that you'd just log in to with your Yale credentials. Until then, venture below at your own peril. Install Before you get started, you will need to request a licence from Structura from their website . These instructions are gently modified from the official cryoSPARC documentation . 1. Set up Environment First allocate an interactive job on a compute node to run the install on. srun --cpus-per-task 2 --pty -p interactive bash Then, set the following environment variables to suit your install. We filled in some defaults for you. # where to install cryosparc2 and its sample database install_path = $( readlink -f ${ HOME } /project ) /software/cryosparc2 # the license ID you got from Structura license_id = # your email my_email = $( head -n1 ~/.forward ) # slurm partition to submit your cryosparc jobs to # not sure you can change at runtime? partition = gpu 2. Set up Directories, Download installers # your username my_name = ${ USER } # a temp password cryosparc_passwd = Password123 # load the right CUDA module load CUDA/9.0.176 # set up some more paths db_path = ${ install_path } /database worker_path = ${ install_path } /cryosparc2_worker ssd_path = /tmp/ ${ USER } /cryosparc2_cache # go get the installers mkdir -p $install_path cd $install_path curl -sL https://get.cryosparc.com/download/master-latest/ $license_id > cryosparc2_master.tar.gz curl -sL https://get.cryosparc.com/download/worker-latest/ $license_id > cryosparc2_worker.tar.gz tar -xf cryosparc2_master.tar.gz tar -xf cryosparc2_worker.tar.gz 3. Install the Server and Worker cd ${ install_path } /cryosparc2_master ./install.sh --license $license_id --hostname $( hostname ) --dbpath $db_path --yes source ~/.bashrc cd ${ install_path } /cryosparc2_worker ./install.sh --license $license_id --cudapath $CUDA_HOME --yes source ~/.bashrc 4. Configure for Farnam # Farnam cluster setup mkdir -p ${ install_path } /site_configs && cd ${ install_path } /site_configs cat << EOF > cluster_info.json { \"name\" : \"farnam\", \"worker_bin_path\" : \"${install_path}/cryosparc2_worker/bin/cryosparcw\", \"cache_path\" : \"/tmp/{{ cryosparc_username }}/cryosparc_cache\", \"send_cmd_tpl\" : \"{{ command }}\", \"qsub_cmd_tpl\" : \"sbatch {{ script_path_abs }}\", \"qstat_cmd_tpl\" : \"squeue -j {{ cluster_job_id }}\", \"qdel_cmd_tpl\" : \"scancel {{ cluster_job_id }}\", \"qinfo_cmd_tpl\" : \"sinfo\" } EOF cat << EOF > cluster_script.sh #!/usr/bin/env bash #SBATCH --job-name cryosparc_{{ project_uid }}_{{ job_uid }} #SBATCH -c {{ num_cpu }} #SBATCH --gres=gpu:{{ num_gpu }} #SBATCH -p ${partition} #SBATCH --mem={{ (ram_gb*1024)|int }} #SBATCH -o {{ job_dir_abs }} #SBATCH -e {{ job_dir_abs }} module load CUDA/9.0.176 mkdir -p /tmp/${USER}/cryosparc2_cache {{ run_cmd }} EOF Run srun --cpus-per-task 2 --pty -p interactive bash master_host = $( hostname ) base_dir = $( dirname \" $( dirname \" $( which cryosparcm ) \" ) \" ) sed -i.bak 's/export CRYOSPARC_MASTER_HOSTNAME.*$/export CRYOSPARC_MASTER_HOSTNAME=\\\"' \" $master_host \" '\\\"/g' $base_dir /config.sh source $base_dir /config.sh cryosparcm start cryosparcm status # run the output from the following command on your local linux/mac machine echo \"ssh -N -L $CRYOSPARC_BASE_PORT : $master_host : $CRYOSPARC_BASE_PORT $USER @farnam.hpc.yale.edu\" Database errors If your database won't start and you're sure there isn't another server running, you can remove lock files and try again. # rm -f $CRYOSPARC_DB_PATH/WiredTiger.lock $CRYOSPARC_DB_PATH/mongod.lock","title":"cryoSPARCv2 on Farnam"},{"location":"clusters-at-yale/guides/cryosparc/#cryosparcv2-on-farnam","text":"Getting cryoSPARC set up and running on the YCRC clusters is something of a task. This guide is meant for intermediate/advanced users. If enought people can convince Structura bio ( see ticket here ) to make cryoSPARC more cluster-friendly we could have a single instance running that you'd just log in to with your Yale credentials. Until then, venture below at your own peril.","title":"cryoSPARCv2 on Farnam"},{"location":"clusters-at-yale/guides/cryosparc/#install","text":"Before you get started, you will need to request a licence from Structura from their website . These instructions are gently modified from the official cryoSPARC documentation .","title":"Install"},{"location":"clusters-at-yale/guides/cryosparc/#1-set-up-environment","text":"First allocate an interactive job on a compute node to run the install on. srun --cpus-per-task 2 --pty -p interactive bash Then, set the following environment variables to suit your install. We filled in some defaults for you. # where to install cryosparc2 and its sample database install_path = $( readlink -f ${ HOME } /project ) /software/cryosparc2 # the license ID you got from Structura license_id = # your email my_email = $( head -n1 ~/.forward ) # slurm partition to submit your cryosparc jobs to # not sure you can change at runtime? partition = gpu","title":"1. Set up Environment"},{"location":"clusters-at-yale/guides/cryosparc/#2-set-up-directories-download-installers","text":"# your username my_name = ${ USER } # a temp password cryosparc_passwd = Password123 # load the right CUDA module load CUDA/9.0.176 # set up some more paths db_path = ${ install_path } /database worker_path = ${ install_path } /cryosparc2_worker ssd_path = /tmp/ ${ USER } /cryosparc2_cache # go get the installers mkdir -p $install_path cd $install_path curl -sL https://get.cryosparc.com/download/master-latest/ $license_id > cryosparc2_master.tar.gz curl -sL https://get.cryosparc.com/download/worker-latest/ $license_id > cryosparc2_worker.tar.gz tar -xf cryosparc2_master.tar.gz tar -xf cryosparc2_worker.tar.gz","title":"2. Set up Directories, Download installers"},{"location":"clusters-at-yale/guides/cryosparc/#3-install-the-server-and-worker","text":"cd ${ install_path } /cryosparc2_master ./install.sh --license $license_id --hostname $( hostname ) --dbpath $db_path --yes source ~/.bashrc cd ${ install_path } /cryosparc2_worker ./install.sh --license $license_id --cudapath $CUDA_HOME --yes source ~/.bashrc","title":"3. Install the Server and Worker"},{"location":"clusters-at-yale/guides/cryosparc/#4-configure-for-farnam","text":"# Farnam cluster setup mkdir -p ${ install_path } /site_configs && cd ${ install_path } /site_configs cat << EOF > cluster_info.json { \"name\" : \"farnam\", \"worker_bin_path\" : \"${install_path}/cryosparc2_worker/bin/cryosparcw\", \"cache_path\" : \"/tmp/{{ cryosparc_username }}/cryosparc_cache\", \"send_cmd_tpl\" : \"{{ command }}\", \"qsub_cmd_tpl\" : \"sbatch {{ script_path_abs }}\", \"qstat_cmd_tpl\" : \"squeue -j {{ cluster_job_id }}\", \"qdel_cmd_tpl\" : \"scancel {{ cluster_job_id }}\", \"qinfo_cmd_tpl\" : \"sinfo\" } EOF cat << EOF > cluster_script.sh #!/usr/bin/env bash #SBATCH --job-name cryosparc_{{ project_uid }}_{{ job_uid }} #SBATCH -c {{ num_cpu }} #SBATCH --gres=gpu:{{ num_gpu }} #SBATCH -p ${partition} #SBATCH --mem={{ (ram_gb*1024)|int }} #SBATCH -o {{ job_dir_abs }} #SBATCH -e {{ job_dir_abs }} module load CUDA/9.0.176 mkdir -p /tmp/${USER}/cryosparc2_cache {{ run_cmd }} EOF","title":"4. Configure for Farnam"},{"location":"clusters-at-yale/guides/cryosparc/#run","text":"srun --cpus-per-task 2 --pty -p interactive bash master_host = $( hostname ) base_dir = $( dirname \" $( dirname \" $( which cryosparcm ) \" ) \" ) sed -i.bak 's/export CRYOSPARC_MASTER_HOSTNAME.*$/export CRYOSPARC_MASTER_HOSTNAME=\\\"' \" $master_host \" '\\\"/g' $base_dir /config.sh source $base_dir /config.sh cryosparcm start cryosparcm status # run the output from the following command on your local linux/mac machine echo \"ssh -N -L $CRYOSPARC_BASE_PORT : $master_host : $CRYOSPARC_BASE_PORT $USER @farnam.hpc.yale.edu\"","title":"Run"},{"location":"clusters-at-yale/guides/cryosparc/#database-errors","text":"If your database won't start and you're sure there isn't another server running, you can remove lock files and try again. # rm -f $CRYOSPARC_DB_PATH/WiredTiger.lock $CRYOSPARC_DB_PATH/mongod.lock","title":"Database errors"},{"location":"clusters-at-yale/guides/gaussian/","text":"Gaussian Gaussian is an electronic structure modeling program that Yale has licensed for its HPC clusters. The latest version of Gaussian is Gaussian 16, which also includes GaussView 6. Older versions of both applications are also available. To see a full list of available versions of Gaussian on the cluster, run: module avail gaussian Running Gaussian on the Cluster The examples here are for Gaussian 16. In most cases, you could run the older version Gaussian 09 by replacing \"g16\" with \"g09\" wherever it occurs. When running Gaussian, it is recommended that users request exclusive access to allocated nodes (e.g., by requesting all the cpus on the node) and that they specify the largest possible memory allocation for the number of nodes requested. In addition, in most cases, the scratch storage location (set by the environment variable GAUSS_SCRDIR ) should be on the local parallel scratch file system (e.g., scratch60) of the cluster, rather than in the user\u2019s home directory. (This is the default in the Gaussian module files.) Before running Gaussian, you must set up a number of environment variables. This is accomplished most easily by loading the Gaussian module file using: module load Gaussian To run Gaussian interactively, you need to create an interactive session on a compute node. You could start an interactive session using 4 cores for 4 hours using srun --pty -c 4 -p interactive -t 4 :00:00 bash See our Slurm documentation for more detailed information on requesting resources for interactive jobs. GaussView In connection with Gaussian 16, we have also installed GaussView 6, Gaussian Inc.'s most advanced and powerful graphical interface for Gaussian. With GaussView, you can import or build the molecular structures that interest you; set up, launch, monitor and control Gaussian calculations; and retrieve and view the results, all without ever leaving the application. GaussView 6 includes many new features designed to make working with large systems of chemical interest convenient and straightforward. It also provides full support for all of the new modeling methods and features in Gaussian 16. In order to use GaussView, you must run an X Server on your desktop or laptop, and you must enable X forwarding when logging into the cluster. See our X11 forwarding documentation for instructions. Loading the module file for Gaussian sets up your environment for GaussView as well. Then you can start GaussView by typing the command gv . GaussView 6 may not be compatible with certain versions of the X servers you may run on your desktop or laptop. If you encounter problems, these can often be overcome by starting GaussView with the command gv -mesagl or gv -soft .","title":"Gaussian"},{"location":"clusters-at-yale/guides/gaussian/#gaussian","text":"Gaussian is an electronic structure modeling program that Yale has licensed for its HPC clusters. The latest version of Gaussian is Gaussian 16, which also includes GaussView 6. Older versions of both applications are also available. To see a full list of available versions of Gaussian on the cluster, run: module avail gaussian","title":"Gaussian"},{"location":"clusters-at-yale/guides/gaussian/#running-gaussian-on-the-cluster","text":"The examples here are for Gaussian 16. In most cases, you could run the older version Gaussian 09 by replacing \"g16\" with \"g09\" wherever it occurs. When running Gaussian, it is recommended that users request exclusive access to allocated nodes (e.g., by requesting all the cpus on the node) and that they specify the largest possible memory allocation for the number of nodes requested. In addition, in most cases, the scratch storage location (set by the environment variable GAUSS_SCRDIR ) should be on the local parallel scratch file system (e.g., scratch60) of the cluster, rather than in the user\u2019s home directory. (This is the default in the Gaussian module files.) Before running Gaussian, you must set up a number of environment variables. This is accomplished most easily by loading the Gaussian module file using: module load Gaussian To run Gaussian interactively, you need to create an interactive session on a compute node. You could start an interactive session using 4 cores for 4 hours using srun --pty -c 4 -p interactive -t 4 :00:00 bash See our Slurm documentation for more detailed information on requesting resources for interactive jobs.","title":"Running Gaussian on the Cluster"},{"location":"clusters-at-yale/guides/gaussian/#gaussview","text":"In connection with Gaussian 16, we have also installed GaussView 6, Gaussian Inc.'s most advanced and powerful graphical interface for Gaussian. With GaussView, you can import or build the molecular structures that interest you; set up, launch, monitor and control Gaussian calculations; and retrieve and view the results, all without ever leaving the application. GaussView 6 includes many new features designed to make working with large systems of chemical interest convenient and straightforward. It also provides full support for all of the new modeling methods and features in Gaussian 16. In order to use GaussView, you must run an X Server on your desktop or laptop, and you must enable X forwarding when logging into the cluster. See our X11 forwarding documentation for instructions. Loading the module file for Gaussian sets up your environment for GaussView as well. Then you can start GaussView by typing the command gv . GaussView 6 may not be compatible with certain versions of the X servers you may run on your desktop or laptop. If you encounter problems, these can often be overcome by starting GaussView with the command gv -mesagl or gv -soft .","title":"GaussView"},{"location":"clusters-at-yale/guides/github/","text":"Version Control with GitHub Whether developing large frameworks or simply working on small scripts, version control is an important tool to ensure that your work is never lost. We recommend using git for its flexibility and versatility and GitHub for its power in enabling research and collaboration. 1 Here we will cover the basics of version control and how to use git and GitHub. What is version control? Version contol is an easy and powerful way to track changes to your work. This extends from code to writing documents (if using LaTeX/Tex). It produces and saves \"tagged\" copies of your project so that you don't need to worry about breaking your code-base. This provides a \"coding safety net\" to let you try new features while retaining the ability to roll-back to a working version. What is git and how does it work? Git is a tool that tracks changes to a file (or set of files) through a series of snapshots called \"commits\" or \"revisions\". These snapshots are stored in \"repositories\" which contain the history of all the changes to that file. This helps prevent repetative naming or project_final_final2_v3.txt problems. It acts as a record of all the edits, along with the ability to compare the current version to previous commits. How to create a git repository You can create a repository at any time by running the following commands: cd /path/to/your/project # initialize the repository git init # add files to be tracked git add main.py input.txt # commit the files to the repository, creating the first snapshot git commit -m \"Initial Commit\" This sets up a repository containing a single snapshot of the project's two files. We can then edit these files and commit the changes into a new snapshot: # edit files echo \"changed this file\" >> input.txt $ git status On branch master Changes not staged for commit: ( use \"git add <file>...\" to update what will be committed ) ( use \"git checkout -- <file>...\" to discard changes in working directory ) modified: input.txt no changes added to commit ( use \"git add\" and/or \"git commit -a\" ) Finally, we can stage input.txt and then commit the changes: # stage changes for commit git add input.txt git commit -m \"modified input file\" Configuring git It's very helpful to configure your email and username with git : git config --global user.name \"Your Name\" git config --global user.email \"your.email@yale.edu\" This will then tag your changes with your name and email when collaborating with people on a larger project. Working with remote repositories on GitHub We recommend using an off-site repository like GitHub that provides a secure and co-located backup of your local repositories. To start, create a repository on GitHub by going to https://github.com/new and providing a name and choose either public or private access. Then you can connect your local repository to the GitHub repo (named my_new_repo ): git remote add origin git@github.com:user_name/my_new_repo.git git push -u origin master Alternatively, a repository can be created on GitHub and then cloned to your local machine: $ git clone git@github.com:user_name/my_new_repo.git Cloning into 'my_new_repo' ... remote: Enumerating objects: 3 , done . remote: Counting objects: 100 % ( 3 /3 ) , done . remote: Total 3 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 0 Receiving objects: 100 % ( 3 /3 ) , done . This creates a new directory ( my_new_repo ) where you can place all your code. After making any changes and commiting them to the local repository, you can \"push\" them to a remote repository: # commit to local repository git commit -m \"new changes\" # push commits to remote repository on GitHub git push Educational GitHub All students and research staff are able to request free Educational discounts from GitHub. This provides a \"Pro\" account for free, including unlimited private repositories. To get started, create a free GitHub account with your Yale email address. Then go to https://education.github.com and request the educational discount. It normally takes less than 24 hours for them to grant the discount. Educational discounts are also available for teams and collaborations. This is perfect for a research group or collaboration and can include non-Yale affiliated people. Resources and links YCRC Version Control Bootcamp Educational GitHub GitHub's Try-it Instruqt Getting Started With Git We do not recommend the use of https://git.yale.edu , which is an internal-only tool not designed for research use. \u21a9","title":"Version Control with GitHub"},{"location":"clusters-at-yale/guides/github/#version-control-with-github","text":"Whether developing large frameworks or simply working on small scripts, version control is an important tool to ensure that your work is never lost. We recommend using git for its flexibility and versatility and GitHub for its power in enabling research and collaboration. 1 Here we will cover the basics of version control and how to use git and GitHub.","title":"Version Control with GitHub"},{"location":"clusters-at-yale/guides/github/#what-is-version-control","text":"Version contol is an easy and powerful way to track changes to your work. This extends from code to writing documents (if using LaTeX/Tex). It produces and saves \"tagged\" copies of your project so that you don't need to worry about breaking your code-base. This provides a \"coding safety net\" to let you try new features while retaining the ability to roll-back to a working version.","title":"What is version control?"},{"location":"clusters-at-yale/guides/github/#what-is-git-and-how-does-it-work","text":"Git is a tool that tracks changes to a file (or set of files) through a series of snapshots called \"commits\" or \"revisions\". These snapshots are stored in \"repositories\" which contain the history of all the changes to that file. This helps prevent repetative naming or project_final_final2_v3.txt problems. It acts as a record of all the edits, along with the ability to compare the current version to previous commits.","title":"What is git and how does it work?"},{"location":"clusters-at-yale/guides/github/#how-to-create-a-git-repository","text":"You can create a repository at any time by running the following commands: cd /path/to/your/project # initialize the repository git init # add files to be tracked git add main.py input.txt # commit the files to the repository, creating the first snapshot git commit -m \"Initial Commit\" This sets up a repository containing a single snapshot of the project's two files. We can then edit these files and commit the changes into a new snapshot: # edit files echo \"changed this file\" >> input.txt $ git status On branch master Changes not staged for commit: ( use \"git add <file>...\" to update what will be committed ) ( use \"git checkout -- <file>...\" to discard changes in working directory ) modified: input.txt no changes added to commit ( use \"git add\" and/or \"git commit -a\" ) Finally, we can stage input.txt and then commit the changes: # stage changes for commit git add input.txt git commit -m \"modified input file\"","title":"How to create a git repository"},{"location":"clusters-at-yale/guides/github/#configuring-git","text":"It's very helpful to configure your email and username with git : git config --global user.name \"Your Name\" git config --global user.email \"your.email@yale.edu\" This will then tag your changes with your name and email when collaborating with people on a larger project.","title":"Configuring git"},{"location":"clusters-at-yale/guides/github/#working-with-remote-repositories-on-github","text":"We recommend using an off-site repository like GitHub that provides a secure and co-located backup of your local repositories. To start, create a repository on GitHub by going to https://github.com/new and providing a name and choose either public or private access. Then you can connect your local repository to the GitHub repo (named my_new_repo ): git remote add origin git@github.com:user_name/my_new_repo.git git push -u origin master Alternatively, a repository can be created on GitHub and then cloned to your local machine: $ git clone git@github.com:user_name/my_new_repo.git Cloning into 'my_new_repo' ... remote: Enumerating objects: 3 , done . remote: Counting objects: 100 % ( 3 /3 ) , done . remote: Total 3 ( delta 0 ) , reused 0 ( delta 0 ) , pack-reused 0 Receiving objects: 100 % ( 3 /3 ) , done . This creates a new directory ( my_new_repo ) where you can place all your code. After making any changes and commiting them to the local repository, you can \"push\" them to a remote repository: # commit to local repository git commit -m \"new changes\" # push commits to remote repository on GitHub git push","title":"Working with remote repositories on GitHub"},{"location":"clusters-at-yale/guides/github/#educational-github","text":"All students and research staff are able to request free Educational discounts from GitHub. This provides a \"Pro\" account for free, including unlimited private repositories. To get started, create a free GitHub account with your Yale email address. Then go to https://education.github.com and request the educational discount. It normally takes less than 24 hours for them to grant the discount. Educational discounts are also available for teams and collaborations. This is perfect for a research group or collaboration and can include non-Yale affiliated people.","title":"Educational GitHub"},{"location":"clusters-at-yale/guides/github/#resources-and-links","text":"YCRC Version Control Bootcamp Educational GitHub GitHub's Try-it Instruqt Getting Started With Git We do not recommend the use of https://git.yale.edu , which is an internal-only tool not designed for research use. \u21a9","title":"Resources and links"},{"location":"clusters-at-yale/guides/github_pages/","text":"GitHub Pages Personal Website A personal website is a great way to build an online presence for both academic and professional activities. We recommend using GitHub Pages as a tool to maintain and host static websites and blogs. Unlike other hosting platforms, the whole website can be written using Markdown , a simple widely-used markup language. GitHub provides a tutorial to get started with Markdown ( link ). To get started, you're going to need a GitHub account. You can follow the instructions on our GitHub guide to set up a free account. Once you have an account, you will need to create a repository for your website. It's important that you name your repository username.github.io where username is replaced with your actual account name ( ycrc-test in this example). Make sure to initialize the repo with a README, which will help get things started. After clicking \"Create\" your repository will look like this: From here, you can click on \"Settings\" to enable GitHub Pages publication of your site. Scroll down until you see GitHub Pages : GitHub provides a number of templates to help make your website look professional. Click on \"Choose a Theme\" to see examples of these themes: Pick one that you like and click \"Select theme\". Note, some of these themes are aimed at blogs versus project sites, pick one that best fits your desired style. You can change this later, so feel free to try one out and see what you think. After selecting your theme, you will be directed back to your repository where the README.md has been updated with some basics about how Markdown works and how you can start creating your website. Scroll down and commit these changes (leaving the sample text in place). You can now take a look at how GitHub is rendering your site: That's it, this site is now hosted at ycrc-test.github.io ! You now have a simple-to-edit and customize site that can be used to host your CV, detail your academic research, or showcase your independent projects. Project website In addition to hosting a stand-alone website, GitHub Pages can be used to create pages for specific projects or repositories. Here we will take an existing repository amazing-python-project and add a GitHub Pages website on a new branch. Click on the Branch pull-down and create a new branch titled gh-pages : Remove any files from that branch and create a new file called index.md : Add content to the page using Markdown syntax: To customize the site, click on Settings and then scroll down to GitHub Pages : Click on the Theme Chooser and select your favorite style: Finally, you can navigate to your website and see it live! Conclusions We have detailed two ways to add static websites to your work, either as a professional webpage or a project-specific site. This can help increase your works impact and give you a platform to showcase your work. Further Reading Jekyll : the tool that powers GitHub Pages GitHub Learning Lab Academic Pages : forkable template for academic websites Jekyll Academic Example GitHub Pages Websites GitHub and Government , https://github.com/github/government.github.com ElectronJS , https://github.com/electron/electronjs.org Twitter GitHub , https://github.com/twitter/twitter.github.io React , https://github.com/facebook/react","title":"GitHub Pages"},{"location":"clusters-at-yale/guides/github_pages/#github-pages","text":"","title":"GitHub Pages"},{"location":"clusters-at-yale/guides/github_pages/#personal-website","text":"A personal website is a great way to build an online presence for both academic and professional activities. We recommend using GitHub Pages as a tool to maintain and host static websites and blogs. Unlike other hosting platforms, the whole website can be written using Markdown , a simple widely-used markup language. GitHub provides a tutorial to get started with Markdown ( link ). To get started, you're going to need a GitHub account. You can follow the instructions on our GitHub guide to set up a free account. Once you have an account, you will need to create a repository for your website. It's important that you name your repository username.github.io where username is replaced with your actual account name ( ycrc-test in this example). Make sure to initialize the repo with a README, which will help get things started. After clicking \"Create\" your repository will look like this: From here, you can click on \"Settings\" to enable GitHub Pages publication of your site. Scroll down until you see GitHub Pages : GitHub provides a number of templates to help make your website look professional. Click on \"Choose a Theme\" to see examples of these themes: Pick one that you like and click \"Select theme\". Note, some of these themes are aimed at blogs versus project sites, pick one that best fits your desired style. You can change this later, so feel free to try one out and see what you think. After selecting your theme, you will be directed back to your repository where the README.md has been updated with some basics about how Markdown works and how you can start creating your website. Scroll down and commit these changes (leaving the sample text in place). You can now take a look at how GitHub is rendering your site: That's it, this site is now hosted at ycrc-test.github.io ! You now have a simple-to-edit and customize site that can be used to host your CV, detail your academic research, or showcase your independent projects.","title":"Personal Website"},{"location":"clusters-at-yale/guides/github_pages/#project-website","text":"In addition to hosting a stand-alone website, GitHub Pages can be used to create pages for specific projects or repositories. Here we will take an existing repository amazing-python-project and add a GitHub Pages website on a new branch. Click on the Branch pull-down and create a new branch titled gh-pages : Remove any files from that branch and create a new file called index.md : Add content to the page using Markdown syntax: To customize the site, click on Settings and then scroll down to GitHub Pages : Click on the Theme Chooser and select your favorite style: Finally, you can navigate to your website and see it live!","title":"Project website"},{"location":"clusters-at-yale/guides/github_pages/#conclusions","text":"We have detailed two ways to add static websites to your work, either as a professional webpage or a project-specific site. This can help increase your works impact and give you a platform to showcase your work.","title":"Conclusions"},{"location":"clusters-at-yale/guides/github_pages/#further-reading","text":"Jekyll : the tool that powers GitHub Pages GitHub Learning Lab Academic Pages : forkable template for academic websites Jekyll Academic","title":"Further Reading"},{"location":"clusters-at-yale/guides/github_pages/#example-github-pages-websites","text":"GitHub and Government , https://github.com/github/government.github.com ElectronJS , https://github.com/electron/electronjs.org Twitter GitHub , https://github.com/twitter/twitter.github.io React , https://github.com/facebook/react","title":"Example GitHub Pages Websites"},{"location":"clusters-at-yale/guides/gpus-cuda/","text":"GPUs and CUDA There are GPUs available for general use on Grace and Farnam . See cluster pages for hardware and queue/partition specifics. Please do not use nodes with GPUs unless your application or job can make use of them. Any jobs submitted to a GPU partition without having requested a GPU may be terminated without warning. Requesting GPU Nodes To access the GPU nodes you must request them with slurm. Here is an example command to request a 2 hour interactive job for testing or developing code interactively: srun --pty -p gpu_devel -c 2 -t 2 :00:00 --gres = gpu:1 bash or, if the gpu_devel and gpu partitions are busy, try to scavenge some private GPUs: srun --pty -p scavenge_gpu -c 2 -t 2 :00:00 --gres = gpu:1 bash Note The --gres option specifies resources per node , not per task or core. In a two node job, --gres=gpu:2 would result in a total of four GPUs allocated to you. Specific GPUs To request a specific type of GPU (e.g. a P100) for each node in your job, you specify the GPU type in the --gres flag. #SBATCH --gres=gpu:p100:1 Some codes require double-precision capable GPUs. If applicable, you can request any node with a compatible GPU by using the doubleprecision feature (e.g. K80, P100 or V100). #SBATCH -C doubleprecision Conversely, you can use the singleprecision feature to request nodes that have single-precision only GPUs (e.g. GTX 1080, RTX 2080). Monitor Activity and Drivers The CUDA libraries you load will allow you to compile code against them. To run CUDA-enabled code you must also be running on a node with a gpu allocated and a compatible driver installed. The minimum driver versions are listed on this nvidia developer site . You can check the available GPUs, their current usage, installed version of the nvidia drivers, and more with the command nvidia-smi . Either in an interactive job or after connecting to a node running your job with ssh , nvidia-smi output should look something like this: [ user@gpu01 ~ ] $ nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418 .87.00 Driver Version: 418 .87.00 CUDA Version: 10 .1 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | =============================== + ====================== + ====================== | | 0 GeForce GTX 108 ... On | 00000000 :02:00.0 Off | N/A | | 23 % 34C P8 9W / 250W | 1MiB / 11178MiB | 0 % Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | | ============================================================================= | | No running processes found | +-----------------------------------------------------------------------------+ Here we see that the node gpu01 is running driver version 418.87.00 and CUDA version 10.1. There are no processes using the GPU allocated to this job. Software CUDA and cuDNN modules We have seen varying degrees of success in using the runtime CUDA and cuDNN libraries supplied by various conda channels. If that works for you there may be no need to load additional modules. If not, find the corresponding CUDA and cuDNN combination for your desired environment and load or request those modules. To list all the CUDA and cuDNN modules available: module avail cuda/ module avail cudnn/ Tensorflow You can find hints about the correct version of Tensorflow from their tested build configurations . You can also test your install with a simple script that imports Tensorflow (run on a GPU node). If you an ImportError that mentions missing libraries like libcublas.so.9.0 , for example, that means that Tensorflow is probably expecting CUDA v 9.0 but cannot find it. PyTorch As with Tensorflow, sometimes the conda-supplied CUDA libraries are sufficient for the version of PyTorch you are installing. If not make sure you have the version of cuda referenced on the PyTorch site in their install instructions . They also provide instructions on installing previous versions compatible with older versions of CUDA. Create an Example Tensorflow-GPU Environment # example modules for tensorflow 1.12 module purge module load cuDNN/7.1.4-CUDA-9.0.176 module load miniconda Then save your modules as a collection. # save module environment module save cuda90 Now create a virtual environment for your GPU enabled code. For more details on Conda environments, see our Conda documentation . # create conda environment for deep learning/neural networks conda create -y -n tensorflow112 python = 3 .6 anaconda source activate tensorflow112 #install libraries pip install keras tensorflow-gpu == 1 .12 Use Your Environment To re-enter your environment you only need the following: module restore cuda90 source activate tensorflow112 Compile .c or .cpp Files with CUDA code By default, nvcc expects that host code is in files with a .c or .cpp extension, and device code is in files with a .cu extension. When you mix device code in a .c or .cpp file with host code, the device code will not be recoganized by nvcc unless you add this flag: -x cu . nvcc -x cu mycuda.cpp -o mycuda.exe","title":"GPUs and CUDA"},{"location":"clusters-at-yale/guides/gpus-cuda/#gpus-and-cuda","text":"There are GPUs available for general use on Grace and Farnam . See cluster pages for hardware and queue/partition specifics. Please do not use nodes with GPUs unless your application or job can make use of them. Any jobs submitted to a GPU partition without having requested a GPU may be terminated without warning.","title":"GPUs and CUDA"},{"location":"clusters-at-yale/guides/gpus-cuda/#requesting-gpu-nodes","text":"To access the GPU nodes you must request them with slurm. Here is an example command to request a 2 hour interactive job for testing or developing code interactively: srun --pty -p gpu_devel -c 2 -t 2 :00:00 --gres = gpu:1 bash or, if the gpu_devel and gpu partitions are busy, try to scavenge some private GPUs: srun --pty -p scavenge_gpu -c 2 -t 2 :00:00 --gres = gpu:1 bash Note The --gres option specifies resources per node , not per task or core. In a two node job, --gres=gpu:2 would result in a total of four GPUs allocated to you.","title":"Requesting GPU Nodes"},{"location":"clusters-at-yale/guides/gpus-cuda/#specific-gpus","text":"To request a specific type of GPU (e.g. a P100) for each node in your job, you specify the GPU type in the --gres flag. #SBATCH --gres=gpu:p100:1 Some codes require double-precision capable GPUs. If applicable, you can request any node with a compatible GPU by using the doubleprecision feature (e.g. K80, P100 or V100). #SBATCH -C doubleprecision Conversely, you can use the singleprecision feature to request nodes that have single-precision only GPUs (e.g. GTX 1080, RTX 2080).","title":"Specific GPUs"},{"location":"clusters-at-yale/guides/gpus-cuda/#monitor-activity-and-drivers","text":"The CUDA libraries you load will allow you to compile code against them. To run CUDA-enabled code you must also be running on a node with a gpu allocated and a compatible driver installed. The minimum driver versions are listed on this nvidia developer site . You can check the available GPUs, their current usage, installed version of the nvidia drivers, and more with the command nvidia-smi . Either in an interactive job or after connecting to a node running your job with ssh , nvidia-smi output should look something like this: [ user@gpu01 ~ ] $ nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 418 .87.00 Driver Version: 418 .87.00 CUDA Version: 10 .1 | | -------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | =============================== + ====================== + ====================== | | 0 GeForce GTX 108 ... On | 00000000 :02:00.0 Off | N/A | | 23 % 34C P8 9W / 250W | 1MiB / 11178MiB | 0 % Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | | ============================================================================= | | No running processes found | +-----------------------------------------------------------------------------+ Here we see that the node gpu01 is running driver version 418.87.00 and CUDA version 10.1. There are no processes using the GPU allocated to this job.","title":"Monitor Activity and Drivers"},{"location":"clusters-at-yale/guides/gpus-cuda/#software","text":"","title":"Software"},{"location":"clusters-at-yale/guides/gpus-cuda/#cuda-and-cudnn-modules","text":"We have seen varying degrees of success in using the runtime CUDA and cuDNN libraries supplied by various conda channels. If that works for you there may be no need to load additional modules. If not, find the corresponding CUDA and cuDNN combination for your desired environment and load or request those modules. To list all the CUDA and cuDNN modules available: module avail cuda/ module avail cudnn/","title":"CUDA and cuDNN modules"},{"location":"clusters-at-yale/guides/gpus-cuda/#tensorflow","text":"You can find hints about the correct version of Tensorflow from their tested build configurations . You can also test your install with a simple script that imports Tensorflow (run on a GPU node). If you an ImportError that mentions missing libraries like libcublas.so.9.0 , for example, that means that Tensorflow is probably expecting CUDA v 9.0 but cannot find it.","title":"Tensorflow"},{"location":"clusters-at-yale/guides/gpus-cuda/#pytorch","text":"As with Tensorflow, sometimes the conda-supplied CUDA libraries are sufficient for the version of PyTorch you are installing. If not make sure you have the version of cuda referenced on the PyTorch site in their install instructions . They also provide instructions on installing previous versions compatible with older versions of CUDA.","title":"PyTorch"},{"location":"clusters-at-yale/guides/gpus-cuda/#create-an-example-tensorflow-gpu-environment","text":"# example modules for tensorflow 1.12 module purge module load cuDNN/7.1.4-CUDA-9.0.176 module load miniconda Then save your modules as a collection. # save module environment module save cuda90 Now create a virtual environment for your GPU enabled code. For more details on Conda environments, see our Conda documentation . # create conda environment for deep learning/neural networks conda create -y -n tensorflow112 python = 3 .6 anaconda source activate tensorflow112 #install libraries pip install keras tensorflow-gpu == 1 .12","title":"Create an Example Tensorflow-GPU Environment"},{"location":"clusters-at-yale/guides/gpus-cuda/#use-your-environment","text":"To re-enter your environment you only need the following: module restore cuda90 source activate tensorflow112","title":"Use Your Environment"},{"location":"clusters-at-yale/guides/gpus-cuda/#compile-c-or-cpp-files-with-cuda-code","text":"By default, nvcc expects that host code is in files with a .c or .cpp extension, and device code is in files with a .cu extension. When you mix device code in a .c or .cpp file with host code, the device code will not be recoganized by nvcc unless you add this flag: -x cu . nvcc -x cu mycuda.cpp -o mycuda.exe","title":"Compile .c or .cpp Files with CUDA code"},{"location":"clusters-at-yale/guides/jupyter/","text":"Jupyter Notebooks You can use a compute node to run a jupyter notebook and access it from your local machine. We ask that you do not leave notebook jobs running idle for too long, as they exclude the use of computational resources other jobs could be taking advantage of. Before you get started, you will need to be on campus or logged in to the Yale VPN and you will need to set up a jupyter environment. Set up an environment We recommend you use conda to manage your jupyter environments. For example, if you want to create an environment with many commonly used scientific computing Python packages you would run: module load miniconda conda create -yn notebook_env anaconda python = 3 You can then load this environment for jupyter where we indicate below. Open OnDemand Once you have installed jupyter and any other packages you want to use into a conda environment, you can start a notebook server as a job via Open OnDemand . Traditional Method If you want finer control over your notebook job, you can manually configure a jupyter notebook and connect manually. The main steps are: Start a jupyter notebook job. Start an ssh tunnel. Use your local browser to connect. Start the Server Here is a template for submitting a jupyter-notebook server as a batch job. You may need to edit some of the slurm options, including the time limit or the partition. You will also need to either load a module that contains jupyter-notebook or source activate an environment if you're using . Save your edited version of this script on the cluster, and submit it with sbatch . #!/bin/bash #SBATCH --partition general #SBATCH --nodes 1 #SBATCH --ntasks-per-node 1 #SBATCH --mem-per-cpu 8G #SBATCH --time 1-0:00:00 #SBATCH --job-name jupyter-notebook #SBATCH --output jupyter-notebook-%J.log # get tunneling info XDG_RUNTIME_DIR = \"\" port = $( shuf -i8000-9999 -n1 ) node = $( hostname -s ) user = $( whoami ) cluster = $( hostname -f | awk -F \".\" '{print $2}' ) # print tunneling instructions jupyter-log echo -e \" For more info and how to connect from windows, see https://docs.ycrc.yale.edu/clusters-at-yale/guides/jupyter/ MacOS or linux terminal command to create your ssh tunnel ssh -N -L ${ port } : ${ node } : ${ port } ${ user } @ ${ cluster } .hpc.yale.edu Windows MobaXterm info Forwarded port:same as remote port Remote server: ${ node } Remote port: ${ port } SSH server: ${ cluster } .hpc.yale.edu SSH login: $user SSH port: 22 Use a Browser on your local machine to go to: localhost: ${ port } (prefix w/ https:// if using password) \" # load modules or conda environments here # uncomment the following two lines to use your conda environment called notebook_env # module load miniconda # source activate notebook_env # DON'T USE ADDRESS BELOW. # DO USE TOKEN BELOW jupyter-notebook --no-browser --port = ${ port } --ip = ${ node } Start the Tunnel Once you have submitted your job and it starts, your notebook server will be ready for you to connect. You can run squeue -u${USER} to check. You will see an \"R\" in the ST or status column for your notebook job if it is running. If you see a \"PD\" in the status column, you will have to wait for your job to start running to connect. The log file with information about how to connect will be in the directory you submitted the script from, and be named jupyter-notebook-[jobid].log where jobid is the slurm id for your job. MacOS and Linux On a Mac or Linux machine, you can start the tunnel with an SSH command. You can check the output from the job you started to get the specifc info you need. Windows On a windows machine, we recommend you use MobaXterm. See our guide on connecting with MobaXterm for instructions on how to get set up. You will need to take a look at your job's log file to get the details you need. Then start MobaXterm: Under Tools choose \"MobaSSHTunnel (port forwarding)\". Click the \"New SSH Tunnel\" button. Click the radio button for \"Local port forwarding\". Use the information in your jupyter notebook log file to fill out the boxes. Click Save. On your new tunnel, click the key symbol under the settings column and choose your ssh private key. Click the play button under the Start/Stop column. Browse the Notebook Finally, open a web browser on your local machine and enter the address http://localhost:port where port is the one specified in your log file. The address Jupyter creates by default (the one with the name of a compute node) will not work outside the cluster's network. Since version 5 of jupyter, the notebook will automatically generate a token that allows you to authenticate when you connect. It is long, and will be at the end of the url jupyter generates. It will look something like http://c14n06:9230/?token=**ad0775eaff315e6f1d98b13ef10b919bc6b9ef7d0605cc20** If you run into trouble or need help, send us an email at hpc@yale.edu .","title":"Jupyter Notebooks"},{"location":"clusters-at-yale/guides/jupyter/#jupyter-notebooks","text":"You can use a compute node to run a jupyter notebook and access it from your local machine. We ask that you do not leave notebook jobs running idle for too long, as they exclude the use of computational resources other jobs could be taking advantage of. Before you get started, you will need to be on campus or logged in to the Yale VPN and you will need to set up a jupyter environment.","title":"Jupyter Notebooks"},{"location":"clusters-at-yale/guides/jupyter/#set-up-an-environment","text":"We recommend you use conda to manage your jupyter environments. For example, if you want to create an environment with many commonly used scientific computing Python packages you would run: module load miniconda conda create -yn notebook_env anaconda python = 3 You can then load this environment for jupyter where we indicate below.","title":"Set up an environment"},{"location":"clusters-at-yale/guides/jupyter/#open-ondemand","text":"Once you have installed jupyter and any other packages you want to use into a conda environment, you can start a notebook server as a job via Open OnDemand .","title":"Open OnDemand"},{"location":"clusters-at-yale/guides/jupyter/#traditional-method","text":"If you want finer control over your notebook job, you can manually configure a jupyter notebook and connect manually. The main steps are: Start a jupyter notebook job. Start an ssh tunnel. Use your local browser to connect.","title":"Traditional Method"},{"location":"clusters-at-yale/guides/jupyter/#start-the-server","text":"Here is a template for submitting a jupyter-notebook server as a batch job. You may need to edit some of the slurm options, including the time limit or the partition. You will also need to either load a module that contains jupyter-notebook or source activate an environment if you're using . Save your edited version of this script on the cluster, and submit it with sbatch . #!/bin/bash #SBATCH --partition general #SBATCH --nodes 1 #SBATCH --ntasks-per-node 1 #SBATCH --mem-per-cpu 8G #SBATCH --time 1-0:00:00 #SBATCH --job-name jupyter-notebook #SBATCH --output jupyter-notebook-%J.log # get tunneling info XDG_RUNTIME_DIR = \"\" port = $( shuf -i8000-9999 -n1 ) node = $( hostname -s ) user = $( whoami ) cluster = $( hostname -f | awk -F \".\" '{print $2}' ) # print tunneling instructions jupyter-log echo -e \" For more info and how to connect from windows, see https://docs.ycrc.yale.edu/clusters-at-yale/guides/jupyter/ MacOS or linux terminal command to create your ssh tunnel ssh -N -L ${ port } : ${ node } : ${ port } ${ user } @ ${ cluster } .hpc.yale.edu Windows MobaXterm info Forwarded port:same as remote port Remote server: ${ node } Remote port: ${ port } SSH server: ${ cluster } .hpc.yale.edu SSH login: $user SSH port: 22 Use a Browser on your local machine to go to: localhost: ${ port } (prefix w/ https:// if using password) \" # load modules or conda environments here # uncomment the following two lines to use your conda environment called notebook_env # module load miniconda # source activate notebook_env # DON'T USE ADDRESS BELOW. # DO USE TOKEN BELOW jupyter-notebook --no-browser --port = ${ port } --ip = ${ node }","title":"Start the Server"},{"location":"clusters-at-yale/guides/jupyter/#start-the-tunnel","text":"Once you have submitted your job and it starts, your notebook server will be ready for you to connect. You can run squeue -u${USER} to check. You will see an \"R\" in the ST or status column for your notebook job if it is running. If you see a \"PD\" in the status column, you will have to wait for your job to start running to connect. The log file with information about how to connect will be in the directory you submitted the script from, and be named jupyter-notebook-[jobid].log where jobid is the slurm id for your job.","title":"Start the Tunnel"},{"location":"clusters-at-yale/guides/jupyter/#macos-and-linux","text":"On a Mac or Linux machine, you can start the tunnel with an SSH command. You can check the output from the job you started to get the specifc info you need.","title":"MacOS and Linux"},{"location":"clusters-at-yale/guides/jupyter/#windows","text":"On a windows machine, we recommend you use MobaXterm. See our guide on connecting with MobaXterm for instructions on how to get set up. You will need to take a look at your job's log file to get the details you need. Then start MobaXterm: Under Tools choose \"MobaSSHTunnel (port forwarding)\". Click the \"New SSH Tunnel\" button. Click the radio button for \"Local port forwarding\". Use the information in your jupyter notebook log file to fill out the boxes. Click Save. On your new tunnel, click the key symbol under the settings column and choose your ssh private key. Click the play button under the Start/Stop column.","title":"Windows"},{"location":"clusters-at-yale/guides/jupyter/#browse-the-notebook","text":"Finally, open a web browser on your local machine and enter the address http://localhost:port where port is the one specified in your log file. The address Jupyter creates by default (the one with the name of a compute node) will not work outside the cluster's network. Since version 5 of jupyter, the notebook will automatically generate a token that allows you to authenticate when you connect. It is long, and will be at the end of the url jupyter generates. It will look something like http://c14n06:9230/?token=**ad0775eaff315e6f1d98b13ef10b919bc6b9ef7d0605cc20** If you run into trouble or need help, send us an email at hpc@yale.edu .","title":"Browse the Notebook"},{"location":"clusters-at-yale/guides/mathematica/","text":"Mathematica Request an Interactive Job Warning The Mathematica program is too large to fit on a login node. If you try to run it there, it will crash. Instead, launch it in an interactive job (see below). To run Mathematica interactively, you need to request an interactive session on a compute node. You could start an interactive session using Slurm. For example, to use 4 cores on 1 node: srun --pty --x11 -c 4 -p interactive -t 4:00:00 bash Note that if you are on macOS, you will need to install an additional program to use the GUI. See our X11 Forwarding documentation for instructions. See our Slurm documentation for more detailed information on requesting resources for interactive jobs. Start Mathematica To launch Mathematica, you will first need to make sure you have the correct module loaded. You can search for all available Mathematica versions: module avail mathematica Load the appropriate module file. For example, to run version 12.0.0: module load Mathematica/12.0.0 The module load command sets up your environment, including the PATH to find the proper version of the Mathematica program. If you would like to avoid running the load command every session, you can run module save and then the Mathematica module will be loaded every time you login. Once you have the appropriate module loaded in an interactive job, start Mathematica. The & will put the program in the background so you can continue to use your terminal session. Mathematica & Configure Environment for Parallel Jobs In order to run parallel Mathematica jobs on our cluster, you will need to configure your Mathematica environment. Go to Evaluate/Parallel Kernel Configuration in the Mathematica window and change the following settings. Disable Local Kernels Go in Cluster Integration and first Enable it if you did not have it Enabled Click Reset To Default From the menu in Cluster Engine , select SLURM Under Kernels , set desired number (we recommend to set it to lower number first to test) Advanced Settings under Native specification, specify time and RAM per kernel, such as \u2014time=02:00:00 \u2014mem=20G Set While[Length[Kernels[]] == 0, LaunchKernels[]] to avoid a timeout issue Request Help or Access to Wolfram Alpha Pro If you need any assistance with your Mathematica program, you can reach out the YCRC Mathematica expert Misha Guy at hpc@yale.edu .","title":"Mathematica"},{"location":"clusters-at-yale/guides/mathematica/#mathematica","text":"","title":"Mathematica"},{"location":"clusters-at-yale/guides/mathematica/#request-an-interactive-job","text":"Warning The Mathematica program is too large to fit on a login node. If you try to run it there, it will crash. Instead, launch it in an interactive job (see below). To run Mathematica interactively, you need to request an interactive session on a compute node. You could start an interactive session using Slurm. For example, to use 4 cores on 1 node: srun --pty --x11 -c 4 -p interactive -t 4:00:00 bash Note that if you are on macOS, you will need to install an additional program to use the GUI. See our X11 Forwarding documentation for instructions. See our Slurm documentation for more detailed information on requesting resources for interactive jobs.","title":"Request an Interactive Job"},{"location":"clusters-at-yale/guides/mathematica/#start-mathematica","text":"To launch Mathematica, you will first need to make sure you have the correct module loaded. You can search for all available Mathematica versions: module avail mathematica Load the appropriate module file. For example, to run version 12.0.0: module load Mathematica/12.0.0 The module load command sets up your environment, including the PATH to find the proper version of the Mathematica program. If you would like to avoid running the load command every session, you can run module save and then the Mathematica module will be loaded every time you login. Once you have the appropriate module loaded in an interactive job, start Mathematica. The & will put the program in the background so you can continue to use your terminal session. Mathematica &","title":"Start Mathematica"},{"location":"clusters-at-yale/guides/mathematica/#configure-environment-for-parallel-jobs","text":"In order to run parallel Mathematica jobs on our cluster, you will need to configure your Mathematica environment. Go to Evaluate/Parallel Kernel Configuration in the Mathematica window and change the following settings. Disable Local Kernels Go in Cluster Integration and first Enable it if you did not have it Enabled Click Reset To Default From the menu in Cluster Engine , select SLURM Under Kernels , set desired number (we recommend to set it to lower number first to test) Advanced Settings under Native specification, specify time and RAM per kernel, such as \u2014time=02:00:00 \u2014mem=20G Set While[Length[Kernels[]] == 0, LaunchKernels[]] to avoid a timeout issue","title":"Configure Environment for Parallel Jobs"},{"location":"clusters-at-yale/guides/mathematica/#request-help-or-access-to-wolfram-alpha-pro","text":"If you need any assistance with your Mathematica program, you can reach out the YCRC Mathematica expert Misha Guy at hpc@yale.edu .","title":"Request Help or Access to Wolfram Alpha Pro"},{"location":"clusters-at-yale/guides/matlab/","text":"MATLAB Find MATLAB Run one of the commands below, which will list available versions and the corresponding module files: module spider matlab Load the appropriate module file. For example, to run version R2014a: module load MATLAB/2014a The module load command sets up your environment, including the PATH to find the proper version of the MATLAB program. Run MATLAB Warning The MATLAB program is too large to fit on a login node. If you try to run it there, it will crash. Instead, launch it in an interactive or batch job (see below). To launch MATLAB, using matlab command. # launch the MATLAB GUI matlab # or launch the MATLAB command line prompt maltab -nodisplay # or to launch a script matlab -nodisplay < runscript.m Interactive Job To run Matlab interactively, you need to create an interactive session on a compute node. You could start an interactive session using 4 cores on 1 node using something like srun --pty --x11 -c 4 -p interactive -t 4:00:00 bash Once your interactive session starts, you can load the appropriate module file and start Matlab as described above. See our Slurm documentation for more detailed information on requesting resources for interactive jobs. Batch Mode (without a GUI) Create a batch script containing both instructions to the scheduler and shell instructions to set up directories and start Matlab. At the point you wish to start Matlab, use a command like: matlab -nodisplay -nosplash -r YourFunction < /dev/null This command will run the contents of YourFunction.m. Your batch submission script must either be in or cd to the directory containing YourFunction.m for this to work. Below is a sample batch script to run Matlab in batch mode on Grace. If the name of the script is runit.sh, you would submit it using sbatch runit.sh Here's a script for Grace: 1 2 3 4 5 6 7 8 #!/bin/bash #SBATCH -J myjob #SBATCH -c 4 #SBATCH -t 24:00:00 #SBATCH -p day module load MATLAB/2016b matlab -nodisplay -nosplash -r YourFunction < /dev/null Unless you specify otherwise (using > redirects), both output and error logs will show up in the slurm-jobid.out log file in the same directory as your submission script. Using More than 12 Cores with Matlab In Matlab, 12 workers is a poorly documented default limit (seemingly for historical reasons) when setting up the parallel environment. You can override it by explicitly setting up your parpool before calling parfor or other parallel functions. parpool(feature('NumCores'));","title":"MATLAB"},{"location":"clusters-at-yale/guides/matlab/#matlab","text":"","title":"MATLAB"},{"location":"clusters-at-yale/guides/matlab/#find-matlab","text":"Run one of the commands below, which will list available versions and the corresponding module files: module spider matlab Load the appropriate module file. For example, to run version R2014a: module load MATLAB/2014a The module load command sets up your environment, including the PATH to find the proper version of the MATLAB program.","title":"Find MATLAB"},{"location":"clusters-at-yale/guides/matlab/#run-matlab","text":"Warning The MATLAB program is too large to fit on a login node. If you try to run it there, it will crash. Instead, launch it in an interactive or batch job (see below). To launch MATLAB, using matlab command. # launch the MATLAB GUI matlab # or launch the MATLAB command line prompt maltab -nodisplay # or to launch a script matlab -nodisplay < runscript.m","title":"Run MATLAB"},{"location":"clusters-at-yale/guides/matlab/#interactive-job","text":"To run Matlab interactively, you need to create an interactive session on a compute node. You could start an interactive session using 4 cores on 1 node using something like srun --pty --x11 -c 4 -p interactive -t 4:00:00 bash Once your interactive session starts, you can load the appropriate module file and start Matlab as described above. See our Slurm documentation for more detailed information on requesting resources for interactive jobs.","title":"Interactive Job"},{"location":"clusters-at-yale/guides/matlab/#batch-mode-without-a-gui","text":"Create a batch script containing both instructions to the scheduler and shell instructions to set up directories and start Matlab. At the point you wish to start Matlab, use a command like: matlab -nodisplay -nosplash -r YourFunction < /dev/null This command will run the contents of YourFunction.m. Your batch submission script must either be in or cd to the directory containing YourFunction.m for this to work. Below is a sample batch script to run Matlab in batch mode on Grace. If the name of the script is runit.sh, you would submit it using sbatch runit.sh Here's a script for Grace: 1 2 3 4 5 6 7 8 #!/bin/bash #SBATCH -J myjob #SBATCH -c 4 #SBATCH -t 24:00:00 #SBATCH -p day module load MATLAB/2016b matlab -nodisplay -nosplash -r YourFunction < /dev/null Unless you specify otherwise (using > redirects), both output and error logs will show up in the slurm-jobid.out log file in the same directory as your submission script.","title":"Batch Mode (without a GUI)"},{"location":"clusters-at-yale/guides/matlab/#using-more-than-12-cores-with-matlab","text":"In Matlab, 12 workers is a poorly documented default limit (seemingly for historical reasons) when setting up the parallel environment. You can override it by explicitly setting up your parpool before calling parfor or other parallel functions. parpool(feature('NumCores'));","title":"Using More than 12 Cores with Matlab"},{"location":"clusters-at-yale/guides/parallel/","text":"GNU Parallel GNU Parallel a simple but powerful way to run independent tasks in parallel. Although it is possible to run on multiple nodes, it is simplest to run on multiple cpus of a single node, and that is what we will consider here. Note that what is presented here just scratches the surface of what parallel can do. Basic Examples Loop Let's parallelize the following bash loop that prints the letters a through f using bash's brace expansion : for letter in { a..f } ; do echo $letter done ... which produces the following output: a b c d e f To achieve the same result, parallel starts some number of workers and then runs tasks on them. The number of workers and tasks need not be the same. You specify the number of workers with -j . The tasks can be generated with a list of arguments specified after the separator ::: . For parallel to perform well, you should allocate at least the same number of CPUs as workers with the slurm option --cpus-per-task or more simply -c . srun --pty -p interactive -c 4 bash module load parallel parallel -j 4 \"echo {}\" ::: { a..f } This runs four workers that each run echo , filling in the argument {} with the next item in the list. This produces the output: Nested Loop Let's parallelize the following nested bash loop. for letter in { a..c } do for number in { 1 ..7..2 } do echo $letter $number done done ... which produces the following output: a 1 a 2 a 3 b 1 b 2 b 3 c 1 c 2 c 3 You can use the ::: separator with parallel to specify multiple lists of parameters you would like to iterate over. Then you can refer to them by one-based index, e.g. list one is {1} . Using these, you can ask parallel to execute combinations of parameters. Here is a way to recreate the result of the serial bash loop above: parallel -j 4 \"echo {1} {2}\" ::: { a..c } ::: { 1 ..3 } Advanced Examples md5sum You have a number of files scattered throughout a directory tree. Their names end with fastq.gz, e.g. d1/d3/sample3.fastq.gz. You'd like to run md5sum on each, and put the output in a file in the same directory, with a filename ending with .md5sum, e.g. d1/d3/sample3.md5sum. Here is a script that will do that in parallel, using 16 cpus on one node of the cluster: #!/bin/bash #SBATCH -c 16 module load parallel parallel -j ${ SLURM_CPUS_PER_TASK } --plus \"echo {}; md5sum {} > {/fastq.gz/md5sum.new}\" ::: $( find . -name \"*.fastq.gz\" -print ) The $(find . -name \"*.fastq.gz\" -print) portion of the command returns all of the files of interest. They will be plugged into the {} in the md5sum command. {/fastq.gz/md5sum.new} does a string replacement on the filename, producing the desired output filename. String replacement requires the --plus flag to parallel, which enables a number of powerful string manipulation features. Finally, we pass -j ${SLURM_CPUS_PER_TASK} so that parallel will use all of the allocated cpus, however many there are. Parameter Sweep You want to run a simulation program that takes a number of input parameters, and you want to sample a variety of values for each parameter. #!/bin/bash #SBATCH -c 16 module load parallel parallel -j ${ SLURM_CPUS_PER_TASK } simulate { 1 } { 2 } { 3 } ::: { 1 ..5 } ::: 2 16 ::: { 5 ..50..5 } This will run 100 jobs, each with parameters that vary as : simulate 1 2 5 simulate 1 2 10 simulate 1 2 15 ... simulate 5 16 45 simulate 5 16 50 If simulate doesn't create unique output based on parameters, you can use redirection so you can review results from each task. You'll need to use quotes so that the > is seen as part of the command: parallel -j ${ SLURM_CPUS_PER_TASK } \"simulate {1} {2} {3} > results_{1}_{2}_{3}.out\" ::: $( seq 1 5 ) ::: 2 16 ::: $( seq 5 5 50 )","title":"GNU Parallel"},{"location":"clusters-at-yale/guides/parallel/#gnu-parallel","text":"GNU Parallel a simple but powerful way to run independent tasks in parallel. Although it is possible to run on multiple nodes, it is simplest to run on multiple cpus of a single node, and that is what we will consider here. Note that what is presented here just scratches the surface of what parallel can do.","title":"GNU Parallel"},{"location":"clusters-at-yale/guides/parallel/#basic-examples","text":"","title":"Basic Examples"},{"location":"clusters-at-yale/guides/parallel/#loop","text":"Let's parallelize the following bash loop that prints the letters a through f using bash's brace expansion : for letter in { a..f } ; do echo $letter done ... which produces the following output: a b c d e f To achieve the same result, parallel starts some number of workers and then runs tasks on them. The number of workers and tasks need not be the same. You specify the number of workers with -j . The tasks can be generated with a list of arguments specified after the separator ::: . For parallel to perform well, you should allocate at least the same number of CPUs as workers with the slurm option --cpus-per-task or more simply -c . srun --pty -p interactive -c 4 bash module load parallel parallel -j 4 \"echo {}\" ::: { a..f } This runs four workers that each run echo , filling in the argument {} with the next item in the list. This produces the output:","title":"Loop"},{"location":"clusters-at-yale/guides/parallel/#nested-loop","text":"Let's parallelize the following nested bash loop. for letter in { a..c } do for number in { 1 ..7..2 } do echo $letter $number done done ... which produces the following output: a 1 a 2 a 3 b 1 b 2 b 3 c 1 c 2 c 3 You can use the ::: separator with parallel to specify multiple lists of parameters you would like to iterate over. Then you can refer to them by one-based index, e.g. list one is {1} . Using these, you can ask parallel to execute combinations of parameters. Here is a way to recreate the result of the serial bash loop above: parallel -j 4 \"echo {1} {2}\" ::: { a..c } ::: { 1 ..3 }","title":"Nested Loop"},{"location":"clusters-at-yale/guides/parallel/#advanced-examples","text":"","title":"Advanced Examples"},{"location":"clusters-at-yale/guides/parallel/#md5sum","text":"You have a number of files scattered throughout a directory tree. Their names end with fastq.gz, e.g. d1/d3/sample3.fastq.gz. You'd like to run md5sum on each, and put the output in a file in the same directory, with a filename ending with .md5sum, e.g. d1/d3/sample3.md5sum. Here is a script that will do that in parallel, using 16 cpus on one node of the cluster: #!/bin/bash #SBATCH -c 16 module load parallel parallel -j ${ SLURM_CPUS_PER_TASK } --plus \"echo {}; md5sum {} > {/fastq.gz/md5sum.new}\" ::: $( find . -name \"*.fastq.gz\" -print ) The $(find . -name \"*.fastq.gz\" -print) portion of the command returns all of the files of interest. They will be plugged into the {} in the md5sum command. {/fastq.gz/md5sum.new} does a string replacement on the filename, producing the desired output filename. String replacement requires the --plus flag to parallel, which enables a number of powerful string manipulation features. Finally, we pass -j ${SLURM_CPUS_PER_TASK} so that parallel will use all of the allocated cpus, however many there are.","title":"md5sum"},{"location":"clusters-at-yale/guides/parallel/#parameter-sweep","text":"You want to run a simulation program that takes a number of input parameters, and you want to sample a variety of values for each parameter. #!/bin/bash #SBATCH -c 16 module load parallel parallel -j ${ SLURM_CPUS_PER_TASK } simulate { 1 } { 2 } { 3 } ::: { 1 ..5 } ::: 2 16 ::: { 5 ..50..5 } This will run 100 jobs, each with parameters that vary as : simulate 1 2 5 simulate 1 2 10 simulate 1 2 15 ... simulate 5 16 45 simulate 5 16 50 If simulate doesn't create unique output based on parameters, you can use redirection so you can review results from each task. You'll need to use quotes so that the > is seen as part of the command: parallel -j ${ SLURM_CPUS_PER_TASK } \"simulate {1} {2} {3} > results_{1}_{2}_{3}.out\" ::: $( seq 1 5 ) ::: 2 16 ::: $( seq 5 5 50 )","title":"Parameter Sweep"},{"location":"clusters-at-yale/guides/r/","text":"R Load R Conda-based R Environments We recommend setting up your own R installation using Conda so you can manage your own packages and dependencies. You can find detailed instructions on our Conda page . The conda package repository has many (though not all) of the common R packages that can be installed with the typical conda syntax: # Source the conda environment $ source activate my_r_env # Install the lattice package (r-lattice) from the r channel (-c r) ( my_r_env ) $ conda install -c r r-lattice If there are packages that conda does not provide, you can install them locally from within R using the install.packages function. To install a package ( lattice for example) directly, simply run: $ module load miniconda $ source activate my_r_env ( my_r_env ) $ R > install.packages ( \"lattice\" , repos = \"http://cran.r-project.org\" ) Each conda environment manages a unique $R_LIBS directory so that there is no conflict between installations. System Environment We also provide a basic R installing on some of the clusters which you can use. However, if you find it is missing packages you need, we recommend setting up your own environment as described above. Run one of the commands below, which will list available versions and the corresponding module files: module avail R Load the appropriate module file. For example, to run version 3.4.1: module load R/3.4.1-foss-2016b The module load command sets up your environment, including the PATH to find the proper version of R. This installation includes the most commonly used packages, but if something specific is required, packages can be installed into your local space. To get started load the R module and start R: $ module load R/3.4.1-foss-2016b $ R > install.packages ( \"lattice\" , repos = \"http://cran.r-project.org\" ) This will throw a warning like: Warning in install.packages ( \"lattice\" ) : 'lib = \"/ysm-gpfs/apps/software/R/3.4.1-foss-2016b/lib64/R/library\"' is not writable Would you like to create a personal library ~/R/x86_64-unknown-linux-gnu-library/3.3 to install packages into? ( y/n ) This will install the lattice package locally and will then be available to load into an R session. Alternatively, you can install R packages from the command line using: conda install -c [ channel-name ] [ package-name ] You can search for available packages on the conda website. Parallel R It is often desireable to use R in parallel across multiple nodes. While there are a few different ways this can be achieved, we recommend using conda to set up a specific R environment with the required packages and libraries. In particular, you will need Rmpi , doMC , and doMPI . The first two can be installed via conda, while the last one must be installed manually. To get started, load the miniconda module and create a new environment using the conda-forge channel: # load the miniconda module module load miniconda # create the environment with the required packages conda create --name parallel_r -c conda-forge r-base r-essentials r-doMC r-Rmpi # activate the environment source activate parallel_r This will produce an environment that is nearly ready to-go. The last step is to install doMPI , which at the moment is not available via conda . We can use the install.packages method from within R to get this final piece: ( parallel_r ) $ R > install.packages ( 'doMPI' ) It's important that this last step is performed on a login node so that it doesn't interfere with the SLURM scheduler. Once this is complete, you should have a fully functional parallel-enabled R environment. To test it, we can create a simple R script named ex1.R library ( \"Rmpi\" ) n <- mpi.comm.size ( 0 ) me <- mpi.comm.rank ( 0 ) mpi.barrier ( 0 ) val <- 777 mpi.bcast ( val , 1 , 0 , 0 ) print ( paste ( \"me\" , me , \"val\" , val )) mpi.barrier ( 0 ) mpi.quit () Then we can launch it with an sbatch script ( ex1.sh ): #!/bin/bash #SBATCH -n 4 -N 4 -t 5:00 #SBATCH --mail-type=none module purge module load miniconda source activate parallel_r mpirun R --slave -f ex1.R This script should execute a simple broadcast and complete in a few seconds. Run R To run R, launch it using the R command. # launch an R session R # or to launch a script R --slave -f myscript.R Warning The R program is too large to fit on a login node. If you try to run it there, it will crash. Instead, launch it in an interactive or batch job (see below). Interactive Job To run R interactively, you need to launch an interactive session on a compute node. For example srun --pty -p interactive -t 4 :00:00 bash Once your interactive session starts, you can load the appropriate module file and start R as described above. See our Slurm documentation for more detailed information on requesting resources for interactive jobs. Batch Mode To run R in batch mode, you create a batch script . In that script, you would invoke your R script in batch mode. #!/bin/bash #SBATCH -J my_r_program R --slave -f myscript.R","title":"R"},{"location":"clusters-at-yale/guides/r/#r","text":"","title":"R"},{"location":"clusters-at-yale/guides/r/#load-r","text":"","title":"Load R"},{"location":"clusters-at-yale/guides/r/#conda-based-r-environments","text":"We recommend setting up your own R installation using Conda so you can manage your own packages and dependencies. You can find detailed instructions on our Conda page . The conda package repository has many (though not all) of the common R packages that can be installed with the typical conda syntax: # Source the conda environment $ source activate my_r_env # Install the lattice package (r-lattice) from the r channel (-c r) ( my_r_env ) $ conda install -c r r-lattice If there are packages that conda does not provide, you can install them locally from within R using the install.packages function. To install a package ( lattice for example) directly, simply run: $ module load miniconda $ source activate my_r_env ( my_r_env ) $ R > install.packages ( \"lattice\" , repos = \"http://cran.r-project.org\" ) Each conda environment manages a unique $R_LIBS directory so that there is no conflict between installations.","title":"Conda-based R Environments"},{"location":"clusters-at-yale/guides/r/#system-environment","text":"We also provide a basic R installing on some of the clusters which you can use. However, if you find it is missing packages you need, we recommend setting up your own environment as described above. Run one of the commands below, which will list available versions and the corresponding module files: module avail R Load the appropriate module file. For example, to run version 3.4.1: module load R/3.4.1-foss-2016b The module load command sets up your environment, including the PATH to find the proper version of R. This installation includes the most commonly used packages, but if something specific is required, packages can be installed into your local space. To get started load the R module and start R: $ module load R/3.4.1-foss-2016b $ R > install.packages ( \"lattice\" , repos = \"http://cran.r-project.org\" ) This will throw a warning like: Warning in install.packages ( \"lattice\" ) : 'lib = \"/ysm-gpfs/apps/software/R/3.4.1-foss-2016b/lib64/R/library\"' is not writable Would you like to create a personal library ~/R/x86_64-unknown-linux-gnu-library/3.3 to install packages into? ( y/n ) This will install the lattice package locally and will then be available to load into an R session. Alternatively, you can install R packages from the command line using: conda install -c [ channel-name ] [ package-name ] You can search for available packages on the conda website.","title":"System Environment"},{"location":"clusters-at-yale/guides/r/#parallel-r","text":"It is often desireable to use R in parallel across multiple nodes. While there are a few different ways this can be achieved, we recommend using conda to set up a specific R environment with the required packages and libraries. In particular, you will need Rmpi , doMC , and doMPI . The first two can be installed via conda, while the last one must be installed manually. To get started, load the miniconda module and create a new environment using the conda-forge channel: # load the miniconda module module load miniconda # create the environment with the required packages conda create --name parallel_r -c conda-forge r-base r-essentials r-doMC r-Rmpi # activate the environment source activate parallel_r This will produce an environment that is nearly ready to-go. The last step is to install doMPI , which at the moment is not available via conda . We can use the install.packages method from within R to get this final piece: ( parallel_r ) $ R > install.packages ( 'doMPI' ) It's important that this last step is performed on a login node so that it doesn't interfere with the SLURM scheduler. Once this is complete, you should have a fully functional parallel-enabled R environment. To test it, we can create a simple R script named ex1.R library ( \"Rmpi\" ) n <- mpi.comm.size ( 0 ) me <- mpi.comm.rank ( 0 ) mpi.barrier ( 0 ) val <- 777 mpi.bcast ( val , 1 , 0 , 0 ) print ( paste ( \"me\" , me , \"val\" , val )) mpi.barrier ( 0 ) mpi.quit () Then we can launch it with an sbatch script ( ex1.sh ): #!/bin/bash #SBATCH -n 4 -N 4 -t 5:00 #SBATCH --mail-type=none module purge module load miniconda source activate parallel_r mpirun R --slave -f ex1.R This script should execute a simple broadcast and complete in a few seconds.","title":"Parallel R"},{"location":"clusters-at-yale/guides/r/#run-r","text":"To run R, launch it using the R command. # launch an R session R # or to launch a script R --slave -f myscript.R Warning The R program is too large to fit on a login node. If you try to run it there, it will crash. Instead, launch it in an interactive or batch job (see below).","title":"Run R"},{"location":"clusters-at-yale/guides/r/#interactive-job","text":"To run R interactively, you need to launch an interactive session on a compute node. For example srun --pty -p interactive -t 4 :00:00 bash Once your interactive session starts, you can load the appropriate module file and start R as described above. See our Slurm documentation for more detailed information on requesting resources for interactive jobs.","title":"Interactive Job"},{"location":"clusters-at-yale/guides/r/#batch-mode","text":"To run R in batch mode, you create a batch script . In that script, you would invoke your R script in batch mode. #!/bin/bash #SBATCH -J my_r_program R --slave -f myscript.R","title":"Batch Mode"},{"location":"clusters-at-yale/guides/rclone/","text":"Rclone rclone is a command line program to sync files and directories to and from all major cloud storage sites and services. You can use rclone to sync files and directories between Yale clusters and Yale Box and google drive. The following instructions covers basics to setup and use rclone on Yale clusters. For more information about Rclone, please visit its website at https://rclone.org . Setup Rclone on Yale clusters You need to run rclone config to receive authorization of accessing a cloud service. Since rclone config will bring up a browser to authorize the cloud service, you will need X Windows on the host where you run rclone config . This means, to setup rclone on one of Yale clusters, you need to login to the cluster with X11 forwarding . X11 forwarding is extremly slow. A better way is to run VNC on the cluster and setup a tunnel to access the remote VNC desktop on the cluster. If the cluster is Grace or Farnam, the best way is to use their OnDemand portal . The example below is a screen dump from setting up rclone on Farnam for Yale Box. [ pl543@transfer-farnam ~ ] $ module load Rclone [ pl543@transfer-farnam ~ ] $ rclone config 2019 / 10 / 21 09 : 14 : 05 NOTICE : Config file \"/home/pl543/.config/rclone/rclone.conf\" not found - using defaults No remotes found - make a new one n ) New remote s ) Set configuration password q ) Quit config n / s / q > n name > remote Type of storage to configure . Enter a string value . Press Enter for the default ( \"\" ). Choose a number from below , or type in your own value 1 / 1 Fichier \\ \"fichier\" [ snip ] 6 / Box \\ \"box\" [ snip ] Storage > box ** See help for box backend at : https : // rclone . org / box / ** Box App Client Id . Leave blank normally . Enter a string value . Press Enter for the default ( \"\" ). client_id > Box App Client Secret Leave blank normally . Enter a string value . Press Enter for the default ( \"\" ). client_secret > Edit advanced config ? ( y / n ) y ) Yes n ) No y / n > n Remote config Use auto config ? * Say Y if not sure * Say N if you are working on a remote or headless machine y ) Yes n ) No y / n > y If your browser does not open automatically go to the following link : http : // 127.0.0.1 : 53682 / auth Log in and authorize rclone for access Waiting for code ... ( In the browser opened , login with your Yale email and password . You will be redirect to CAS authentication . This may take a while .) Got code -------------------- [ remote ] type = box token = { \"access_token\" : \"PjIXHUZ34VQSmeUZ9r6bhc9ux44KMU0e\" , \"token_type\" : \"bearer\" , \"refresh_token\" : \"VumWPWP5Nd0M2C1GyfgfJL51zUeWPPVLc6VC6lBQduEPsQ9a6ibSor2dvHmyZ6B8\" , \"expiry\" : \"2019-10-21T11:00:36.839586736-04:00\" } -------------------- y ) Yes this is OK e ) Edit this remote d ) Delete this remote y / e / d > y Current remotes : Name Type ==== ==== remote box e ) Edit existing remote n ) New remote d ) Delete remote r ) Rename remote c ) Copy remote s ) Set configuration password q ) Quit config e / n / d / r / c / s / q > q rclone setup creates a file that stores all the config of rclone . You can check the file name with rclone config file . The config file can be copied to other clusters so that you can use rclone on the other clusters without running rclone config again. Setting up rclone for \"Google Drive\" follows a similar procedure. Use Rclone on Yale clusters List files rclone ls remote:/ Copy files rclone copy remote:/path/to/filename . rclone copy filename remote:/path/to/ Help rclone help","title":"Rclone"},{"location":"clusters-at-yale/guides/rclone/#rclone","text":"rclone is a command line program to sync files and directories to and from all major cloud storage sites and services. You can use rclone to sync files and directories between Yale clusters and Yale Box and google drive. The following instructions covers basics to setup and use rclone on Yale clusters. For more information about Rclone, please visit its website at https://rclone.org .","title":"Rclone"},{"location":"clusters-at-yale/guides/rclone/#setup-rclone-on-yale-clusters","text":"You need to run rclone config to receive authorization of accessing a cloud service. Since rclone config will bring up a browser to authorize the cloud service, you will need X Windows on the host where you run rclone config . This means, to setup rclone on one of Yale clusters, you need to login to the cluster with X11 forwarding . X11 forwarding is extremly slow. A better way is to run VNC on the cluster and setup a tunnel to access the remote VNC desktop on the cluster. If the cluster is Grace or Farnam, the best way is to use their OnDemand portal . The example below is a screen dump from setting up rclone on Farnam for Yale Box. [ pl543@transfer-farnam ~ ] $ module load Rclone [ pl543@transfer-farnam ~ ] $ rclone config 2019 / 10 / 21 09 : 14 : 05 NOTICE : Config file \"/home/pl543/.config/rclone/rclone.conf\" not found - using defaults No remotes found - make a new one n ) New remote s ) Set configuration password q ) Quit config n / s / q > n name > remote Type of storage to configure . Enter a string value . Press Enter for the default ( \"\" ). Choose a number from below , or type in your own value 1 / 1 Fichier \\ \"fichier\" [ snip ] 6 / Box \\ \"box\" [ snip ] Storage > box ** See help for box backend at : https : // rclone . org / box / ** Box App Client Id . Leave blank normally . Enter a string value . Press Enter for the default ( \"\" ). client_id > Box App Client Secret Leave blank normally . Enter a string value . Press Enter for the default ( \"\" ). client_secret > Edit advanced config ? ( y / n ) y ) Yes n ) No y / n > n Remote config Use auto config ? * Say Y if not sure * Say N if you are working on a remote or headless machine y ) Yes n ) No y / n > y If your browser does not open automatically go to the following link : http : // 127.0.0.1 : 53682 / auth Log in and authorize rclone for access Waiting for code ... ( In the browser opened , login with your Yale email and password . You will be redirect to CAS authentication . This may take a while .) Got code -------------------- [ remote ] type = box token = { \"access_token\" : \"PjIXHUZ34VQSmeUZ9r6bhc9ux44KMU0e\" , \"token_type\" : \"bearer\" , \"refresh_token\" : \"VumWPWP5Nd0M2C1GyfgfJL51zUeWPPVLc6VC6lBQduEPsQ9a6ibSor2dvHmyZ6B8\" , \"expiry\" : \"2019-10-21T11:00:36.839586736-04:00\" } -------------------- y ) Yes this is OK e ) Edit this remote d ) Delete this remote y / e / d > y Current remotes : Name Type ==== ==== remote box e ) Edit existing remote n ) New remote d ) Delete remote r ) Rename remote c ) Copy remote s ) Set configuration password q ) Quit config e / n / d / r / c / s / q > q rclone setup creates a file that stores all the config of rclone . You can check the file name with rclone config file . The config file can be copied to other clusters so that you can use rclone on the other clusters without running rclone config again. Setting up rclone for \"Google Drive\" follows a similar procedure.","title":"Setup Rclone on Yale clusters"},{"location":"clusters-at-yale/guides/rclone/#use-rclone-on-yale-clusters","text":"","title":"Use Rclone on Yale clusters"},{"location":"clusters-at-yale/guides/rclone/#list-files","text":"rclone ls remote:/","title":"List files"},{"location":"clusters-at-yale/guides/rclone/#copy-files","text":"rclone copy remote:/path/to/filename . rclone copy filename remote:/path/to/","title":"Copy files"},{"location":"clusters-at-yale/guides/rclone/#help","text":"rclone help","title":"Help"},{"location":"clusters-at-yale/guides/singularity/","text":"Singularity Singularity is a linux container technology that is well suited to use in shared-user environments such as the clusters we maintain at Yale. It is similar to Docker ; You can bring with you a stack of software, libraries, and a Linux operating system that is independent of the host computer you run the container on. This can be very useful if you want to share your software environment with other researchers or yourself across several computers. Because Singularity containers run as the user that started them and mount home directories by default, you can usually see the data you're interested in working on that is stored on a host computer without any extra work. Below we will outline some common use cases covering the creation and use of containers. There is also excellent documentation available on the full and official user guide for Singularity . We are happy to help, just email us with your questions. Warning On the Yale clusters, Singularity is not installed on login nodes. You will need to run it from compute nodes. Singularity Containers Images are the file(s) you use to run your container. Singularity images are single files that usually end in .sif and are read-only by default, meaning changes you make to the environment inside the container are not persistent. Use a Pre-existing Container If someone has already built a container that suits your needs, you can use it directly. Singularity images are single files that can be transferred to the clusters. You can fetch images from container registries such as Docker Hub , Singularity Hub , and Sylabs Container Library . Container images can take up a lot of disk space (dozens of gigabytes), so you may want to change the default location singularity uses to cache these files. To do this before getting started, you should add something like the example below to to your ~/.bashrc file: # set SINGULARITY_CACHEDIR if you want to pull files (which can get big) somewhere other than $HOME/.singularity # e.g. export SINGULARITY_CACHEDIR = ~/scratch60/.singularity Here are some examples of getting containers already built by someone else with singularity: # from Docker Hub (https://hub.docker.com/) singularity build ubuntu-18.10.sif docker://ubuntu:18.10 singularity build tensorflow-10.0-py3.sif docker://tensorflow/tensorflow:1.10.0-py3 # from Singularity Hub (https://singularity-hub.org/) singularity build bioconvert-latest.sif shub://biokit/bioconvert:latest Build Your Own Container You can define a container image to be exactly how you want/need it to be, including applications, libraries, and files of your choosing with a definition file. Singularity definition files are similar to Docker's Dockerfile , but use different syntax. To build a container from a definition file, you need administrative privileges on a Linux machine where Singularity is installed . Sylabs provides a cloud-based container building platform ( link ). This web-interface allows for the uploading and building of customized containers in the cloud. Additionally, users can generate a token to allow remote building from the command-line: # login to the remote platform with their token: singularity remote login # launch a remote build of a local definition file (my_container.def) singularity build --remote my_container.sif my_container.def This can be performed from any compute note on the clusters without elevated privileges. For full definition files and more documentation please see the singularity site . Header Every container definition must begin with a header that defines what image to start with, or bootstrap from. This can be an official Linux distribution or someone else's container that gets you nearly what you want. To start from Ubuntu Bionic Beaver (18.04 LTS): Bootstrap: docker From: ubuntu:18.04 Or an Nvidia developer image Bootstrap: docker From: nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04 The rest of the sections all begin with % and the section name. You will see section contents indented by convention, but this is not required. %labels The labels section allows you to define metadata for your container: %labels Name Maintainer \"YCRC Support Team\" <hpc@yale.edu>Version v99.9 Architecture x86_64 URL https://research.computing.yale.edu/</hpc@yale.edu> You can examine container metadata with the singularity inspect command. %files If you'd like to copy any files from the system you are building on, you do so in the %files section. Each line in the files section is a pair of source and destination paths, where the source is on your host system, and destination is a path in the container. %files sample_data.tar /opt/sample_data/ example_script.sh /opt/sample_data/ %post The post section is where you can run updates, installs, etc in your container to customize it. %post echo \"Customizing Ubuntu\" apt-get update apt-get -y install software-properties-common build-essential cmake add-apt-repository universe apt-get update apt-get -y libboost-all-dev libgl1-mesa-dev libglu1-mesa-dev cd /tmp git clone https://github.com/gitdudette/myapp && cd myapp # ... etc etc %environment The environment section allows you to define environment variables for your container. These variables are available when you run the built container, not during its build. %environment export PATH = /opt/my_app/bin: $PATH export LD_LIBRARY_PATH = /opt/my_app/lib: $LD_LIBRARY_PATH Building To finally build your container after saving your definition file as my_app.def , for example, you would run singularity build my_app.sif my_app.def Use a Container Image Once you have a container image, you can run it as a part of a batch job, or interactively. Interactively To get a shell in a container so you can interactively work in its environment: singularity shell --shell /bin/bash containername.sif In a Job Script You can also run applications from your container non-interactively as you would in a batch job. If I wanted to run a script called my_script.py using my container's python: singularity exec containername.sif python my_script.py Environment Variables If you are unsure if you are running inside or outside your container, you can run: echo $SINGULARITY_NAME If you get back text, you are in your container. If you'd like to pass environment variables into your container, you can do so by defining them prefixed with SINGULARITYENV_ . For Example: export SINGULARITYENV_BLASTDB = /home/me/db/blast singularity exec my_blast_image.sif env | grep BLAST Should return BLASTDB=/home/me/db/blast , which means you set the BLASTDB environment variable in the container properly. Additional Notes MPI MPI support for Singularity is relatively straight-forward. The only thing to watch is to make sure that you are using the same version of MPI inside your container as you are on the cluster. GPUs You can use gpu-accelerated code inside your container, which will need most everything also installed in your container (e.g. CUDA, cuDNN). In order for your applications to have access to the right drivers on the host machine, use the --nv flag. For example: singularity exec --nv tensorflow-10.0-py3.sif python ./my-tf-model.py Home Directories Sometimes the maintainer of a docker container you are trying to use installed software into a special user's home directory. If you need access to someone's home directory that exists in the container and not on the host, you should add the --contain option. Unfortunately, you will also then have to explicitly tell Singularity about the paths that you want to use from inside the container with the --bind option. singularity shell --shell /bin/bash --contain --bind /gpfs/ysm/project/be59:/home/be59/project bioconvert-latest.sif","title":"Singularity"},{"location":"clusters-at-yale/guides/singularity/#singularity","text":"Singularity is a linux container technology that is well suited to use in shared-user environments such as the clusters we maintain at Yale. It is similar to Docker ; You can bring with you a stack of software, libraries, and a Linux operating system that is independent of the host computer you run the container on. This can be very useful if you want to share your software environment with other researchers or yourself across several computers. Because Singularity containers run as the user that started them and mount home directories by default, you can usually see the data you're interested in working on that is stored on a host computer without any extra work. Below we will outline some common use cases covering the creation and use of containers. There is also excellent documentation available on the full and official user guide for Singularity . We are happy to help, just email us with your questions. Warning On the Yale clusters, Singularity is not installed on login nodes. You will need to run it from compute nodes.","title":"Singularity"},{"location":"clusters-at-yale/guides/singularity/#singularity-containers","text":"Images are the file(s) you use to run your container. Singularity images are single files that usually end in .sif and are read-only by default, meaning changes you make to the environment inside the container are not persistent.","title":"Singularity Containers"},{"location":"clusters-at-yale/guides/singularity/#use-a-pre-existing-container","text":"If someone has already built a container that suits your needs, you can use it directly. Singularity images are single files that can be transferred to the clusters. You can fetch images from container registries such as Docker Hub , Singularity Hub , and Sylabs Container Library . Container images can take up a lot of disk space (dozens of gigabytes), so you may want to change the default location singularity uses to cache these files. To do this before getting started, you should add something like the example below to to your ~/.bashrc file: # set SINGULARITY_CACHEDIR if you want to pull files (which can get big) somewhere other than $HOME/.singularity # e.g. export SINGULARITY_CACHEDIR = ~/scratch60/.singularity Here are some examples of getting containers already built by someone else with singularity: # from Docker Hub (https://hub.docker.com/) singularity build ubuntu-18.10.sif docker://ubuntu:18.10 singularity build tensorflow-10.0-py3.sif docker://tensorflow/tensorflow:1.10.0-py3 # from Singularity Hub (https://singularity-hub.org/) singularity build bioconvert-latest.sif shub://biokit/bioconvert:latest","title":"Use a Pre-existing Container"},{"location":"clusters-at-yale/guides/singularity/#build-your-own-container","text":"You can define a container image to be exactly how you want/need it to be, including applications, libraries, and files of your choosing with a definition file. Singularity definition files are similar to Docker's Dockerfile , but use different syntax. To build a container from a definition file, you need administrative privileges on a Linux machine where Singularity is installed . Sylabs provides a cloud-based container building platform ( link ). This web-interface allows for the uploading and building of customized containers in the cloud. Additionally, users can generate a token to allow remote building from the command-line: # login to the remote platform with their token: singularity remote login # launch a remote build of a local definition file (my_container.def) singularity build --remote my_container.sif my_container.def This can be performed from any compute note on the clusters without elevated privileges. For full definition files and more documentation please see the singularity site .","title":"Build Your Own Container"},{"location":"clusters-at-yale/guides/singularity/#header","text":"Every container definition must begin with a header that defines what image to start with, or bootstrap from. This can be an official Linux distribution or someone else's container that gets you nearly what you want. To start from Ubuntu Bionic Beaver (18.04 LTS): Bootstrap: docker From: ubuntu:18.04 Or an Nvidia developer image Bootstrap: docker From: nvidia/cuda:9.2-cudnn7-devel-ubuntu18.04 The rest of the sections all begin with % and the section name. You will see section contents indented by convention, but this is not required.","title":"Header"},{"location":"clusters-at-yale/guides/singularity/#labels","text":"The labels section allows you to define metadata for your container: %labels Name Maintainer \"YCRC Support Team\" <hpc@yale.edu>Version v99.9 Architecture x86_64 URL https://research.computing.yale.edu/</hpc@yale.edu> You can examine container metadata with the singularity inspect command.","title":"%labels"},{"location":"clusters-at-yale/guides/singularity/#files","text":"If you'd like to copy any files from the system you are building on, you do so in the %files section. Each line in the files section is a pair of source and destination paths, where the source is on your host system, and destination is a path in the container. %files sample_data.tar /opt/sample_data/ example_script.sh /opt/sample_data/","title":"%files"},{"location":"clusters-at-yale/guides/singularity/#post","text":"The post section is where you can run updates, installs, etc in your container to customize it. %post echo \"Customizing Ubuntu\" apt-get update apt-get -y install software-properties-common build-essential cmake add-apt-repository universe apt-get update apt-get -y libboost-all-dev libgl1-mesa-dev libglu1-mesa-dev cd /tmp git clone https://github.com/gitdudette/myapp && cd myapp # ... etc etc","title":"%post"},{"location":"clusters-at-yale/guides/singularity/#environment","text":"The environment section allows you to define environment variables for your container. These variables are available when you run the built container, not during its build. %environment export PATH = /opt/my_app/bin: $PATH export LD_LIBRARY_PATH = /opt/my_app/lib: $LD_LIBRARY_PATH","title":"%environment"},{"location":"clusters-at-yale/guides/singularity/#building","text":"To finally build your container after saving your definition file as my_app.def , for example, you would run singularity build my_app.sif my_app.def","title":"Building"},{"location":"clusters-at-yale/guides/singularity/#use-a-container-image","text":"Once you have a container image, you can run it as a part of a batch job, or interactively.","title":"Use a Container Image"},{"location":"clusters-at-yale/guides/singularity/#interactively","text":"To get a shell in a container so you can interactively work in its environment: singularity shell --shell /bin/bash containername.sif","title":"Interactively"},{"location":"clusters-at-yale/guides/singularity/#in-a-job-script","text":"You can also run applications from your container non-interactively as you would in a batch job. If I wanted to run a script called my_script.py using my container's python: singularity exec containername.sif python my_script.py","title":"In a Job Script"},{"location":"clusters-at-yale/guides/singularity/#environment-variables","text":"If you are unsure if you are running inside or outside your container, you can run: echo $SINGULARITY_NAME If you get back text, you are in your container. If you'd like to pass environment variables into your container, you can do so by defining them prefixed with SINGULARITYENV_ . For Example: export SINGULARITYENV_BLASTDB = /home/me/db/blast singularity exec my_blast_image.sif env | grep BLAST Should return BLASTDB=/home/me/db/blast , which means you set the BLASTDB environment variable in the container properly.","title":"Environment Variables"},{"location":"clusters-at-yale/guides/singularity/#additional-notes","text":"","title":"Additional Notes"},{"location":"clusters-at-yale/guides/singularity/#mpi","text":"MPI support for Singularity is relatively straight-forward. The only thing to watch is to make sure that you are using the same version of MPI inside your container as you are on the cluster.","title":"MPI"},{"location":"clusters-at-yale/guides/singularity/#gpus","text":"You can use gpu-accelerated code inside your container, which will need most everything also installed in your container (e.g. CUDA, cuDNN). In order for your applications to have access to the right drivers on the host machine, use the --nv flag. For example: singularity exec --nv tensorflow-10.0-py3.sif python ./my-tf-model.py","title":"GPUs"},{"location":"clusters-at-yale/guides/singularity/#home-directories","text":"Sometimes the maintainer of a docker container you are trying to use installed software into a special user's home directory. If you need access to someone's home directory that exists in the container and not on the host, you should add the --contain option. Unfortunately, you will also then have to explicitly tell Singularity about the paths that you want to use from inside the container with the --bind option. singularity shell --shell /bin/bash --contain --bind /gpfs/ysm/project/be59:/home/be59/project bioconvert-latest.sif","title":"Home Directories"},{"location":"clusters-at-yale/guides/tmux/","text":"tmux tmux is a \"terminal multiplexer\", it enables a number of terminals (or windows) to be accessed and controlled from a single terminal. tmux is a great way to save an interactive session between connections you make to the clusters. You can reconnect to the session from a workstation in your lab or from your laptop from home! Get Started To begin a tmux session named myproject, type tmux new -s myproject You should see a bar across the bottom of your terminal window now that gives you some information about your session. If you are disconnected or detached from this session, anything you were doing will still be there waiting when you reattach The most important shortcut to remember is Ctrl + b (hold the ctrl or control key, then type \"b\"). This is how you signal to tmux that the following keystroke is meant for it and not the session you are working in. For example: if you want to gracefully detach from your session, you can type Ctrl + b , then d for detach. To reattach to our sample tmux session after detatching, type: tmux attach -t myproject #If you are lazy and have only one session running, #This works too: tmux a Lines starting with a \"#\" denote a commented line, which aren't read as code Finally, to exit, you can type exit or Ctrl + d tmux on the Clusters Using tmux on the cluster allows you to create interactive allocations that you can detach from. Normally, if you get an interactive allocation (e.g. srun --pty) then disconnect from the cluster, for example by putting your laptop to sleep, your allocation will be terminated and your job killed. Using tmux, you can detach gracefully and tmux will maintain your allocation. Here is how to do this correctly: ssh to your cluster of choice Start tmux Inside your tmux session, submit an interactive job with srun. See the Slurm documentation for more details Inside your job allocation (on a compute node), start your application (e.g. matlab) Detach from tmux by typing Ctrl + b then d Later, on the same login node, reattach by running tmux attach Make sure to: run tmux on the login node, NOT on compute nodes run srun inside tmux, not the reverse. Warning Every cluster has two login nodes. If you cannot find your tmux session, it might be running on the other node. Check the hostname of your current login node (from either your command prompt or from running hostname -s ), then use ssh to login to the other one. For example, if you are logged in to farnam1, use ssh -Y farnam2 to reach the other login node. Windows and Panes tmux allows you to create, toggle between and manipulate panes and windows in your session. A window is the whole screen that tmux displays to you. Panes are subdivisions in the curent window, where each runs an independent terminal. Especially at first, you probably won't need more than one pane at a time. Multiple windows can be created and run off-screen. Here is an example where this may be useful. Say you just submitted an interactive job that is running on a compute node inside your tmux session. [ be59@farnam2 ~ ] $ tmux new -s analysis # I am in my tmux session now [ be59@farnam2 ~ ] $ srun --pty -p interactive bash [ be59@c23n08 ~ ] $ ./my_fancy_analysis.sh Now you can easily monitor its CPU and memory utilization without ever taking your eyes off of it by creating a new pane and running top there. Split your window by typing: Ctrl + b then % ssh into the compute node you are working on, then run top to watch your work as it runs all from the same window. # I'm in a new pane now. [ be59@farnam2 ~ ] $ ssh c23n08 [ be59@c23n08 ~ ] $ top Your view will look something like this: To switch back and forth between panes, type Ctrl + b then o","title":"tmux"},{"location":"clusters-at-yale/guides/tmux/#tmux","text":"tmux is a \"terminal multiplexer\", it enables a number of terminals (or windows) to be accessed and controlled from a single terminal. tmux is a great way to save an interactive session between connections you make to the clusters. You can reconnect to the session from a workstation in your lab or from your laptop from home!","title":"tmux"},{"location":"clusters-at-yale/guides/tmux/#get-started","text":"To begin a tmux session named myproject, type tmux new -s myproject You should see a bar across the bottom of your terminal window now that gives you some information about your session. If you are disconnected or detached from this session, anything you were doing will still be there waiting when you reattach The most important shortcut to remember is Ctrl + b (hold the ctrl or control key, then type \"b\"). This is how you signal to tmux that the following keystroke is meant for it and not the session you are working in. For example: if you want to gracefully detach from your session, you can type Ctrl + b , then d for detach. To reattach to our sample tmux session after detatching, type: tmux attach -t myproject #If you are lazy and have only one session running, #This works too: tmux a Lines starting with a \"#\" denote a commented line, which aren't read as code Finally, to exit, you can type exit or Ctrl + d","title":"Get Started"},{"location":"clusters-at-yale/guides/tmux/#tmux-on-the-clusters","text":"Using tmux on the cluster allows you to create interactive allocations that you can detach from. Normally, if you get an interactive allocation (e.g. srun --pty) then disconnect from the cluster, for example by putting your laptop to sleep, your allocation will be terminated and your job killed. Using tmux, you can detach gracefully and tmux will maintain your allocation. Here is how to do this correctly: ssh to your cluster of choice Start tmux Inside your tmux session, submit an interactive job with srun. See the Slurm documentation for more details Inside your job allocation (on a compute node), start your application (e.g. matlab) Detach from tmux by typing Ctrl + b then d Later, on the same login node, reattach by running tmux attach Make sure to: run tmux on the login node, NOT on compute nodes run srun inside tmux, not the reverse. Warning Every cluster has two login nodes. If you cannot find your tmux session, it might be running on the other node. Check the hostname of your current login node (from either your command prompt or from running hostname -s ), then use ssh to login to the other one. For example, if you are logged in to farnam1, use ssh -Y farnam2 to reach the other login node.","title":"tmux on the Clusters"},{"location":"clusters-at-yale/guides/tmux/#windows-and-panes","text":"tmux allows you to create, toggle between and manipulate panes and windows in your session. A window is the whole screen that tmux displays to you. Panes are subdivisions in the curent window, where each runs an independent terminal. Especially at first, you probably won't need more than one pane at a time. Multiple windows can be created and run off-screen. Here is an example where this may be useful. Say you just submitted an interactive job that is running on a compute node inside your tmux session. [ be59@farnam2 ~ ] $ tmux new -s analysis # I am in my tmux session now [ be59@farnam2 ~ ] $ srun --pty -p interactive bash [ be59@c23n08 ~ ] $ ./my_fancy_analysis.sh Now you can easily monitor its CPU and memory utilization without ever taking your eyes off of it by creating a new pane and running top there. Split your window by typing: Ctrl + b then % ssh into the compute node you are working on, then run top to watch your work as it runs all from the same window. # I'm in a new pane now. [ be59@farnam2 ~ ] $ ssh c23n08 [ be59@c23n08 ~ ] $ top Your view will look something like this: To switch back and forth between panes, type Ctrl + b then o","title":"Windows and Panes"},{"location":"clusters-at-yale/guides/vasp/","text":"VASP NOTE: VASP requires a paid license. If you looking to use VASP, but your research group has not purchased a license, please do not use the cluster installations without first contacting hpc@yale.edu . Thank you for your cooperation. VASP and Slurm In Slurm, there is big difference between --ntasks and --cpus-per-task which is explained in our Requesting Resources documentation . For the purposes of VASP, --ntasks-per-node should always equal NCORE (in your INCAR file). Then --nodes should be equal to the total number of cores you want, divided by --ntasks-per-node . VASP has two parameters for controlling processor layouts, NCORE and NPAR , but you only need to set one of them. If you set NCORE , you don\u2019t need to set NPAR . Instead VASP will automatically set NPAR . In your mpirun line, you should specify the number of MPI tasks as: mpirun -n $SLURM_NTASKS vasp_std Cores Layout Examples If you want 40 cores (2 nodes and 20 cpus per node): in your submission script: #SBATCH --nodes=2 #SBATCH --ntasks-per-node=20 mpirun -n 2 vasp_std in INCAR : NCORE=20 You may however find that the wait time to get 20 cores on two nodes can be very long since cores request via --cpus-per-task can\u2019t span multiple nodes. Instead you might want to try breaking it up into smaller chunks. Therefore, try: in your submission script: #SBATCH --nodes=4 #SBATCH --ntasks-per-node=10 mpirun -n 4 vasp_std in INCAR : NCORE=10 which would likely spread over 4 nodes using 10 cores each and spend less time in the queue. Grace mpi partition On Grace's mpi parttion, since cores are assigned as whole 24-core nodes, NCORE should always be equal to 24 and then you can just request ntasks in multiples of 24. in your submission script: #SBATCH --ntasks=48 # some multiple of 24 mpirun -n $SLURM_NTASKS vasp_std in INCAR : NCORE=24 Additional Performance Some users have found that if they actually assign 2 MPI tasks per node (rather than 1), they see even better performance because the MPI tasks doesn't span the two sockets on the node. To try this, set NCORE to half of your nodes' core count and increase mpirun -n to twice the number of nodes you requested. Additional Reading Here is some documentation on how to optimally configure NCORE and NPAR: https://cms.mpi.univie.ac.at/wiki/index.php/NCORE https://cms.mpi.univie.ac.at/wiki/index.php/NPAR https://www.nsc.liu.se/~pla/blog/2015/01/12/vasp-how-many-cores/","title":"VASP"},{"location":"clusters-at-yale/guides/vasp/#vasp","text":"NOTE: VASP requires a paid license. If you looking to use VASP, but your research group has not purchased a license, please do not use the cluster installations without first contacting hpc@yale.edu . Thank you for your cooperation.","title":"VASP"},{"location":"clusters-at-yale/guides/vasp/#vasp-and-slurm","text":"In Slurm, there is big difference between --ntasks and --cpus-per-task which is explained in our Requesting Resources documentation . For the purposes of VASP, --ntasks-per-node should always equal NCORE (in your INCAR file). Then --nodes should be equal to the total number of cores you want, divided by --ntasks-per-node . VASP has two parameters for controlling processor layouts, NCORE and NPAR , but you only need to set one of them. If you set NCORE , you don\u2019t need to set NPAR . Instead VASP will automatically set NPAR . In your mpirun line, you should specify the number of MPI tasks as: mpirun -n $SLURM_NTASKS vasp_std","title":"VASP and Slurm"},{"location":"clusters-at-yale/guides/vasp/#cores-layout-examples","text":"If you want 40 cores (2 nodes and 20 cpus per node): in your submission script: #SBATCH --nodes=2 #SBATCH --ntasks-per-node=20 mpirun -n 2 vasp_std in INCAR : NCORE=20 You may however find that the wait time to get 20 cores on two nodes can be very long since cores request via --cpus-per-task can\u2019t span multiple nodes. Instead you might want to try breaking it up into smaller chunks. Therefore, try: in your submission script: #SBATCH --nodes=4 #SBATCH --ntasks-per-node=10 mpirun -n 4 vasp_std in INCAR : NCORE=10 which would likely spread over 4 nodes using 10 cores each and spend less time in the queue.","title":"Cores Layout Examples"},{"location":"clusters-at-yale/guides/vasp/#grace-mpi-partition","text":"On Grace's mpi parttion, since cores are assigned as whole 24-core nodes, NCORE should always be equal to 24 and then you can just request ntasks in multiples of 24. in your submission script: #SBATCH --ntasks=48 # some multiple of 24 mpirun -n $SLURM_NTASKS vasp_std in INCAR : NCORE=24","title":"Grace mpi partition"},{"location":"clusters-at-yale/guides/vasp/#additional-performance","text":"Some users have found that if they actually assign 2 MPI tasks per node (rather than 1), they see even better performance because the MPI tasks doesn't span the two sockets on the node. To try this, set NCORE to half of your nodes' core count and increase mpirun -n to twice the number of nodes you requested.","title":"Additional Performance"},{"location":"clusters-at-yale/guides/vasp/#additional-reading","text":"Here is some documentation on how to optimally configure NCORE and NPAR: https://cms.mpi.univie.ac.at/wiki/index.php/NCORE https://cms.mpi.univie.ac.at/wiki/index.php/NPAR https://www.nsc.liu.se/~pla/blog/2015/01/12/vasp-how-many-cores/","title":"Additional Reading"},{"location":"clusters-at-yale/job-scheduling/","text":"Run Jobs with Slurm Slurm is used to submit jobs to a specified set of compute resources, which are variously called queues or partitions. Slurm uses the term partition. Partitions, their defaults, limits and purposes are listed on each cluster page . To see more details about how jobs are scheduled see our information on factors affecting scheduling priority . Please be a good cluster citizen. Do not run jobs on login nodes (e.g. grace1, farnam2), as these can impact the sessions and connectivity of everyone else on the cluster. We also sometimes find jobs on the clusters that allocate resources incorrectly for the job that is running. Please see our documentation on Monitoring CPU and Memory Usage for examples of how to measure the resources your jobs use. If you find yourself wondering how best to schedule a job feel free to email us or come to office hours . Efficient jobs help you get your work done faster and free resources for others as well. Common Slurm Commands Submit a submission script (see below for details) sbatch <script> List queued and running jobs squeue -u $USER Cancel a queued job or kill a running job scancel <job_id> Check status of individual job (including failed or completed) sacct -j <job_id> Interactive Jobs Interactive jobs can be used for testing and troubleshooting code. By requesting an interactive job, you will be allocated resources and logged onto the node in a shell. srun --pty -p interactive bash This will assign a free node to you, allocating the requested number of CPUs, walltime, and memory, and put you within a shell on that node. You can run any number of commands within that shell. To free the allocated node, exit from the shell. Tip When using an interactive shell under slurm, your job is vulnerable to being killed if you lose your network connection. We recommend using tmux alleviate this. When using tmux , please be sure to keep track of your allocations and free those no longer needed! To use a GUI application (such as Matlab), when in an interactive job, use the --x11 flag: srun --pty --x11 -p interactive [ additional slurm options ] bash Warning For X11 forwarding to work, you need to have your local machine setup properly. Please see our X11 setup guide for more info. Batch Jobs To submit a job via Slurm, you first write a simple shell script called a \"submission script\" that wraps your job. A submission script is comprised of three parts: The program that should run the script. This is normally #!/bin/bash . The \"directives\" that tell the scheduler how to setup the computational resources for your job. These lines must appear before any other commands or definitions, otherwise they will be ignored. The actual \"script\" portion, which are the commands you want executed during your job. Here is an example script.sh that runs a job on one CPU on single node: #!/bin/bash #SBATCH --job-name=my_job #SBATCH --ntasks=1 --nodes=1 #SBATCH --mem-per-cpu=5G #SBATCH --time=12:00:00 #SBATCH --mail-type=ALL #SBATCH --mail-user=<email> ./myprog -p 20 arg1 arg2 arg3 ... Directives As shown in the above example, \"directives\" are comprised of #SBATCH followed by Slurm options. Most commonly used options include: Full Option Abbreviated Description --job-name -J Custom job name --partition -p Partition to run on --nodes -N Total number of nodes --ntasks -n Number of \"tasks\". For use with distributed parallelism. See below. --cpus-per-task -c # of CPUs allocated to each task. For use with shared memory parallelism. --ntasks-per-node Number of \"tasks\" per node. For use with distributed parallelism. See below. --time -t Maximum walltime of the job in the format D-HH:MM:SS (e.g. --time=1- for one day or --time=4:00:00 for 4 hours) --constraint -C specific node architecture (if applicable) --mem-per-cpu Memory requested per CPU (e.g. 10G for 10 GB) --mem Memory requested per node (e.g. 40G for 40 GB) --mail-user Mail address (alternatively, put your email address in ~/.forward) --mail-type Control emails to user on job events. Use ALL to receive email notications at the beginning and end of the job. Additional options can be found on in the official Slurm documentation . The lists of available partitions can be found on the cluster pages . If you don't specify a partition, your job will be placed into the default partition for your cluster (as indicated on the cluster page). Resource Limit Enforcement Slurm uses the linux cgroup feature to enforce limits on CPUs, GPUs, and memory. Jobs are only permitted to run on a node if they have a valid allocation, and only within the limits specified by that limitation. Thus, if you request a single core from slurm (the default) and start a job that runs 20 parallel threads, those threads will be packed into a single CPU, and run very slowly. Similarly, if you do not explicitly request memory, your job will be granted 5G of RAM per CPU, and if your job attempts to exceed that amount, it will be killed. Using Private Partitions If you have special permission to submit to a partition that belongs to another group, you may be asked to assign a special \"account\" to your jobs in that partition. You will be given the name of this account when you get access the partition and then simple add the -A <account> flag to your submission command or as an additional directive in your submission script.","title":"Run Jobs with Slurm"},{"location":"clusters-at-yale/job-scheduling/#run-jobs-with-slurm","text":"Slurm is used to submit jobs to a specified set of compute resources, which are variously called queues or partitions. Slurm uses the term partition. Partitions, their defaults, limits and purposes are listed on each cluster page . To see more details about how jobs are scheduled see our information on factors affecting scheduling priority . Please be a good cluster citizen. Do not run jobs on login nodes (e.g. grace1, farnam2), as these can impact the sessions and connectivity of everyone else on the cluster. We also sometimes find jobs on the clusters that allocate resources incorrectly for the job that is running. Please see our documentation on Monitoring CPU and Memory Usage for examples of how to measure the resources your jobs use. If you find yourself wondering how best to schedule a job feel free to email us or come to office hours . Efficient jobs help you get your work done faster and free resources for others as well.","title":"Run Jobs with Slurm"},{"location":"clusters-at-yale/job-scheduling/#common-slurm-commands","text":"Submit a submission script (see below for details) sbatch <script> List queued and running jobs squeue -u $USER Cancel a queued job or kill a running job scancel <job_id> Check status of individual job (including failed or completed) sacct -j <job_id>","title":"Common Slurm Commands"},{"location":"clusters-at-yale/job-scheduling/#interactive-jobs","text":"Interactive jobs can be used for testing and troubleshooting code. By requesting an interactive job, you will be allocated resources and logged onto the node in a shell. srun --pty -p interactive bash This will assign a free node to you, allocating the requested number of CPUs, walltime, and memory, and put you within a shell on that node. You can run any number of commands within that shell. To free the allocated node, exit from the shell. Tip When using an interactive shell under slurm, your job is vulnerable to being killed if you lose your network connection. We recommend using tmux alleviate this. When using tmux , please be sure to keep track of your allocations and free those no longer needed! To use a GUI application (such as Matlab), when in an interactive job, use the --x11 flag: srun --pty --x11 -p interactive [ additional slurm options ] bash Warning For X11 forwarding to work, you need to have your local machine setup properly. Please see our X11 setup guide for more info.","title":"Interactive Jobs"},{"location":"clusters-at-yale/job-scheduling/#batch-jobs","text":"To submit a job via Slurm, you first write a simple shell script called a \"submission script\" that wraps your job. A submission script is comprised of three parts: The program that should run the script. This is normally #!/bin/bash . The \"directives\" that tell the scheduler how to setup the computational resources for your job. These lines must appear before any other commands or definitions, otherwise they will be ignored. The actual \"script\" portion, which are the commands you want executed during your job. Here is an example script.sh that runs a job on one CPU on single node: #!/bin/bash #SBATCH --job-name=my_job #SBATCH --ntasks=1 --nodes=1 #SBATCH --mem-per-cpu=5G #SBATCH --time=12:00:00 #SBATCH --mail-type=ALL #SBATCH --mail-user=<email> ./myprog -p 20 arg1 arg2 arg3 ...","title":"Batch Jobs"},{"location":"clusters-at-yale/job-scheduling/#directives","text":"As shown in the above example, \"directives\" are comprised of #SBATCH followed by Slurm options. Most commonly used options include: Full Option Abbreviated Description --job-name -J Custom job name --partition -p Partition to run on --nodes -N Total number of nodes --ntasks -n Number of \"tasks\". For use with distributed parallelism. See below. --cpus-per-task -c # of CPUs allocated to each task. For use with shared memory parallelism. --ntasks-per-node Number of \"tasks\" per node. For use with distributed parallelism. See below. --time -t Maximum walltime of the job in the format D-HH:MM:SS (e.g. --time=1- for one day or --time=4:00:00 for 4 hours) --constraint -C specific node architecture (if applicable) --mem-per-cpu Memory requested per CPU (e.g. 10G for 10 GB) --mem Memory requested per node (e.g. 40G for 40 GB) --mail-user Mail address (alternatively, put your email address in ~/.forward) --mail-type Control emails to user on job events. Use ALL to receive email notications at the beginning and end of the job. Additional options can be found on in the official Slurm documentation . The lists of available partitions can be found on the cluster pages . If you don't specify a partition, your job will be placed into the default partition for your cluster (as indicated on the cluster page).","title":"Directives"},{"location":"clusters-at-yale/job-scheduling/#resource-limit-enforcement","text":"Slurm uses the linux cgroup feature to enforce limits on CPUs, GPUs, and memory. Jobs are only permitted to run on a node if they have a valid allocation, and only within the limits specified by that limitation. Thus, if you request a single core from slurm (the default) and start a job that runs 20 parallel threads, those threads will be packed into a single CPU, and run very slowly. Similarly, if you do not explicitly request memory, your job will be granted 5G of RAM per CPU, and if your job attempts to exceed that amount, it will be killed.","title":"Resource Limit Enforcement"},{"location":"clusters-at-yale/job-scheduling/#using-private-partitions","text":"If you have special permission to submit to a partition that belongs to another group, you may be asked to assign a special \"account\" to your jobs in that partition. You will be given the name of this account when you get access the partition and then simple add the -A <account> flag to your submission command or as an additional directive in your submission script.","title":"Using Private Partitions"},{"location":"clusters-at-yale/job-scheduling/common_job_failures/","text":"Common Job Failures Though there are many types of work that happens on the clusters, there are a common set of reasons that many jobs fail to run successfully. Here we will outline some of these common failure modes and detail steps to correcting them. Running out of memory Jobs can often fail due to an insufficient amount of memory being requested. Depending on the job, this failure might present in a SLURM error: slurmstepd : error : Detected 1 oom - kill event ( s ). Some of your processes may have been killed by the cgroup out - of - memory handler . This is stating that SLRUM detected the job hitting the maximum requested memory and then the job was killed. A program that runs out of memory can also crash generating a Bus error (core dumped) . This error means that the program is attempting to access memory that it does not have permission to reach. These errors can be fixed in two ways. First is to increase the amount of memory that is requested from SLURM. This can be done in an sbatch script: #SBATCH --mem-per-cpu=10G or on the command-line using srun : srun --pty -p interactive --mem-per-cpus=10G Alternatively, the code can be inspected to see if the program can reduce the amount of memory that is being used. Further details about monitoring memory usage can be found here . Disk Quota Exceeded Since the clusters are shared resources, we have quotas in place to prevent any group from using too much disk space. More details about quotas can be found here . When a group or user reaches the quota, files cannot be created. Additionally, existing files will not be able to be written to. This will frequently kill jobs that need to write output or log files. To inspect your quota, we have developed a tool called getquota : $ getquota This script shows information about your quotas on the current gpfs filesystem. If you plan to poll this sort of information extensively, please contact us for help at hpc@yale.edu ## Usage Details for hpcprog (as of Nov 20 2019 05:00) Fileset User Usage ( GB ) File Count ------------- ----- ---------- ------------- project ahs3 82 33 ,788 project cag94 0 1 project kln26 366 533 ,998 project njc2 0 1 project pl543 115 259 ,212 project tl397 370 529 ,026 ---- scratch60 ahs3 0 89 scratch60 cag94 0 1 scratch60 kln26 2510 714 ,703 scratch60 njc2 0 1 scratch60 pl543 0 6 scratch60 tl397 13056 282 ,212 ## Quota Summary for hpcprog (as of right now) Fileset Type Usage ( GB ) Quota ( GB ) File Count File Limit Backup Purged ------------- ------- ------------ ----------- ------------- ------------- --------- --------- home.grace USR 39 100 190 ,055 200 ,000 Yes No project GRP 707 6144 1 ,611,981 5 ,000,000 No No scratch60 GRP 4054 20480 987 ,336 5 ,000,000 No 60 days There are different quotas for $HOME , $PROJECT , and $SCRATCH60 . In addition to a disk usage quota (in GB), there is a File Count quota that limits the number of files that can be created. If your account is running into any of these quotas, jobs may not run successfully. As the $HOME space is backed up, there is a much smaller quota for each user. If your home-space is at max capacity, please investigate whether some files can be moved to $PROJECT . Loading modules from different toolchains Software on the cluster is managed using toolchains . These are collections of software tools that are all built using the same compilers to ensure that they work properly together. If modules from different toolchains are loaded at the same time, conflicts can arise that lead to jobs not running successfully. A sign that incompatible modules are being loaded is a print-out highlighting modules being 'reloaded': [ tl397@grace2 ~ ] $ module load STAR / 2.6.0 c - foss - 2018 a [ tl397@grace2 ~ ] $ module load TopHat / 2.1.1 - foss - 2016 b The following have been reloaded with a version change : 1 ) FFTW / 3.3.7 - gompi - 2018 a => FFTW / 3.3.4 - gompi - 2016 b 2 ) GCC / 6.4.0 - 2.28 => GCC / 5.4.0 - 2.26 3 ) GCCcore / 6.4.0 => GCCcore / 5.4.0 4 ) OpenBLAS / 0.2.20 - GCC - 6.4.0 - 2.28 => OpenBLAS / 0.2.18 - GCC - 5.4.0 - 2.26 - LAPACK - 3.6.1 5 ) OpenMPI / 2.1.2 - GCC - 6.4.0 - 2.28 => OpenMPI / 1.10.3 - GCC - 5.4.0 - 2.26 6 ) ScaLAPACK / 2.0.2 - gompi - 2018 a - OpenBLAS - 0.2.20 => ScaLAPACK / 2.0.2 - gompi - 2016 b - OpenBLAS - 0.2.18 - LAPACK - 3.6.1 7 ) binutils / 2.28 - GCCcore - 6.4.0 => binutils / 2.26 - GCCcore - 5.4.0 8 ) foss / 2018 a => foss / 2016 b 9 ) gompi / 2018 a => gompi / 2016 b 10 ) hwloc / 1.11.8 - GCCcore - 6.4.0 => hwloc / 1.11.3 - GCC - 5.4.0 - 2.26 11 ) numactl / 2.0.11 - GCCcore - 6.4.0 => numactl / 2.0.11 - GCC - 5.4.0 - 2.26 12 ) zlib / 1.2.11 - GCCcore - 6.4.0 => zlib / 1.2.11 - GCCcore - 5.4.0 Here we first loaded a tool from the foss-2018a toolchain, but then loaded a module from the incompatible foss-2016b toolchain. To ensure that your jobs run successfully, only use one toolchain at a time. If your work requires a version of software that is not installed, email research.computing@yale.edu and we will work to help. Conda Environment Conda environments provide a nice way to manage python and R packages and modules. This is achieved by setting environmental variables that point to the conda environment directory. However, when moving from the login node to a compute node through SLURM, these paths can get messed up. This can lead to python not locating packages that are installed within an environment. The which command can be used to identify which python executable is being located: $ which python /gpfs/loomis/apps/avx/software/miniconda/4.7.10/bin/python If this doesn't point to the conda environment, you may need to source the environment again. To make sure that the environmental variables are correctly set up, avoid activating the conda environment on the login node. Instead, wait until on a compute node to activate the environment or include the source activate my_env statement in the SBATCH submission script.","title":"Common Job Failures"},{"location":"clusters-at-yale/job-scheduling/common_job_failures/#common-job-failures","text":"Though there are many types of work that happens on the clusters, there are a common set of reasons that many jobs fail to run successfully. Here we will outline some of these common failure modes and detail steps to correcting them.","title":"Common Job Failures"},{"location":"clusters-at-yale/job-scheduling/common_job_failures/#running-out-of-memory","text":"Jobs can often fail due to an insufficient amount of memory being requested. Depending on the job, this failure might present in a SLURM error: slurmstepd : error : Detected 1 oom - kill event ( s ). Some of your processes may have been killed by the cgroup out - of - memory handler . This is stating that SLRUM detected the job hitting the maximum requested memory and then the job was killed. A program that runs out of memory can also crash generating a Bus error (core dumped) . This error means that the program is attempting to access memory that it does not have permission to reach. These errors can be fixed in two ways. First is to increase the amount of memory that is requested from SLURM. This can be done in an sbatch script: #SBATCH --mem-per-cpu=10G or on the command-line using srun : srun --pty -p interactive --mem-per-cpus=10G Alternatively, the code can be inspected to see if the program can reduce the amount of memory that is being used. Further details about monitoring memory usage can be found here .","title":"Running out of memory"},{"location":"clusters-at-yale/job-scheduling/common_job_failures/#disk-quota-exceeded","text":"Since the clusters are shared resources, we have quotas in place to prevent any group from using too much disk space. More details about quotas can be found here . When a group or user reaches the quota, files cannot be created. Additionally, existing files will not be able to be written to. This will frequently kill jobs that need to write output or log files. To inspect your quota, we have developed a tool called getquota : $ getquota This script shows information about your quotas on the current gpfs filesystem. If you plan to poll this sort of information extensively, please contact us for help at hpc@yale.edu ## Usage Details for hpcprog (as of Nov 20 2019 05:00) Fileset User Usage ( GB ) File Count ------------- ----- ---------- ------------- project ahs3 82 33 ,788 project cag94 0 1 project kln26 366 533 ,998 project njc2 0 1 project pl543 115 259 ,212 project tl397 370 529 ,026 ---- scratch60 ahs3 0 89 scratch60 cag94 0 1 scratch60 kln26 2510 714 ,703 scratch60 njc2 0 1 scratch60 pl543 0 6 scratch60 tl397 13056 282 ,212 ## Quota Summary for hpcprog (as of right now) Fileset Type Usage ( GB ) Quota ( GB ) File Count File Limit Backup Purged ------------- ------- ------------ ----------- ------------- ------------- --------- --------- home.grace USR 39 100 190 ,055 200 ,000 Yes No project GRP 707 6144 1 ,611,981 5 ,000,000 No No scratch60 GRP 4054 20480 987 ,336 5 ,000,000 No 60 days There are different quotas for $HOME , $PROJECT , and $SCRATCH60 . In addition to a disk usage quota (in GB), there is a File Count quota that limits the number of files that can be created. If your account is running into any of these quotas, jobs may not run successfully. As the $HOME space is backed up, there is a much smaller quota for each user. If your home-space is at max capacity, please investigate whether some files can be moved to $PROJECT .","title":"Disk Quota Exceeded"},{"location":"clusters-at-yale/job-scheduling/common_job_failures/#loading-modules-from-different-toolchains","text":"Software on the cluster is managed using toolchains . These are collections of software tools that are all built using the same compilers to ensure that they work properly together. If modules from different toolchains are loaded at the same time, conflicts can arise that lead to jobs not running successfully. A sign that incompatible modules are being loaded is a print-out highlighting modules being 'reloaded': [ tl397@grace2 ~ ] $ module load STAR / 2.6.0 c - foss - 2018 a [ tl397@grace2 ~ ] $ module load TopHat / 2.1.1 - foss - 2016 b The following have been reloaded with a version change : 1 ) FFTW / 3.3.7 - gompi - 2018 a => FFTW / 3.3.4 - gompi - 2016 b 2 ) GCC / 6.4.0 - 2.28 => GCC / 5.4.0 - 2.26 3 ) GCCcore / 6.4.0 => GCCcore / 5.4.0 4 ) OpenBLAS / 0.2.20 - GCC - 6.4.0 - 2.28 => OpenBLAS / 0.2.18 - GCC - 5.4.0 - 2.26 - LAPACK - 3.6.1 5 ) OpenMPI / 2.1.2 - GCC - 6.4.0 - 2.28 => OpenMPI / 1.10.3 - GCC - 5.4.0 - 2.26 6 ) ScaLAPACK / 2.0.2 - gompi - 2018 a - OpenBLAS - 0.2.20 => ScaLAPACK / 2.0.2 - gompi - 2016 b - OpenBLAS - 0.2.18 - LAPACK - 3.6.1 7 ) binutils / 2.28 - GCCcore - 6.4.0 => binutils / 2.26 - GCCcore - 5.4.0 8 ) foss / 2018 a => foss / 2016 b 9 ) gompi / 2018 a => gompi / 2016 b 10 ) hwloc / 1.11.8 - GCCcore - 6.4.0 => hwloc / 1.11.3 - GCC - 5.4.0 - 2.26 11 ) numactl / 2.0.11 - GCCcore - 6.4.0 => numactl / 2.0.11 - GCC - 5.4.0 - 2.26 12 ) zlib / 1.2.11 - GCCcore - 6.4.0 => zlib / 1.2.11 - GCCcore - 5.4.0 Here we first loaded a tool from the foss-2018a toolchain, but then loaded a module from the incompatible foss-2016b toolchain. To ensure that your jobs run successfully, only use one toolchain at a time. If your work requires a version of software that is not installed, email research.computing@yale.edu and we will work to help.","title":"Loading modules from different toolchains"},{"location":"clusters-at-yale/job-scheduling/common_job_failures/#conda-environment","text":"Conda environments provide a nice way to manage python and R packages and modules. This is achieved by setting environmental variables that point to the conda environment directory. However, when moving from the login node to a compute node through SLURM, these paths can get messed up. This can lead to python not locating packages that are installed within an environment. The which command can be used to identify which python executable is being located: $ which python /gpfs/loomis/apps/avx/software/miniconda/4.7.10/bin/python If this doesn't point to the conda environment, you may need to source the environment again. To make sure that the environmental variables are correctly set up, avoid activating the conda environment on the login node. Instead, wait until on a compute node to activate the environment or include the source activate my_env statement in the SBATCH submission script.","title":"Conda Environment"},{"location":"clusters-at-yale/job-scheduling/dsq/","text":"Submit Job Arrays with dSQ Dead Simple Queue is a light-weight tool to help submit large batches of homogenous jobs to a Slurm -based HPC cluster. It wraps around slurm's sbatch to help you submit independent jobs as job arrays . Job arrays have several advantages over submitting your jobs in a loop: Your job array will grow during the run to use available resources, up to a limit you can set. Even if the cluster is busy, you probably get work done because each job from your array can be run independently. Your job will only use the resources needed to complete remaining jobs. It will shrink as your jobs finish, giving you and your peers better access to compute resources. If you run your array on a pre-emptable partition (scavenge on YCRC clusters), only individual jobs are preempted. Your whole array will continue. dSQ adds a few nice features on top of job arrays: Your jobs don't need to know they're running in an array; your job file is a great way to document what was done in a way that you can move to other systems relatively easily. You get a simple report of which job ran where and for how long dSQAutopsy can create a new job file that has only the jobs that didn't complete from your last run. All you need is Python 2.7+, or Python 3. dSQ is not recommended for situations where the initialization of the job takes most of its execution time and it is re-usable. These situations are much better handled by a worker-based job handler. Step 1: Create Your Job File First, you'll need to generate a job file. Each line of this job file needs to specify exactly what you want run for each job, including any modules that need to be loaded or modifications to your environment variables. Empty lines or lines that begin with # will be ignored when submitting your job array. Note: slurm jobs start in the directory from which your job was submitted. For example, imagine that you have 1000 fastq files that correspond to individual samples you want to map to a genome with bowtie2 and convert to bam files with samtools . Given some initial testing, you think that each job needs 4 GB of RAM, and will run in less than 10 minutes. Create a file with the jobs you want to run, one per line. A simple loop that prints your jobs should usually suffice. A job can be a simple command invocation, or a sequence of commands. You can call the job file anything, but for this example assume it's called \"joblist.txt\" and contains: module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample1 --rg SM:sample1 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1.fastq - | samtools view -Shu - | samtools sort - sample1 module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample2 --rg SM:sample2 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample2.fastq - | samtools view -Shu - | samtools sort - sample2 ... module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample1000 --rg SM:sample1000 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1000.fastq - | samtools view -Shu - | samtools sort - sample1000 Step 2: Generate Batch Script with dsq On YCRC clusters you can load Dead Simple Queue onto your path with: module load dSQ You can also download or clone this repo and use the scripts directly. dsq takes a few arguments, then writes a job submission script (default) or can directly submit a job for you. The resources you request will be given to each job in the array (each line in your job file) , e.g. requesting 2 GB of RAM with dSQ will run each individual job with a separate 2 GB of RAM available. Run sbatch --help or see the official Slurm documentation for more info on sbatch options. dSQ will set a default job name of dsq-jobfile (your job file name without the file extension). dSQ will also set the job output file name pattern to dsq-jobfile-%A_%a-%N.out, which will capture each of your jobs' output to a file with the job's ID(%A), its array index or zero-based line number(%a), and the host name of the node it ran on (%N). If you are handling output in each of your jobs, set this to /dev/null , which will stop these files from being created. Required Arguments: --job-file jobs.txt Job file, one self-contained job per line. Optional Arguments: -h, --help Show this help message and exit. --version show program's version number and exit --batch-file sub_script.sh Name for batch script file. Defaults to dsq-jobfile-YYYY-MM-DD.sh -J jobname, --job-name jobname Name of your job array. Defaults to dsq-jobfile --max-jobs number Maximum number of simultaneously running jobs from the job array. -o fmt_string, --output fmt_string Slurm output file pattern. There will be one file per line in your job file. To suppress slurm out files, set this to /dev/null. Defaults to dsq-jobfile-%A_%a-%N.out --status-dir dir Directory to save the job_jobid_status.tsv file to. Defaults to working directory. --suppress-stats-file Don't save job stats to job_jobid_status.tsv --submit Submit the job array on the fly instead of creating a submission script. In the example above, we want walltime of 10 minutes and memory=4GB per job. Our invocation would be: dsq --job-file joblist.txt --mem-per-cpu 4g -t 10 :00 --mail-type ALL Which will create a file called dsq-joblist-yyyy-mm-dd.sh , where the y, m, and d are today's date. After creating the batch script, take a look at its contents. It should look quite familiar. #!/bin/bash #SBATCH --array 0-999 #SBATCH --output dsq-joblist-%A_%3a-%N.out #SBATCH --job-name dsq-joblist #SBATCH --mem-per-cpu 4g -t 10:00 --mail-type ALL # DO NOT EDIT LINE BELOW /path/to/dSQBatch.py --job-file /path/to/joblist.txt --status-dir /path/to/here Step 3: Submit Batch Script sbatch dsq-joblist-yyyy-mm-dd.sh Manage Your dSQ Job You can refer to any portion of your job with jobid_index syntax, or the entire array with its jobid. The index Dead Simple Queue uses starts at zero , so the 3rd line in your job file will have an index of 2. You can also specify ranges. # to cancel job 4 for array job 14567 scancel 14567_4 # to cancel jobs 10-20 for job 14567: scancel 14567_ [ 10 -20 ] dSQ Output You can monitor the status of your jobs in Slurm by using squeue -u <netid> , squeue -j <jobid> , or dsqa -j <jobid> . dSQ creates a file named job_jobid_status.tsv , unless you suppress this output with --supress-stats-file . This file will report the success or failure of each job as it finishes. Note this file will not contain information for any jobs that were canceled (e.g. by the user with scancel) before they began. This file contains details about the completed jobs in the following tab-separated columns: Job_ID: the zero-based line number from your job file. Exit_Code: exit code returned from your job (non-zero number generally indicates a failed job). Hostname: The hostname of the compute node that this job ran on. Time_Started: time started, formatted as year-month-day hour:minute:second. Time_Ended: time started, formatted as year-month-day hour:minute:second. Time_Elapsed: in seconds. Job: the line from your job file. dSQAutopsy You can use dSQAutopsy or dsqa to create a simple report of the array of jobs, and a new jobsfile that contains just the jobs you want to re-run if you specify the original jobsfile. Options listed below -j JOB_ID, --job-id JOB_ID The Job ID of a running or completed dSQ Array -f JOB_FILE, --job-file JOB_FILE Job file, one job per line (not your job submission script). -s STATES, --states STATES Comma separated list of states to use for re-writing job file. Default: CANCELLED,NODE_FAIL,PREEMPTED Asking for a simple report: dsqa -j 13233846 Produces one State Summary for Array 13233846 State Num_Jobs Indices ----- -------- ------- COMPLETED 12 4,7-17 RUNNING 5 1-3,5-6 PREEMPTED 1 0 You can redirect the report and the failed jobs to separate files: dsqa -j 2629186 -f jobsfile.txt > re-run_jobs.txt 2 > 2629186_report.txt","title":"Submit Job Arrays with dSQ"},{"location":"clusters-at-yale/job-scheduling/dsq/#submit-job-arrays-with-dsq","text":"Dead Simple Queue is a light-weight tool to help submit large batches of homogenous jobs to a Slurm -based HPC cluster. It wraps around slurm's sbatch to help you submit independent jobs as job arrays . Job arrays have several advantages over submitting your jobs in a loop: Your job array will grow during the run to use available resources, up to a limit you can set. Even if the cluster is busy, you probably get work done because each job from your array can be run independently. Your job will only use the resources needed to complete remaining jobs. It will shrink as your jobs finish, giving you and your peers better access to compute resources. If you run your array on a pre-emptable partition (scavenge on YCRC clusters), only individual jobs are preempted. Your whole array will continue. dSQ adds a few nice features on top of job arrays: Your jobs don't need to know they're running in an array; your job file is a great way to document what was done in a way that you can move to other systems relatively easily. You get a simple report of which job ran where and for how long dSQAutopsy can create a new job file that has only the jobs that didn't complete from your last run. All you need is Python 2.7+, or Python 3. dSQ is not recommended for situations where the initialization of the job takes most of its execution time and it is re-usable. These situations are much better handled by a worker-based job handler.","title":"Submit Job Arrays with dSQ"},{"location":"clusters-at-yale/job-scheduling/dsq/#step-1-create-your-job-file","text":"First, you'll need to generate a job file. Each line of this job file needs to specify exactly what you want run for each job, including any modules that need to be loaded or modifications to your environment variables. Empty lines or lines that begin with # will be ignored when submitting your job array. Note: slurm jobs start in the directory from which your job was submitted. For example, imagine that you have 1000 fastq files that correspond to individual samples you want to map to a genome with bowtie2 and convert to bam files with samtools . Given some initial testing, you think that each job needs 4 GB of RAM, and will run in less than 10 minutes. Create a file with the jobs you want to run, one per line. A simple loop that prints your jobs should usually suffice. A job can be a simple command invocation, or a sequence of commands. You can call the job file anything, but for this example assume it's called \"joblist.txt\" and contains: module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample1 --rg SM:sample1 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1.fastq - | samtools view -Shu - | samtools sort - sample1 module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample2 --rg SM:sample2 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample2.fastq - | samtools view -Shu - | samtools sort - sample2 ... module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample1000 --rg SM:sample1000 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1000.fastq - | samtools view -Shu - | samtools sort - sample1000","title":"Step 1: Create Your Job File"},{"location":"clusters-at-yale/job-scheduling/dsq/#step-2-generate-batch-script-with-dsq","text":"On YCRC clusters you can load Dead Simple Queue onto your path with: module load dSQ You can also download or clone this repo and use the scripts directly. dsq takes a few arguments, then writes a job submission script (default) or can directly submit a job for you. The resources you request will be given to each job in the array (each line in your job file) , e.g. requesting 2 GB of RAM with dSQ will run each individual job with a separate 2 GB of RAM available. Run sbatch --help or see the official Slurm documentation for more info on sbatch options. dSQ will set a default job name of dsq-jobfile (your job file name without the file extension). dSQ will also set the job output file name pattern to dsq-jobfile-%A_%a-%N.out, which will capture each of your jobs' output to a file with the job's ID(%A), its array index or zero-based line number(%a), and the host name of the node it ran on (%N). If you are handling output in each of your jobs, set this to /dev/null , which will stop these files from being created. Required Arguments: --job-file jobs.txt Job file, one self-contained job per line. Optional Arguments: -h, --help Show this help message and exit. --version show program's version number and exit --batch-file sub_script.sh Name for batch script file. Defaults to dsq-jobfile-YYYY-MM-DD.sh -J jobname, --job-name jobname Name of your job array. Defaults to dsq-jobfile --max-jobs number Maximum number of simultaneously running jobs from the job array. -o fmt_string, --output fmt_string Slurm output file pattern. There will be one file per line in your job file. To suppress slurm out files, set this to /dev/null. Defaults to dsq-jobfile-%A_%a-%N.out --status-dir dir Directory to save the job_jobid_status.tsv file to. Defaults to working directory. --suppress-stats-file Don't save job stats to job_jobid_status.tsv --submit Submit the job array on the fly instead of creating a submission script. In the example above, we want walltime of 10 minutes and memory=4GB per job. Our invocation would be: dsq --job-file joblist.txt --mem-per-cpu 4g -t 10 :00 --mail-type ALL Which will create a file called dsq-joblist-yyyy-mm-dd.sh , where the y, m, and d are today's date. After creating the batch script, take a look at its contents. It should look quite familiar. #!/bin/bash #SBATCH --array 0-999 #SBATCH --output dsq-joblist-%A_%3a-%N.out #SBATCH --job-name dsq-joblist #SBATCH --mem-per-cpu 4g -t 10:00 --mail-type ALL # DO NOT EDIT LINE BELOW /path/to/dSQBatch.py --job-file /path/to/joblist.txt --status-dir /path/to/here","title":"Step 2: Generate Batch Script with dsq"},{"location":"clusters-at-yale/job-scheduling/dsq/#step-3-submit-batch-script","text":"sbatch dsq-joblist-yyyy-mm-dd.sh","title":"Step 3: Submit Batch Script"},{"location":"clusters-at-yale/job-scheduling/dsq/#manage-your-dsq-job","text":"You can refer to any portion of your job with jobid_index syntax, or the entire array with its jobid. The index Dead Simple Queue uses starts at zero , so the 3rd line in your job file will have an index of 2. You can also specify ranges. # to cancel job 4 for array job 14567 scancel 14567_4 # to cancel jobs 10-20 for job 14567: scancel 14567_ [ 10 -20 ]","title":"Manage Your dSQ Job"},{"location":"clusters-at-yale/job-scheduling/dsq/#dsq-output","text":"You can monitor the status of your jobs in Slurm by using squeue -u <netid> , squeue -j <jobid> , or dsqa -j <jobid> . dSQ creates a file named job_jobid_status.tsv , unless you suppress this output with --supress-stats-file . This file will report the success or failure of each job as it finishes. Note this file will not contain information for any jobs that were canceled (e.g. by the user with scancel) before they began. This file contains details about the completed jobs in the following tab-separated columns: Job_ID: the zero-based line number from your job file. Exit_Code: exit code returned from your job (non-zero number generally indicates a failed job). Hostname: The hostname of the compute node that this job ran on. Time_Started: time started, formatted as year-month-day hour:minute:second. Time_Ended: time started, formatted as year-month-day hour:minute:second. Time_Elapsed: in seconds. Job: the line from your job file.","title":"dSQ Output"},{"location":"clusters-at-yale/job-scheduling/dsq/#dsqautopsy","text":"You can use dSQAutopsy or dsqa to create a simple report of the array of jobs, and a new jobsfile that contains just the jobs you want to re-run if you specify the original jobsfile. Options listed below -j JOB_ID, --job-id JOB_ID The Job ID of a running or completed dSQ Array -f JOB_FILE, --job-file JOB_FILE Job file, one job per line (not your job submission script). -s STATES, --states STATES Comma separated list of states to use for re-writing job file. Default: CANCELLED,NODE_FAIL,PREEMPTED Asking for a simple report: dsqa -j 13233846 Produces one State Summary for Array 13233846 State Num_Jobs Indices ----- -------- ------- COMPLETED 12 4,7-17 RUNNING 5 1-3,5-6 PREEMPTED 1 0 You can redirect the report and the failed jobs to separate files: dsqa -j 2629186 -f jobsfile.txt > re-run_jobs.txt 2 > 2629186_report.txt","title":"dSQAutopsy"},{"location":"clusters-at-yale/job-scheduling/fairshare/","text":"Factors Affecting Scheduling Job Priority Score Fairshare To ensure well-balanced access to cluster resources, we institute a fairshare system on our clusters. In practice this means jobs have a priority score that dictates when it can be run in relation to other jobs. This score is affected by the amount of CPU-equivalent hours used by a group in the past few weeks. The number of CPU-equivalents allocated to a job is defined as the larger of (a) the number of requested cores and (b) the total amount of requested memory divided by the default memory per core (usually 5G/core). If a group has used a large amount of CPU-equivalent hours, their jobs are given a lower priority score and therefore will take longer to start if the cluster is busy. Regardless of a job's prority, the scheduler still considers all jobs for backfill (see below). To see all pending jobs sorted by priority (jobs with higher priority at the top), use the following squeue command: squeue --sort=-p -t PD -p <partition_name> To monitor usage of members of your group, run the sshare command: sshare -a -A <group> Note: Resources used on private partitions do not count affect fairshare. Similarly, resources used in the scavenge partition cost 10% of comparable resources in the other partitions. Length of Time in Queue In addition to fairshare, any pending job will accrue priority over time, which can help overcome small fairshare penalties. To see the factors affecting your job's priority, run the following sprio command: sprio -j <job_id> Backfill In addition to the main scheduling cycle, where jobs are run in the order of priority and availability of resources, all jobs are also considered for \"backfill\". Backfill is a mechanism which will let jobs with lower priority score start before high priority jobs if they can fit in around them. For example, if a higher priority job needs 4 nodes with 20 cores on each node and it will have to wait 30 hours for those resources to be available, if a lower priority job only needs a couple cores for an hour, Slurm will run that job in the meantime. For this reason, it is important to request accurate walltime limits for your jobs. If your job only requires 2 hours to run, but you request 24 hours, the likelihood that your job will be backfilled is greatly lowered. Moreover, for performance reasons, the backfill scheduler on Grace only looks at the top 10 jobs by each user. Therefore, if you bundle similar jobs into job arrays (see dSQ ), the backfill cycle will consider more of your jobs since entire job arrays only count as one job for the limit accounting.","title":"Factors Affecting Scheduling"},{"location":"clusters-at-yale/job-scheduling/fairshare/#factors-affecting-scheduling","text":"","title":"Factors Affecting Scheduling"},{"location":"clusters-at-yale/job-scheduling/fairshare/#job-priority-score","text":"","title":"Job Priority Score"},{"location":"clusters-at-yale/job-scheduling/fairshare/#fairshare","text":"To ensure well-balanced access to cluster resources, we institute a fairshare system on our clusters. In practice this means jobs have a priority score that dictates when it can be run in relation to other jobs. This score is affected by the amount of CPU-equivalent hours used by a group in the past few weeks. The number of CPU-equivalents allocated to a job is defined as the larger of (a) the number of requested cores and (b) the total amount of requested memory divided by the default memory per core (usually 5G/core). If a group has used a large amount of CPU-equivalent hours, their jobs are given a lower priority score and therefore will take longer to start if the cluster is busy. Regardless of a job's prority, the scheduler still considers all jobs for backfill (see below). To see all pending jobs sorted by priority (jobs with higher priority at the top), use the following squeue command: squeue --sort=-p -t PD -p <partition_name> To monitor usage of members of your group, run the sshare command: sshare -a -A <group> Note: Resources used on private partitions do not count affect fairshare. Similarly, resources used in the scavenge partition cost 10% of comparable resources in the other partitions.","title":"Fairshare"},{"location":"clusters-at-yale/job-scheduling/fairshare/#length-of-time-in-queue","text":"In addition to fairshare, any pending job will accrue priority over time, which can help overcome small fairshare penalties. To see the factors affecting your job's priority, run the following sprio command: sprio -j <job_id>","title":"Length of Time in Queue"},{"location":"clusters-at-yale/job-scheduling/fairshare/#backfill","text":"In addition to the main scheduling cycle, where jobs are run in the order of priority and availability of resources, all jobs are also considered for \"backfill\". Backfill is a mechanism which will let jobs with lower priority score start before high priority jobs if they can fit in around them. For example, if a higher priority job needs 4 nodes with 20 cores on each node and it will have to wait 30 hours for those resources to be available, if a lower priority job only needs a couple cores for an hour, Slurm will run that job in the meantime. For this reason, it is important to request accurate walltime limits for your jobs. If your job only requires 2 hours to run, but you request 24 hours, the likelihood that your job will be backfilled is greatly lowered. Moreover, for performance reasons, the backfill scheduler on Grace only looks at the top 10 jobs by each user. Therefore, if you bundle similar jobs into job arrays (see dSQ ), the backfill cycle will consider more of your jobs since entire job arrays only count as one job for the limit accounting.","title":"Backfill"},{"location":"clusters-at-yale/job-scheduling/mpi/","text":"MPI Partition Grace has a special common partition called mpi . The mpi partition is a bit different from other partitions on Grace--it always allocates entire nodes to jobs submitted to the partition. Each node in the mpi partition are identical 24 core, 2x Skylake Gold 6136, 96GB RAM (90GB usable) nodes. While this partition is available to all Grace users, only certain types of jobs are allowed on the partition (similar to the restrictions on our GPU partitions). Appropriate Jobs This partition is specifically designed to support jobs that use tightly-coupled MPI-enabled codes that will run across multiple nodes and are sensitive to sharing their nodes with other jobs. Since every node on the mpi partition is identical, it can support workloads that are sensitive to hardware difference across a single job. We expect most of jobs submitted to mpi to use all 24 cores on each node. There are occasionally instances were a tightly coupled code will use multiple nodes but less than all 24 cores due to load balancing or memory limitations. For example, some codes require power of 2 cores in the job, but 24 cores doesn't always divide evenly into those configurations. So we occasionally see jobs that use multiple nodes but only 16 of the 24 cores per node and are also acceptable submissions to the mpi partition. Jobs that do not require exclusive nodes, even if they use mpirun to launch, will run fine and experience normal wait times in the day and week (and scavenge) partitions. As such, we ask you to protect the special mpi partition nodes for the more resource sensitive jobs listed above and, therefore, submit any jobs that will not be using whole node(s) to the other partitions.\u200b If smaller or single core jobs are submitted to the mpi partition, they may be cancelled without warning. As with our GPU partitions, if you would like to make use of available cores on any mpi nodes for small jobs, the scavenge partition is the correct way to do that. If you have any questions about whether your workload is appropariate for the mpi partition, please contact us at hpc@yale.edu . Core Layouts Please review the Request Compute Resources documentation for the appropriate Slurm flags for different types of core and node layouts. If you have any questions, feel free to email us at hpc@yale.edu .","title":"MPI Partition"},{"location":"clusters-at-yale/job-scheduling/mpi/#mpi-partition","text":"Grace has a special common partition called mpi . The mpi partition is a bit different from other partitions on Grace--it always allocates entire nodes to jobs submitted to the partition. Each node in the mpi partition are identical 24 core, 2x Skylake Gold 6136, 96GB RAM (90GB usable) nodes. While this partition is available to all Grace users, only certain types of jobs are allowed on the partition (similar to the restrictions on our GPU partitions).","title":"MPI Partition"},{"location":"clusters-at-yale/job-scheduling/mpi/#appropriate-jobs","text":"This partition is specifically designed to support jobs that use tightly-coupled MPI-enabled codes that will run across multiple nodes and are sensitive to sharing their nodes with other jobs. Since every node on the mpi partition is identical, it can support workloads that are sensitive to hardware difference across a single job. We expect most of jobs submitted to mpi to use all 24 cores on each node. There are occasionally instances were a tightly coupled code will use multiple nodes but less than all 24 cores due to load balancing or memory limitations. For example, some codes require power of 2 cores in the job, but 24 cores doesn't always divide evenly into those configurations. So we occasionally see jobs that use multiple nodes but only 16 of the 24 cores per node and are also acceptable submissions to the mpi partition. Jobs that do not require exclusive nodes, even if they use mpirun to launch, will run fine and experience normal wait times in the day and week (and scavenge) partitions. As such, we ask you to protect the special mpi partition nodes for the more resource sensitive jobs listed above and, therefore, submit any jobs that will not be using whole node(s) to the other partitions.\u200b If smaller or single core jobs are submitted to the mpi partition, they may be cancelled without warning. As with our GPU partitions, if you would like to make use of available cores on any mpi nodes for small jobs, the scavenge partition is the correct way to do that. If you have any questions about whether your workload is appropariate for the mpi partition, please contact us at hpc@yale.edu .","title":"Appropriate Jobs"},{"location":"clusters-at-yale/job-scheduling/mpi/#core-layouts","text":"Please review the Request Compute Resources documentation for the appropriate Slurm flags for different types of core and node layouts. If you have any questions, feel free to email us at hpc@yale.edu .","title":"Core Layouts"},{"location":"clusters-at-yale/job-scheduling/resource-requests/","text":"Request Compute Resources Request Cores and Nodes Slurm is very explicit in how one requests cores and nodes. While extremely powerful, the three flags, --nodes , --ntasks , and --cpus-per-task can be a bit confusing at first. We attempt to disambiguate them below. --ntasks vs --cpus-per-task The term \"task\" in this context can be thought of as a \"process\". Therefore, a multi-process program (e.g. MPI) is comprised of multiple tasks. And a multi-threaded program is comprised of a single task, which can in turn use multiple CPUs. In Slurm, tasks are requested with the --ntasks flag. CPUs, for the multithreaded programs, are requested with the --cpus-per-task flag. 1. Multi-threaded & multi-process programs To request CPUs for your multi-threaded program, use the --cpus-per-task flag. Individual tasks cannot be split across multiple compute nodes, so requesting a number of CPUs with --cpus-per-task flag will always result in all your CPUs allocated on the same compute node. 2. MPI programs In Slurm, the --ntasks flag specifies the number of MPI tasks created for your job. Note that, even within the same job, multiple tasks do not necessarily run on a single node. Therefore, requesting the same number of CPUs as above, but with the --ntasks flag, could result in those CPUs being allocated on several, distinct compute nodes. For many users, differentiating between --ntasks and --cpus-per-task is sufficient. However, for more control over how Slurm lays out your job, you can add the --nodes and --ntasks-per-node flags. --nodes specifies how many nodes to allocate to your job. Slurm will allocate your requested number of cores to a minimal number of nodes on the cluster, so it is extremely likely if you request a small number of tasks that they will all be allocated on the same node. However, to ensure they are on the same node, set --nodes=1 (obviously this is contingent on the number of CPUs on your cluster's nodes and requesting too many may result in a job that will never run). Conversely, if you would like to ensure a specific layout, such as one task per node for memory, I/O or other reasons, you can also set --ntasks-per-node=1 . Note that the following must be true: ntasks-per-node * nodes >= ntasks 3. Hybrid (MPI+OpenMP) programs For the most predictable performance for hybrid codes, you will need to use all three of the --ntasks , --cpus-per-task , and --nodes flags, where --ntasks equals the number of MPI tasks, --cpus-per-task equals the number of OMP_NUM_THREADS and --nodes is the number of nodes required to fit --ntasks * --cpus-per-task . Request GPUs Some of our clusters have nodes that contain GPU co-processors. Please refer to the cluster-specific documentation regarding the node configurations that include GPUs. In order for your job to be able to access gpus, you must request them as a Slurm \"Generic Resource\" or gres. You specify the gres configuration per-node for a job with the --gres flag and a number of GPUs. If you are agnostic about the kind of GPU your job gets, --gres=gpu:1 will allocate one of any kind of GPU per node. To specifically request, for example, a P100 for each node in your job you would use the flag --gres=gpu:p100:1 . Some codes require double-precision capable GPUs--if so, see the next section for using \"features\" to request any node with compatible GPUs. Tip As with requesting multiple cores or multiple nodes, we strongly recommend that you test your jobs using the gpu_devel partition to make sure they can well utilize multiple GPUs before requesting them; allocating more GPUs does not magically speed up code that can only use one at a time. For more documentation on using GPUs on our clusters, please see Python Deep Learning with GPUs and GPUs and CUDA . Features and Constraints You may want to run programs that require specific hardware. To ensure your job runs on specific types of nodes, use the --constraint flag. You can use the processor codename (e.g. haswell ) or processor type (e.g. E5-2660_v3 ) to limit your job to specific node types. You can also specify an instruction set (e.g. avx2 ) to require that no matter what CPU your job runs on, it must understand at least these instructions. See the individual cluster pages for the exact tags for the different node types. # run on a node with a haswell codenamed CPU (e.g. a E5-2660 v3) sbatch --constraint = haswell submit.sh # only run on nodes with E5-2660 v4 CPUs sbatch --constraint = E5-2660_v4 submit.sh # run on any node that understands avx2 instructions # Your job may also launch on an avx512 node sbatch --constraint = avx2 submit.sh We also have keyword features to help you constrain your jobs to certain categories of nodes. oldest : the oldest generation of node on the cluster. Use this constraint when compiling code if you wish to ensure it can run on any standard node on the cluster. nogpu : nodes without GPUs. standard : nodes without GPUs or extra memory. Useful for protecting special nodes in a private partition for jobs that can use the extra capabilities. singleprecision : nodes with single-precision only capable GPUs (e.g. GTX 1080s, RTX 2080s). doubleprecision : nodes with double-precision capable GPUs (e.g. K80s, P100s and V100s). Tip Use the command scontrol show node <hostname> , replacing <hostname> with the node's name you're interested in, to see more information about the node including its features.","title":"Request Compute Resources"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#request-compute-resources","text":"","title":"Request Compute Resources"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#request-cores-and-nodes","text":"Slurm is very explicit in how one requests cores and nodes. While extremely powerful, the three flags, --nodes , --ntasks , and --cpus-per-task can be a bit confusing at first. We attempt to disambiguate them below.","title":"Request Cores and Nodes"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#-ntasks-vs-cpus-per-task","text":"The term \"task\" in this context can be thought of as a \"process\". Therefore, a multi-process program (e.g. MPI) is comprised of multiple tasks. And a multi-threaded program is comprised of a single task, which can in turn use multiple CPUs. In Slurm, tasks are requested with the --ntasks flag. CPUs, for the multithreaded programs, are requested with the --cpus-per-task flag.","title":"--ntasks vs --cpus-per-task"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#1-multi-threaded-multi-process-programs","text":"To request CPUs for your multi-threaded program, use the --cpus-per-task flag. Individual tasks cannot be split across multiple compute nodes, so requesting a number of CPUs with --cpus-per-task flag will always result in all your CPUs allocated on the same compute node.","title":"1. Multi-threaded &amp; multi-process programs"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#2-mpi-programs","text":"In Slurm, the --ntasks flag specifies the number of MPI tasks created for your job. Note that, even within the same job, multiple tasks do not necessarily run on a single node. Therefore, requesting the same number of CPUs as above, but with the --ntasks flag, could result in those CPUs being allocated on several, distinct compute nodes. For many users, differentiating between --ntasks and --cpus-per-task is sufficient. However, for more control over how Slurm lays out your job, you can add the --nodes and --ntasks-per-node flags. --nodes specifies how many nodes to allocate to your job. Slurm will allocate your requested number of cores to a minimal number of nodes on the cluster, so it is extremely likely if you request a small number of tasks that they will all be allocated on the same node. However, to ensure they are on the same node, set --nodes=1 (obviously this is contingent on the number of CPUs on your cluster's nodes and requesting too many may result in a job that will never run). Conversely, if you would like to ensure a specific layout, such as one task per node for memory, I/O or other reasons, you can also set --ntasks-per-node=1 . Note that the following must be true: ntasks-per-node * nodes >= ntasks","title":"2. MPI programs"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#3-hybrid-mpiopenmp-programs","text":"For the most predictable performance for hybrid codes, you will need to use all three of the --ntasks , --cpus-per-task , and --nodes flags, where --ntasks equals the number of MPI tasks, --cpus-per-task equals the number of OMP_NUM_THREADS and --nodes is the number of nodes required to fit --ntasks * --cpus-per-task .","title":"3. Hybrid (MPI+OpenMP) programs"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#request-gpus","text":"Some of our clusters have nodes that contain GPU co-processors. Please refer to the cluster-specific documentation regarding the node configurations that include GPUs. In order for your job to be able to access gpus, you must request them as a Slurm \"Generic Resource\" or gres. You specify the gres configuration per-node for a job with the --gres flag and a number of GPUs. If you are agnostic about the kind of GPU your job gets, --gres=gpu:1 will allocate one of any kind of GPU per node. To specifically request, for example, a P100 for each node in your job you would use the flag --gres=gpu:p100:1 . Some codes require double-precision capable GPUs--if so, see the next section for using \"features\" to request any node with compatible GPUs. Tip As with requesting multiple cores or multiple nodes, we strongly recommend that you test your jobs using the gpu_devel partition to make sure they can well utilize multiple GPUs before requesting them; allocating more GPUs does not magically speed up code that can only use one at a time. For more documentation on using GPUs on our clusters, please see Python Deep Learning with GPUs and GPUs and CUDA .","title":"Request GPUs"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#features-and-constraints","text":"You may want to run programs that require specific hardware. To ensure your job runs on specific types of nodes, use the --constraint flag. You can use the processor codename (e.g. haswell ) or processor type (e.g. E5-2660_v3 ) to limit your job to specific node types. You can also specify an instruction set (e.g. avx2 ) to require that no matter what CPU your job runs on, it must understand at least these instructions. See the individual cluster pages for the exact tags for the different node types. # run on a node with a haswell codenamed CPU (e.g. a E5-2660 v3) sbatch --constraint = haswell submit.sh # only run on nodes with E5-2660 v4 CPUs sbatch --constraint = E5-2660_v4 submit.sh # run on any node that understands avx2 instructions # Your job may also launch on an avx512 node sbatch --constraint = avx2 submit.sh We also have keyword features to help you constrain your jobs to certain categories of nodes. oldest : the oldest generation of node on the cluster. Use this constraint when compiling code if you wish to ensure it can run on any standard node on the cluster. nogpu : nodes without GPUs. standard : nodes without GPUs or extra memory. Useful for protecting special nodes in a private partition for jobs that can use the extra capabilities. singleprecision : nodes with single-precision only capable GPUs (e.g. GTX 1080s, RTX 2080s). doubleprecision : nodes with double-precision capable GPUs (e.g. K80s, P100s and V100s). Tip Use the command scontrol show node <hostname> , replacing <hostname> with the node's name you're interested in, to see more information about the node including its features.","title":"Features and Constraints"},{"location":"clusters-at-yale/job-scheduling/resource-usage/","text":"Monitor CPU and Memory General Note Making sure your jobs use the right amount of RAM and the right number of CPUs helps you and others using the clusters use these resources more effeciently, and in turn get work done more quickly. Below are some examples of how to measure your CPU and RAM (aka memory) usage so you can make this happen. Be sure to check the Slurm documentation and the clusters page (especially the partitions and hardware sections) to make sure you are submitting the right jobs to the right hardware. Future Jobs If you launch a program by putting /usr/bin/time in front of it, time will watch your program and provide statistics about the resources it used. For example: [ be59@c01n01 ~ ] $ /usr/bin/time -v stress-ng --cpu 8 --timeout 10s stress-ng: info: [ 32574 ] dispatching hogs: 8 cpu stress-ng: info: [ 32574 ] successful run completed in 10 .08s Command being timed: \"stress-ng --cpu 8 --timeout 10s\" User time ( seconds ) : 80 .22 System time ( seconds ) : 0 .04 Percent of CPU this job got: 795 % Elapsed ( wall clock ) time ( h:mm:ss or m:ss ) : 0 :10.09 Average shared text size ( kbytes ) : 0 Average unshared data size ( kbytes ) : 0 Average stack size ( kbytes ) : 0 Average total size ( kbytes ) : 0 Maximum resident set size ( kbytes ) : 6328 Average resident set size ( kbytes ) : 0 Major ( requiring I/O ) page faults: 0 Minor ( reclaiming a frame ) page faults: 30799 Voluntary context switches: 1380 Involuntary context switches: 68 Swaps: 0 File system inputs: 0 File system outputs: 0 Socket messages sent: 0 Socket messages received: 0 Signals delivered: 0 To know how much RAM your job used (and what jobs like it will need in the future), look at the \"Maximum resident set size\" Running Jobs If your job is already running, you can check on its usage, but will have to wait until it has finished to find the maximum memory and CPU used. The easiest way to check the instantaneous memory and CPU usage of a job is to ssh to a compute node your job is running on. To find the node you should ssh to, run: [be59@farnam1 ~]$ squeue -u$USER JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 21252409 general 12345 be59 R 32:17 17 c13n[02-04],c14n[05-10],c16n[03-10] Then use ssh to connect to a node your job is running on from the NODELIST column: [be59@farnam1 ~]$ ssh c13n03 [be59@c13n03 ~]$ Once you are on the compute node, run either ps or top . ps ps will give you instantaneous usage every time you run it. Here is some sample ps output: [ be59 @ bigmem01 ~ ] $ ps - u $ USER - o %cpu,rss,args %CPU RSS COMMAND 92.6 79446140 / gpfs / ysm / apps / hpc / Apps / Matlab / R2016b / bin / glnxa64 / MATLAB - dmlworker - nodisplay - r distcomp_evaluate_filetask 94.5 80758040 / gpfs / ysm / apps / hpc / Apps / Matlab / R2016b / bin / glnxa64 / MATLAB - dmlworker - nodisplay - r distcomp_evaluate_filetask 92.6 79676460 / gpfs / ysm / apps / hpc / Apps / Matlab / R2016b / bin / glnxa64 / MATLAB - dmlworker - nodisplay - r distcomp_evaluate_filetask 92.5 81243364 / gpfs / ysm / apps / hpc / Apps / Matlab / R2016b / bin / glnxa64 / MATLAB - dmlworker - nodisplay - r distcomp_evaluate_filetask 93.8 80799668 / gpfs / ysm / apps / hpc / Apps / Matlab / R2016b / bin / glnxa64 / MATLAB - dmlworker - nodisplay - r distcomp_evaluate_filetask ps reports memory used in kilobytes, so each of the 5 matlab processes is using ~77GB of RAM. They are also using most of 5 cores, so future jobs like this should request 5 CPUs. top top runs interactively and shows you live usage statistics. You can press u , enter your netid, then enter to filter just your processes. For Memory usage, the number you are interested in is RES. In the case below, the YEPNEE.exe programs are each consuming ~600MB of memory and each fully utilizing one CPU. You can press ? for help and q to quit. ClusterShell For multi-node jobs clush can be very useful. Please see our guide on how to set up and use ClusterShell . Completed Jobs Slurm records statistics for every job, including how much memory and CPU was used. seff After the job completes, you can run seff <jobid> to get some useful information about your job, including the memory used and what percent of your allocated memory that amounts to. [rdb9@farnam1 ~]$ seff 21294645 Job ID: 21294645 Cluster: farnam User/Group: rdb9/lsprog State: COMPLETED (exit code 0) Cores: 1 CPU Utilized: 00:15:55 CPU Efficiency: 17.04% of 01:33:23 core-walltime Job Wall-clock time: 01:33:23 Memory Utilized: 446.20 MB Memory Efficiency: 8.71% of 5.00 GB seff-array For job arrays (see here for details) it is helpful to look at statistics for how resources are used by each element of the array. The seff-array tool takes the job ID of the array and then calculates the distribution and average CPU and memory usage: [ tl397@grace1 ~ ] $ seff - array 43283382 ========== Max Memory Usage ========== # NumSamples = 90 ; Min = 896.29 MB ; Max = 900.48 MB # Mean = 897.77 MB ; Variance = 0.40 MB ; SD = 0.63 MB ; Median 897.78 MB # each \u220e represents a count of 1 806.6628 - 896.7108 MB [ 2 ] : \u220e\u220e 896.7108 - 897.1296 MB [ 9 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 897.1296 - 897.5484 MB [ 21 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 897.5484 - 897.9672 MB [ 34 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 897.9672 - 898.3860 MB [ 15 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 898.3860 - 898.8048 MB [ 4 ] : \u220e\u220e\u220e\u220e 898.8048 - 899.2236 MB [ 1 ] : \u220e 899.2236 - 899.6424 MB [ 3 ] : \u220e\u220e\u220e 899.6424 - 900.0612 MB [ 0 ] : 900.0612 - 900.4800 MB [ 1 ] : \u220e The requested memory was 2000 MB . ========== Elapsed Time ========== # NumSamples = 90 ; Min = 00 : 03 : 25.0 ; Max = 00 : 07 : 24.0 # Mean = 00 : 05 : 45.0 ; SD = 00 : 01 : 39.0 ; Median 00 : 06 : 44.0 # each \u220e represents a count of 1 00 : 03 : 5.0 - 00 : 03 : 48.0 [ 30 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 00 : 03 : 48.0 - 00 : 04 : 11.0 [ 0 ] : 00 : 04 : 11.0 - 00 : 04 : 34.0 [ 0 ] : 00 : 04 : 34.0 - 00 : 04 : 57.0 [ 0 ] : 00 : 04 : 57.0 - 00 : 05 : 20.0 [ 0 ] : 00 : 05 : 20.0 - 00 : 05 : 43.0 [ 0 ] : 00 : 05 : 43.0 - 00 : 06 : 6.0 [ 0 ] : 00 : 06 : 6.0 - 00 : 06 : 29.0 [ 0 ] : 00 : 06 : 29.0 - 00 : 06 : 52.0 [ 30 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 00 : 06 : 52.0 - 00 : 07 : 15.0 [ 28 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e ******************************************************************************** The requested runtime was 01 : 00 : 00. The average runtime was 00 : 05 : 45.0 . Requesting less time would allow jobs to run more quickly . ******************************************************************************** This shows how efficiently the resource request was for all the jobs in an array. In this example, we see that the average memory usage was just under 1GB, which is reasonable for the 2GB requested. However, the requested runtime was for an hour, while the jobs only ran for six minutes. These jobs could have been scheduled more quickly if a more accurate runtime was specified. sacct You can also use the more flexible sacct to get that info, along with other more advanced job queries. Unfortunately, the default output from sacct is not as useful. We recommend setting an environment variable to customize the output. [rdb9@farnam1 ~]$ export SACCT_FORMAT=\"JobID%20,JobName,User,Partition,NodeList,Elapsed,State,ExitCode,MaxRSS,AllocTRES%32\" [rdb9@farnam1 ~]$ sacct -j 21294645 JobID JobName User Partition NodeList Elapsed State ExitCode MaxRSS AllocTRES -------------------- ---------- --------- ---------- --------------- ---------- ---------- -------- ---------- -------------------------------- 21294645 bash rdb9 interacti+ c06n09 01:33:23 COMPLETED 0:0 cpu=1,mem=5G,node=1,billing=1 21294645.extern extern c06n09 01:33:23 COMPLETED 0:0 716K cpu=1,mem=5G,node=1,billing=1 21294645.0 bash c06n09 01:33:23 COMPLETED 0:0 456908K cpu=1,mem=5G,node=1 You should look at the MaxRSS value to see your memory usage.","title":"Monitor CPU and Memory"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#monitor-cpu-and-memory","text":"","title":"Monitor CPU and Memory"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#general-note","text":"Making sure your jobs use the right amount of RAM and the right number of CPUs helps you and others using the clusters use these resources more effeciently, and in turn get work done more quickly. Below are some examples of how to measure your CPU and RAM (aka memory) usage so you can make this happen. Be sure to check the Slurm documentation and the clusters page (especially the partitions and hardware sections) to make sure you are submitting the right jobs to the right hardware.","title":"General Note"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#future-jobs","text":"If you launch a program by putting /usr/bin/time in front of it, time will watch your program and provide statistics about the resources it used. For example: [ be59@c01n01 ~ ] $ /usr/bin/time -v stress-ng --cpu 8 --timeout 10s stress-ng: info: [ 32574 ] dispatching hogs: 8 cpu stress-ng: info: [ 32574 ] successful run completed in 10 .08s Command being timed: \"stress-ng --cpu 8 --timeout 10s\" User time ( seconds ) : 80 .22 System time ( seconds ) : 0 .04 Percent of CPU this job got: 795 % Elapsed ( wall clock ) time ( h:mm:ss or m:ss ) : 0 :10.09 Average shared text size ( kbytes ) : 0 Average unshared data size ( kbytes ) : 0 Average stack size ( kbytes ) : 0 Average total size ( kbytes ) : 0 Maximum resident set size ( kbytes ) : 6328 Average resident set size ( kbytes ) : 0 Major ( requiring I/O ) page faults: 0 Minor ( reclaiming a frame ) page faults: 30799 Voluntary context switches: 1380 Involuntary context switches: 68 Swaps: 0 File system inputs: 0 File system outputs: 0 Socket messages sent: 0 Socket messages received: 0 Signals delivered: 0 To know how much RAM your job used (and what jobs like it will need in the future), look at the \"Maximum resident set size\"","title":"Future Jobs"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#running-jobs","text":"If your job is already running, you can check on its usage, but will have to wait until it has finished to find the maximum memory and CPU used. The easiest way to check the instantaneous memory and CPU usage of a job is to ssh to a compute node your job is running on. To find the node you should ssh to, run: [be59@farnam1 ~]$ squeue -u$USER JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 21252409 general 12345 be59 R 32:17 17 c13n[02-04],c14n[05-10],c16n[03-10] Then use ssh to connect to a node your job is running on from the NODELIST column: [be59@farnam1 ~]$ ssh c13n03 [be59@c13n03 ~]$ Once you are on the compute node, run either ps or top .","title":"Running Jobs"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#ps","text":"ps will give you instantaneous usage every time you run it. Here is some sample ps output: [ be59 @ bigmem01 ~ ] $ ps - u $ USER - o %cpu,rss,args %CPU RSS COMMAND 92.6 79446140 / gpfs / ysm / apps / hpc / Apps / Matlab / R2016b / bin / glnxa64 / MATLAB - dmlworker - nodisplay - r distcomp_evaluate_filetask 94.5 80758040 / gpfs / ysm / apps / hpc / Apps / Matlab / R2016b / bin / glnxa64 / MATLAB - dmlworker - nodisplay - r distcomp_evaluate_filetask 92.6 79676460 / gpfs / ysm / apps / hpc / Apps / Matlab / R2016b / bin / glnxa64 / MATLAB - dmlworker - nodisplay - r distcomp_evaluate_filetask 92.5 81243364 / gpfs / ysm / apps / hpc / Apps / Matlab / R2016b / bin / glnxa64 / MATLAB - dmlworker - nodisplay - r distcomp_evaluate_filetask 93.8 80799668 / gpfs / ysm / apps / hpc / Apps / Matlab / R2016b / bin / glnxa64 / MATLAB - dmlworker - nodisplay - r distcomp_evaluate_filetask ps reports memory used in kilobytes, so each of the 5 matlab processes is using ~77GB of RAM. They are also using most of 5 cores, so future jobs like this should request 5 CPUs.","title":"ps"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#top","text":"top runs interactively and shows you live usage statistics. You can press u , enter your netid, then enter to filter just your processes. For Memory usage, the number you are interested in is RES. In the case below, the YEPNEE.exe programs are each consuming ~600MB of memory and each fully utilizing one CPU. You can press ? for help and q to quit.","title":"top"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#clustershell","text":"For multi-node jobs clush can be very useful. Please see our guide on how to set up and use ClusterShell .","title":"ClusterShell"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#completed-jobs","text":"Slurm records statistics for every job, including how much memory and CPU was used.","title":"Completed Jobs"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#seff","text":"After the job completes, you can run seff <jobid> to get some useful information about your job, including the memory used and what percent of your allocated memory that amounts to. [rdb9@farnam1 ~]$ seff 21294645 Job ID: 21294645 Cluster: farnam User/Group: rdb9/lsprog State: COMPLETED (exit code 0) Cores: 1 CPU Utilized: 00:15:55 CPU Efficiency: 17.04% of 01:33:23 core-walltime Job Wall-clock time: 01:33:23 Memory Utilized: 446.20 MB Memory Efficiency: 8.71% of 5.00 GB","title":"seff"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#seff-array","text":"For job arrays (see here for details) it is helpful to look at statistics for how resources are used by each element of the array. The seff-array tool takes the job ID of the array and then calculates the distribution and average CPU and memory usage: [ tl397@grace1 ~ ] $ seff - array 43283382 ========== Max Memory Usage ========== # NumSamples = 90 ; Min = 896.29 MB ; Max = 900.48 MB # Mean = 897.77 MB ; Variance = 0.40 MB ; SD = 0.63 MB ; Median 897.78 MB # each \u220e represents a count of 1 806.6628 - 896.7108 MB [ 2 ] : \u220e\u220e 896.7108 - 897.1296 MB [ 9 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 897.1296 - 897.5484 MB [ 21 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 897.5484 - 897.9672 MB [ 34 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 897.9672 - 898.3860 MB [ 15 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 898.3860 - 898.8048 MB [ 4 ] : \u220e\u220e\u220e\u220e 898.8048 - 899.2236 MB [ 1 ] : \u220e 899.2236 - 899.6424 MB [ 3 ] : \u220e\u220e\u220e 899.6424 - 900.0612 MB [ 0 ] : 900.0612 - 900.4800 MB [ 1 ] : \u220e The requested memory was 2000 MB . ========== Elapsed Time ========== # NumSamples = 90 ; Min = 00 : 03 : 25.0 ; Max = 00 : 07 : 24.0 # Mean = 00 : 05 : 45.0 ; SD = 00 : 01 : 39.0 ; Median 00 : 06 : 44.0 # each \u220e represents a count of 1 00 : 03 : 5.0 - 00 : 03 : 48.0 [ 30 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 00 : 03 : 48.0 - 00 : 04 : 11.0 [ 0 ] : 00 : 04 : 11.0 - 00 : 04 : 34.0 [ 0 ] : 00 : 04 : 34.0 - 00 : 04 : 57.0 [ 0 ] : 00 : 04 : 57.0 - 00 : 05 : 20.0 [ 0 ] : 00 : 05 : 20.0 - 00 : 05 : 43.0 [ 0 ] : 00 : 05 : 43.0 - 00 : 06 : 6.0 [ 0 ] : 00 : 06 : 6.0 - 00 : 06 : 29.0 [ 0 ] : 00 : 06 : 29.0 - 00 : 06 : 52.0 [ 30 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 00 : 06 : 52.0 - 00 : 07 : 15.0 [ 28 ] : \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e ******************************************************************************** The requested runtime was 01 : 00 : 00. The average runtime was 00 : 05 : 45.0 . Requesting less time would allow jobs to run more quickly . ******************************************************************************** This shows how efficiently the resource request was for all the jobs in an array. In this example, we see that the average memory usage was just under 1GB, which is reasonable for the 2GB requested. However, the requested runtime was for an hour, while the jobs only ran for six minutes. These jobs could have been scheduled more quickly if a more accurate runtime was specified.","title":"seff-array"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#sacct","text":"You can also use the more flexible sacct to get that info, along with other more advanced job queries. Unfortunately, the default output from sacct is not as useful. We recommend setting an environment variable to customize the output. [rdb9@farnam1 ~]$ export SACCT_FORMAT=\"JobID%20,JobName,User,Partition,NodeList,Elapsed,State,ExitCode,MaxRSS,AllocTRES%32\" [rdb9@farnam1 ~]$ sacct -j 21294645 JobID JobName User Partition NodeList Elapsed State ExitCode MaxRSS AllocTRES -------------------- ---------- --------- ---------- --------------- ---------- ---------- -------- ---------- -------------------------------- 21294645 bash rdb9 interacti+ c06n09 01:33:23 COMPLETED 0:0 cpu=1,mem=5G,node=1,billing=1 21294645.extern extern c06n09 01:33:23 COMPLETED 0:0 716K cpu=1,mem=5G,node=1,billing=1 21294645.0 bash c06n09 01:33:23 COMPLETED 0:0 456908K cpu=1,mem=5G,node=1 You should look at the MaxRSS value to see your memory usage.","title":"sacct"},{"location":"clusters-at-yale/job-scheduling/scavenge/","text":"Scavenge Partition A scavenge partition is available on all of our clusters. It allows you to (a) run jobs outside of your normal limits (e.g. QOSMaxCpuPerUserLimit ) and (b) use unutilized cores, if available, in any private partition on the cluster. You can also use the scavenge partition to get access to unused cores in special purpose partitions, such as the \"gpu\" or \"mpi\" partitions, and unused GPUs in private partitions. However, any job running in the scavenge partition is subject to preemption if any node in use by the job is required for a job in the node's normal partition. This means that your job may be killed without advance notice, so you should only run jobs in the scavenge partition that either have checkpoint capabilities or that can otherwise be restarted with minimal loss of progress. Warning Not all jobs are a good fit for the scavenge partition, such as jobs with long startup times or jobs that run a long time between checkpoint operations. Automatically Requeue Preempted Jobs If you would like your job to be automatically added back to the queue if preempted, you can add the --requeue flag to your submission script. #SBATCH --requeue Be aware that your job, when started from a requeue, will still re-run the entire original submission script. It will only resume progress if your program has the its own ability to checkpoint and restart from previous progress. Track History of a Requeued Job When a scavenge job is requeued after preemption, it retains the same job id. However, this can make it difficult to track the history of the job (how many times it was requeued, how long it ran for each time). To view the full history of your job use the --duplicates flag for the sacct command. sacct -j <jobid> --duplicates Scavenge GPUs On Grace and Farnam, we also have a scavenge_gpu partition, that contains all scavenge-able GPU enabled nodes and has higher priority for those node than normal scavenge. In all other ways (e.g. preemption, time limit), scavenge_gpu behaves the same as the normal scavenge partition. You can see the full count of GPU nodes in the Compute Node tables on the respective cluster pages. Research Available Nodes If you are interested in specific hardware and its availability, you can use the sinfo command to query how many of each type of node is available and what features it lists. For example: sinfo -e -o \"%.6D|%c|%G|%b\" | column -ts \"|\" will show you the kinds of nodes available, and sinfo -e -o \"%.6D|%T|%c|%G|%b\" | column -ts \"|\" will break out how many nodes in each state (e.g. allocated, mixed, idle) there are. For more options see the official sinfo documentation .","title":"Scavenge Partition"},{"location":"clusters-at-yale/job-scheduling/scavenge/#scavenge-partition","text":"A scavenge partition is available on all of our clusters. It allows you to (a) run jobs outside of your normal limits (e.g. QOSMaxCpuPerUserLimit ) and (b) use unutilized cores, if available, in any private partition on the cluster. You can also use the scavenge partition to get access to unused cores in special purpose partitions, such as the \"gpu\" or \"mpi\" partitions, and unused GPUs in private partitions. However, any job running in the scavenge partition is subject to preemption if any node in use by the job is required for a job in the node's normal partition. This means that your job may be killed without advance notice, so you should only run jobs in the scavenge partition that either have checkpoint capabilities or that can otherwise be restarted with minimal loss of progress. Warning Not all jobs are a good fit for the scavenge partition, such as jobs with long startup times or jobs that run a long time between checkpoint operations.","title":"Scavenge Partition"},{"location":"clusters-at-yale/job-scheduling/scavenge/#automatically-requeue-preempted-jobs","text":"If you would like your job to be automatically added back to the queue if preempted, you can add the --requeue flag to your submission script. #SBATCH --requeue Be aware that your job, when started from a requeue, will still re-run the entire original submission script. It will only resume progress if your program has the its own ability to checkpoint and restart from previous progress.","title":"Automatically Requeue Preempted Jobs"},{"location":"clusters-at-yale/job-scheduling/scavenge/#track-history-of-a-requeued-job","text":"When a scavenge job is requeued after preemption, it retains the same job id. However, this can make it difficult to track the history of the job (how many times it was requeued, how long it ran for each time). To view the full history of your job use the --duplicates flag for the sacct command. sacct -j <jobid> --duplicates","title":"Track History of a Requeued Job"},{"location":"clusters-at-yale/job-scheduling/scavenge/#scavenge-gpus","text":"On Grace and Farnam, we also have a scavenge_gpu partition, that contains all scavenge-able GPU enabled nodes and has higher priority for those node than normal scavenge. In all other ways (e.g. preemption, time limit), scavenge_gpu behaves the same as the normal scavenge partition. You can see the full count of GPU nodes in the Compute Node tables on the respective cluster pages.","title":"Scavenge GPUs"},{"location":"clusters-at-yale/job-scheduling/scavenge/#research-available-nodes","text":"If you are interested in specific hardware and its availability, you can use the sinfo command to query how many of each type of node is available and what features it lists. For example: sinfo -e -o \"%.6D|%c|%G|%b\" | column -ts \"|\" will show you the kinds of nodes available, and sinfo -e -o \"%.6D|%T|%c|%G|%b\" | column -ts \"|\" will break out how many nodes in each state (e.g. allocated, mixed, idle) there are. For more options see the official sinfo documentation .","title":"Research Available Nodes"},{"location":"clusters-at-yale/job-scheduling/simplequeue/","text":"SimpleQueue SimpleQueue is a tool written here to streamline submission of a large number of jobs using a task file. It has a number of advantages: You can run more of your sequential jobs concurrently, since there is a limit on the number of individual qsubs you can run simultaneously. You only have one job to keep track of. If you need to shut everything down, you only need to kill one job. SimpleQueue keeps track of the status of individual jobs. Note that version 3.0+ of SimpleQueue differs from earlier versions in important ways, in particular the meaning of -n. If you have been using an earlier version, please read the following carefully! SimpleQueue is available as a module on our clusters. Run: module avail simplequeue to locate the simplequeue module on your cluster of choice. Example SimpleQueue Job For example, imagine that you have 1000 fastq files that correspond to individual samples you want to map to a genome with bowtie2 and convert to bam files with samtools . Given some initial testing, you think that 80 cpus working together will be enough to finish the job in a reasonable time. Step 1: Create Task List The first step is to create a file with a list of the \"tasks\" you want to run. Each task corresponds to what you might otherwise have run as a single job. A task can be a simple command invocation, or a sequence of commands. You can call the task file anything, but for this example assume it's called \"tasklist.txt\" and contains: module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample1 --rg SM:sample1 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1.fastq - | samtools view -Shu - | samtools sort - sample1 module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample2 --rg SM:sample2 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample2.fastq - | samtools view -Shu - | samtools sort - sample2 ... module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample1000 --rg SM:sample1000 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1000.fastq - | samtools view -Shu - | samtools sort - sample1000 For simplicity, we'll assume that tasklist, input fastq files, and indexed genome are in a directory called ~/genome_proj/mapping . Step 2: Create Submission Script Load the SimpleQueue module, then create the launch script using: sqCreateScript -q general -N genome_map -n 80 tasklist.txt > run.sh These parameters specify that the job, named genome_map, will be submitted to the general queue/partition. This job will find 80 free cores, start 80 workers on them, and begin processing tasks from the taskfile tasklist.txt . sqCreateScript takes a number of options. They differ somewhat from cluster to cluster, particularly the default values for queue, walltime, and memory. You can run sqCreateScript without any arguments to see the exact options on your cluster. Usage: -h, --help show this help message and exit -n WORKERS, --workers=WORKERS Number of workers to use. Not required. Defaults to 1. -c CORES, --cores=CORES Number of cores to request per worker. Defaults to 1. -m MEM, --mem=MEM Memory per worker. Not required. Defaults to 1G -w WALLTIME, --walltime=WALLTIME Walltime to request for the Slurm Job in form [[D-]HH:]MM:SS. Not required. Defaults to 1:00:00. -q QUEUE, --queue=QUEUE Name of queue to use. Not required. Defaults to general -N NAME, --name=NAME Base job name to use. Not required. Defaults to SimpleQueue. --logdir=LOGDIR Name of logging directory. Defaults to SQ_Files_ ${ SLURM_JOB_ID } . Step 3: Submit Your Job Now you can simply submit run.sh to the scheduler. All of the important scheduler options (queue, number of tasks, number of cpus per task) will have been set in the script so you needn't worry about them. Shortly after run.sh begins running, you should see a directory appear called SQ_Files_jobid where jobid is the jobid the scheduler assigned your job. This directory contains logs from all the tasks that are run during your job. In addition, there are a few other files that record information about the job as a whole. Of these, the most important one is SQ.log . It should be reviewed if you encounter a problem with a run. Assuming that all goes well, tasks from the tasklist file will be scheduled automatically onto the cpus you acquired until all the tasks have completed. At that time, the job will terminate, and you'll see several summary files: scheduler_jobid_out.txt : this is the stdout from simple queue proper (it is generally empty). scheduler_jobid_err.txt : this is the stderr from simple queue proper (it is generally a copy of SQ.log ). tasklist.txt.STATUS : this contains a list of all the tasks that were run, including exit status, start time, end time, pid, node run on, and the command run. tasklist.txt.REMAINING : Failed or uncompleted tasks will be listed in this file in the same format as tasklist, so that those tasks can be easily rerun. You should review the status files related to these tasks to understand why they did not complete. This list is provided for convenience. It is always a good idea to scan tasklist.STATUS to double check which tasks did in fact complete with a normal exit status. tasklist.txt.ROGUES : The simple queue system attempts to ensure that all tasks launched eventually exit (normally or abnormally). If it fails to get confirmation that a task has exited, information about the command will be written to this file. This information can be used to hunt down and kill run away processes. Other Important Options If your individual tasks need more than the default memory allocated on your cluster, you can specify a different value using -m. For example: sqCreateScript -m 10g -n 4 ... tasklist > run.sh would request 10GB of RAM for each of your workers. If your jobs are themselves multithreaded, you can request that your workers have multiple cores using the -c option: sqCreateScript -c 20 -n 4 ... tasklist > run.sh This would create 4 workers, each having access to 20 cores.","title":"SimpleQueue"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#simplequeue","text":"SimpleQueue is a tool written here to streamline submission of a large number of jobs using a task file. It has a number of advantages: You can run more of your sequential jobs concurrently, since there is a limit on the number of individual qsubs you can run simultaneously. You only have one job to keep track of. If you need to shut everything down, you only need to kill one job. SimpleQueue keeps track of the status of individual jobs. Note that version 3.0+ of SimpleQueue differs from earlier versions in important ways, in particular the meaning of -n. If you have been using an earlier version, please read the following carefully! SimpleQueue is available as a module on our clusters. Run: module avail simplequeue to locate the simplequeue module on your cluster of choice.","title":"SimpleQueue"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#example-simplequeue-job","text":"For example, imagine that you have 1000 fastq files that correspond to individual samples you want to map to a genome with bowtie2 and convert to bam files with samtools . Given some initial testing, you think that 80 cpus working together will be enough to finish the job in a reasonable time.","title":"Example SimpleQueue Job"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#step-1-create-task-list","text":"The first step is to create a file with a list of the \"tasks\" you want to run. Each task corresponds to what you might otherwise have run as a single job. A task can be a simple command invocation, or a sequence of commands. You can call the task file anything, but for this example assume it's called \"tasklist.txt\" and contains: module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample1 --rg SM:sample1 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1.fastq - | samtools view -Shu - | samtools sort - sample1 module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample2 --rg SM:sample2 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample2.fastq - | samtools view -Shu - | samtools sort - sample2 ... module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample1000 --rg SM:sample1000 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1000.fastq - | samtools view -Shu - | samtools sort - sample1000 For simplicity, we'll assume that tasklist, input fastq files, and indexed genome are in a directory called ~/genome_proj/mapping .","title":"Step 1: Create Task List"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#step-2-create-submission-script","text":"Load the SimpleQueue module, then create the launch script using: sqCreateScript -q general -N genome_map -n 80 tasklist.txt > run.sh These parameters specify that the job, named genome_map, will be submitted to the general queue/partition. This job will find 80 free cores, start 80 workers on them, and begin processing tasks from the taskfile tasklist.txt . sqCreateScript takes a number of options. They differ somewhat from cluster to cluster, particularly the default values for queue, walltime, and memory. You can run sqCreateScript without any arguments to see the exact options on your cluster. Usage: -h, --help show this help message and exit -n WORKERS, --workers=WORKERS Number of workers to use. Not required. Defaults to 1. -c CORES, --cores=CORES Number of cores to request per worker. Defaults to 1. -m MEM, --mem=MEM Memory per worker. Not required. Defaults to 1G -w WALLTIME, --walltime=WALLTIME Walltime to request for the Slurm Job in form [[D-]HH:]MM:SS. Not required. Defaults to 1:00:00. -q QUEUE, --queue=QUEUE Name of queue to use. Not required. Defaults to general -N NAME, --name=NAME Base job name to use. Not required. Defaults to SimpleQueue. --logdir=LOGDIR Name of logging directory. Defaults to SQ_Files_ ${ SLURM_JOB_ID } .","title":"Step 2: Create Submission Script"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#step-3-submit-your-job","text":"Now you can simply submit run.sh to the scheduler. All of the important scheduler options (queue, number of tasks, number of cpus per task) will have been set in the script so you needn't worry about them. Shortly after run.sh begins running, you should see a directory appear called SQ_Files_jobid where jobid is the jobid the scheduler assigned your job. This directory contains logs from all the tasks that are run during your job. In addition, there are a few other files that record information about the job as a whole. Of these, the most important one is SQ.log . It should be reviewed if you encounter a problem with a run. Assuming that all goes well, tasks from the tasklist file will be scheduled automatically onto the cpus you acquired until all the tasks have completed. At that time, the job will terminate, and you'll see several summary files: scheduler_jobid_out.txt : this is the stdout from simple queue proper (it is generally empty). scheduler_jobid_err.txt : this is the stderr from simple queue proper (it is generally a copy of SQ.log ). tasklist.txt.STATUS : this contains a list of all the tasks that were run, including exit status, start time, end time, pid, node run on, and the command run. tasklist.txt.REMAINING : Failed or uncompleted tasks will be listed in this file in the same format as tasklist, so that those tasks can be easily rerun. You should review the status files related to these tasks to understand why they did not complete. This list is provided for convenience. It is always a good idea to scan tasklist.STATUS to double check which tasks did in fact complete with a normal exit status. tasklist.txt.ROGUES : The simple queue system attempts to ensure that all tasks launched eventually exit (normally or abnormally). If it fails to get confirmation that a task has exited, information about the command will be written to this file. This information can be used to hunt down and kill run away processes.","title":"Step 3: Submit Your Job"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#other-important-options","text":"If your individual tasks need more than the default memory allocated on your cluster, you can specify a different value using -m. For example: sqCreateScript -m 10g -n 4 ... tasklist > run.sh would request 10GB of RAM for each of your workers. If your jobs are themselves multithreaded, you can request that your workers have multiple cores using the -c option: sqCreateScript -c 20 -n 4 ... tasklist > run.sh This would create 4 workers, each having access to 20 cores.","title":"Other Important Options"},{"location":"clusters-at-yale/job-scheduling/slurm-account/","text":"Slurm Account Coordinator On the clusters the YCRC maintains, we map your linux user and group to your Slurm user and account, which is what actually gives you permission to submit to the various partitions available on the clusters. By changing the Slurm accounts associated with your user, you can modify access to partitions. As a coordinator of an account, you have permission to modify users' association with that account and modify jobs running that are associated with that account. Below are some useful example commands where we use an example user with the name \"be59\" where you are the coordinator of the slurm account \"cryoem\". Add/Remove Users From an Account sacctmgr add user be59 account = cryoem # add user sacctmgr remove user where user = be59 and account = cryoem # remove user Show Account Info sacctmgr show assoc user = be59 # show user associations sacctmgr show assoc account = cryoem # show assocations for account Submit Jobs srun -A cryoem ... sbatch -A cryoem my_script.sh List Jobs squeue -A cryoem # by account squeue -u be59 # by user Cancel Jobs scancel 1234 # by job ID scancel -u be59 # kill all jobs by user scancel -u be59 --state = running # kill running jobs by user scancel -u be59 --state = pending # kill pending jobs by user scancel -A cryoem # kill all jobs in the account Hold and Release Jobs scontrol hold 1234 # by job ID scontrol release 1234 # remove the hold scontrol uhold 1234 # hold job 1234 but allow the job's owner to release it","title":"Slurm Account Coordinator"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#slurm-account-coordinator","text":"On the clusters the YCRC maintains, we map your linux user and group to your Slurm user and account, which is what actually gives you permission to submit to the various partitions available on the clusters. By changing the Slurm accounts associated with your user, you can modify access to partitions. As a coordinator of an account, you have permission to modify users' association with that account and modify jobs running that are associated with that account. Below are some useful example commands where we use an example user with the name \"be59\" where you are the coordinator of the slurm account \"cryoem\".","title":"Slurm Account Coordinator"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#addremove-users-from-an-account","text":"sacctmgr add user be59 account = cryoem # add user sacctmgr remove user where user = be59 and account = cryoem # remove user","title":"Add/Remove Users From an Account"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#show-account-info","text":"sacctmgr show assoc user = be59 # show user associations sacctmgr show assoc account = cryoem # show assocations for account","title":"Show Account Info"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#submit-jobs","text":"srun -A cryoem ... sbatch -A cryoem my_script.sh","title":"Submit Jobs"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#list-jobs","text":"squeue -A cryoem # by account squeue -u be59 # by user","title":"List Jobs"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#cancel-jobs","text":"scancel 1234 # by job ID scancel -u be59 # kill all jobs by user scancel -u be59 --state = running # kill running jobs by user scancel -u be59 --state = pending # kill pending jobs by user scancel -A cryoem # kill all jobs in the account","title":"Cancel Jobs"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#hold-and-release-jobs","text":"scontrol hold 1234 # by job ID scontrol release 1234 # remove the hold scontrol uhold 1234 # hold job 1234 but allow the job's owner to release it","title":"Hold and Release Jobs"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/","text":"Submission Script Examples In addition to those below, we have additional example submission scripts for Parallel R, Matlab and Python . Single threaded programs (basic) 1 2 3 4 5 6 #!/bin/bash #SBATCH --job-name=my_job #SBATCH --time=10:00 ./hello.omp Multi-threaded programs 1 2 3 4 5 6 7 8 9 10 #!/bin/bash #SBATCH --job-name=omp_job #SBATCH --output=omp_job.txt #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --time=10:00 export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK ./hello.omp Multi-process programs 1 2 3 4 5 6 7 8 #!/bin/bash #SBATCH --job-name=mpi #SBATCH --output=mpi_job.txt #SBATCH --ntasks=4 #SBATCH --time=10:00 mpirun hello.mpi Tip On Grace's mpi partition, try to make ntasks equal to a multiple of 24. Hybrid (MPI+OpenMP) programs 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash #SBATCH --job-name=hybrid #SBATCH --output=hydrid_job.txt #SBATCH --ntasks=8 #SBATCH --cpus-per-task=5 #SBATCH --nodes=2 #SBATCH --time=10:00 export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK mpirun hello_hybrid.mpi GPU job 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash #SBATCH --job-name=deep_learn #SBATCH --output=gpu_job.txt #SBATCH --ntasks=1 #SBATCH --cpus-per-task=2 #SBATCH --gres=gpu:k80:2 #SBATCH --partition=gpu #SBATCH --time=10:00 module load CUDA module load cuDNN # using your anaconda environment source activate deep-learn python my_tensorflow.py","title":"Submission Script Examples"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#submission-script-examples","text":"In addition to those below, we have additional example submission scripts for Parallel R, Matlab and Python .","title":"Submission Script Examples"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#single-threaded-programs-basic","text":"1 2 3 4 5 6 #!/bin/bash #SBATCH --job-name=my_job #SBATCH --time=10:00 ./hello.omp","title":"Single threaded programs (basic)"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#multi-threaded-programs","text":"1 2 3 4 5 6 7 8 9 10 #!/bin/bash #SBATCH --job-name=omp_job #SBATCH --output=omp_job.txt #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --time=10:00 export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK ./hello.omp","title":"Multi-threaded programs"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#multi-process-programs","text":"1 2 3 4 5 6 7 8 #!/bin/bash #SBATCH --job-name=mpi #SBATCH --output=mpi_job.txt #SBATCH --ntasks=4 #SBATCH --time=10:00 mpirun hello.mpi Tip On Grace's mpi partition, try to make ntasks equal to a multiple of 24.","title":"Multi-process programs"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#hybrid-mpiopenmp-programs","text":"1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash #SBATCH --job-name=hybrid #SBATCH --output=hydrid_job.txt #SBATCH --ntasks=8 #SBATCH --cpus-per-task=5 #SBATCH --nodes=2 #SBATCH --time=10:00 export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK mpirun hello_hybrid.mpi","title":"Hybrid (MPI+OpenMP) programs"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#gpu-job","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash #SBATCH --job-name=deep_learn #SBATCH --output=gpu_job.txt #SBATCH --ntasks=1 #SBATCH --cpus-per-task=2 #SBATCH --gres=gpu:k80:2 #SBATCH --partition=gpu #SBATCH --time=10:00 module load CUDA module load cuDNN # using your anaconda environment source activate deep-learn python my_tensorflow.py","title":"GPU job"},{"location":"data/","text":"Data Storage Options Google Drive via EliApps Capacity: Unlimited. Cost: Free - 5TB max file size No sensitive data (e.g. ePHI, HIPAA) Can be mounted on your local machine and transferred to via Globus Google Drive Connector Google Drive is a cloud service for file storage, document editing and sharing. All members of the Yale community with an EliApps (Google Apps for Education) account have unlimited (yes, unlimited) storage at no cost in the associated Google Drive account. Moreover, EliApps users can request Shared Drives, which are shared spaces where all files are group-owned. For more information on Google Drive through EliApps, see our Google Drive documentation for more information. Storage @ Yale Capacity: As requested. Cost: See below No sensitive data (e.g. ePHI, HIPAA) for cluster mounts Can be mounted on the cluster or computers on campus (but not both) Storage @ Yale (S@Y) is a central storage service provided by ITS. S@Y shares can either be accessible on campus computers or the clusters, but not both. All prices are charged monthly for storage used at that time. Standard (daily use): $10.78/TB/month Archive (long term storage): $3.60/TB/month Enhanced (higher performance): $33.87/TB/month For most up to date pricing information, see the ITS Data Rates . To request a share, press the \u201cRequest this Service\u201d button in the right sidebar on the Storage@Yale website . If you would like to request a share that is mounted on the clusters, specify in your request that the share be mounted from the HPC clusters . If you elect to use archive tier storage, be cognizant of its performance characteristics . Warning Cluster-mounted S@Y shares do not provide sufficient performance for computation. We mount the shares on the cluster for convenience but strongly recommend data be copied to project or scratch60 before running jobs. Box at Yale Capacity: 50GB per user. Cost: Free. 15 GB max file size. Sensitive data (e.g. ePHI, HIPAA) only in Secure Box Can be mounted on your local machine and transferred with rclone All members of the Yale community have access to a share at Box at Yale. Box is another cloud-based file sharing and storage service. You can upload and access your data using the web portal and sync data with your local machines via Box Sync. To access, navigate to yale.box.com and login with your yale.edu account. For sync with your local machine, install Box Sync and authenticate with your yale.edu account. For more information about Box at Yale, see the ITS website.","title":"Data Storage Options"},{"location":"data/#data-storage-options","text":"","title":"Data Storage Options"},{"location":"data/#google-drive-via-eliapps","text":"Capacity: Unlimited. Cost: Free - 5TB max file size No sensitive data (e.g. ePHI, HIPAA) Can be mounted on your local machine and transferred to via Globus Google Drive Connector Google Drive is a cloud service for file storage, document editing and sharing. All members of the Yale community with an EliApps (Google Apps for Education) account have unlimited (yes, unlimited) storage at no cost in the associated Google Drive account. Moreover, EliApps users can request Shared Drives, which are shared spaces where all files are group-owned. For more information on Google Drive through EliApps, see our Google Drive documentation for more information.","title":"Google Drive via EliApps"},{"location":"data/#storage-yale","text":"Capacity: As requested. Cost: See below No sensitive data (e.g. ePHI, HIPAA) for cluster mounts Can be mounted on the cluster or computers on campus (but not both) Storage @ Yale (S@Y) is a central storage service provided by ITS. S@Y shares can either be accessible on campus computers or the clusters, but not both. All prices are charged monthly for storage used at that time. Standard (daily use): $10.78/TB/month Archive (long term storage): $3.60/TB/month Enhanced (higher performance): $33.87/TB/month For most up to date pricing information, see the ITS Data Rates . To request a share, press the \u201cRequest this Service\u201d button in the right sidebar on the Storage@Yale website . If you would like to request a share that is mounted on the clusters, specify in your request that the share be mounted from the HPC clusters . If you elect to use archive tier storage, be cognizant of its performance characteristics . Warning Cluster-mounted S@Y shares do not provide sufficient performance for computation. We mount the shares on the cluster for convenience but strongly recommend data be copied to project or scratch60 before running jobs.","title":"Storage @ Yale"},{"location":"data/#box-at-yale","text":"Capacity: 50GB per user. Cost: Free. 15 GB max file size. Sensitive data (e.g. ePHI, HIPAA) only in Secure Box Can be mounted on your local machine and transferred with rclone All members of the Yale community have access to a share at Box at Yale. Box is another cloud-based file sharing and storage service. You can upload and access your data using the web portal and sync data with your local machines via Box Sync. To access, navigate to yale.box.com and login with your yale.edu account. For sync with your local machine, install Box Sync and authenticate with your yale.edu account. For more information about Box at Yale, see the ITS website.","title":"Box at Yale"},{"location":"data/archive/","text":"Archive Your Data Clean Out Unnecessary Files Not every file created during a project needs to be archived. If you proactively reduce the number of extraneous files in your archive, you will both reduce storage costs and increase the usefulness of that data upon retrieval. Common files that can be deleted when archiving data include: Compiled codes, such as .o or .pyc files. These files will likely not even work on the next system you may restore these data to and they can contribute significantly to your file count limit. Just keep the source code and clean installation instructions. Some log files. Many log created by the system are not necessary to store indefinitely. Any Slurm logs from failed runs (prior to a successful run) or outputs from Matlab (e.g. hs_error_pid*.log , java.log.* ) can often safely be ignored. Crash files such are core dumps (e.g. core.* , matlab_crash_dump. ). Compress Your Data Most archive locations (S@Y Archive Tier, Google Drive) perform much better with a smaller number of larger files. In fact, Google Shared Drives have a file count limit of 400,000 files. Therefore, it is highly recommended that your compress, using zip or tar , portions of your data for ease of storage and retrieval. Tips for S@Y Archive Tier (or Any Tape Archive) The archive tier of Storage@Yale is a tape-based system. To use it effectively, you need to be aware of how it works and follow some best practices. When you write to the archive, you are actually copying to a large hard disk-based cache, so writes are normally fast. Your copy will appear to complete as soon as the file is in the disk cache. It is NOT yet on tape. In the background, the system will flush files to tape and delete them from the cache. If you read a file very soon after you write it, it is probably still in the cache, and your read will be quick. However, once some time has elapsed and the file has been moved to tape, it can take several minutes or even longer to copy a file from the archive. This is because the system has to: Wait until a tape drive is free (there are a limited number of drives) Load the correct tape Find the file on the tape And finally, copy it out to a disk cache in front of the tape Only then is the file available to your copy command. There are only a handful of tape drives, so you may have to wait for one to be available. Some key takeaways: Operations that only read the metadata of files will be fast (ls, find, etc) even if the file is on tape, since metadata is kept in the disk cache. Operations that actually read the file (cp, wc -l, tar, etc) will require recovering the entire file to disk cache first, and can take several minutes or longer depending on how nusy the system is. If many files will need to be recovered together, it is much better to store them as a single file first with tar or zip, then write that file to the archive. Please do NOT write huge numbers of small files. They will be difficult or impossible to restore in large numbers. Please do NOT do repetitive operations like rsyncs to the archive, since they overload the system.","title":"Archive Your Data"},{"location":"data/archive/#archive-your-data","text":"","title":"Archive Your Data"},{"location":"data/archive/#clean-out-unnecessary-files","text":"Not every file created during a project needs to be archived. If you proactively reduce the number of extraneous files in your archive, you will both reduce storage costs and increase the usefulness of that data upon retrieval. Common files that can be deleted when archiving data include: Compiled codes, such as .o or .pyc files. These files will likely not even work on the next system you may restore these data to and they can contribute significantly to your file count limit. Just keep the source code and clean installation instructions. Some log files. Many log created by the system are not necessary to store indefinitely. Any Slurm logs from failed runs (prior to a successful run) or outputs from Matlab (e.g. hs_error_pid*.log , java.log.* ) can often safely be ignored. Crash files such are core dumps (e.g. core.* , matlab_crash_dump. ).","title":"Clean Out Unnecessary Files"},{"location":"data/archive/#compress-your-data","text":"Most archive locations (S@Y Archive Tier, Google Drive) perform much better with a smaller number of larger files. In fact, Google Shared Drives have a file count limit of 400,000 files. Therefore, it is highly recommended that your compress, using zip or tar , portions of your data for ease of storage and retrieval.","title":"Compress Your Data"},{"location":"data/archive/#tips-for-sy-archive-tier-or-any-tape-archive","text":"The archive tier of Storage@Yale is a tape-based system. To use it effectively, you need to be aware of how it works and follow some best practices. When you write to the archive, you are actually copying to a large hard disk-based cache, so writes are normally fast. Your copy will appear to complete as soon as the file is in the disk cache. It is NOT yet on tape. In the background, the system will flush files to tape and delete them from the cache. If you read a file very soon after you write it, it is probably still in the cache, and your read will be quick. However, once some time has elapsed and the file has been moved to tape, it can take several minutes or even longer to copy a file from the archive. This is because the system has to: Wait until a tape drive is free (there are a limited number of drives) Load the correct tape Find the file on the tape And finally, copy it out to a disk cache in front of the tape Only then is the file available to your copy command. There are only a handful of tape drives, so you may have to wait for one to be available. Some key takeaways: Operations that only read the metadata of files will be fast (ls, find, etc) even if the file is on tape, since metadata is kept in the disk cache. Operations that actually read the file (cp, wc -l, tar, etc) will require recovering the entire file to disk cache first, and can take several minutes or longer depending on how nusy the system is. If many files will need to be recovered together, it is much better to store them as a single file first with tar or zip, then write that file to the archive. Please do NOT write huge numbers of small files. They will be difficult or impossible to restore in large numbers. Please do NOT do repetitive operations like rsyncs to the archive, since they overload the system.","title":"Tips for S@Y Archive Tier (or Any Tape Archive)"},{"location":"data/external/","text":"Serving Research Data Externally You can use a static website to serve data publicly to collaborators or services that need to see the data via http. A common example of this is hosting tracks for the UCSC Genome Browser. To set one up, first get an account on ITS's Spinup service. After that, follow their instructions on creating a static website , giving it an appropriate website name. Then use an S3 transfer tool like AWS CLI, CrossFTP, or Cyberduck to transfer your files. Info There will be a cost for storing the data (a few cents per GB per month), which you can use Yale charging instructions to pay for.","title":"Serving Research Data Externally"},{"location":"data/external/#serving-research-data-externally","text":"You can use a static website to serve data publicly to collaborators or services that need to see the data via http. A common example of this is hosting tracks for the UCSC Genome Browser. To set one up, first get an account on ITS's Spinup service. After that, follow their instructions on creating a static website , giving it an appropriate website name. Then use an S3 transfer tool like AWS CLI, CrossFTP, or Cyberduck to transfer your files. Info There will be a cost for storing the data (a few cents per GB per month), which you can use Yale charging instructions to pay for.","title":"Serving Research Data Externally"},{"location":"data/google-drive/","text":"Google Drive Through Yale Google Apps for Education (EliApps), researchers have free Google Drive storage with very few limits on storage. The Globus Google Drive connector allows you to create a Globus endpoint that allows you to use the Globus infrastructure to transfer data into your Google Drive account. As always, no sensitive data (e.g. ePHI, HIPAA) is allowed in Google Drive storage. EliApps If your Yale email account is already an EliApps account (Gmail), then you are all set. If your Yale email is in Microsoft Office365, send an email to the ITS helpdesk requesting a \"no-email EliApps account\". Once it is created you can login to Google Drive using your EliApps account name, which will be of the form netid@yale.edu . The Globus connector is configured to only allow data to be uploaded into EliApps Google Drive accounts. Google Shared Drives (formerly Team Drive) Shared Drives is an additional feature for EliApps that is available by request only (at the moment). A Shared Drive is a Google Drive space that solves a lot of ownership and permissions issues present with traditional shared Google Drive folder. Once you create a Shared Drive, e.g. for a project or research group, any data placed in that Drive are owned by the drive and the permission (which accounts can own or access the data) can be easily managed from the Shared Drive interface by drive owners. With Shared Drive, you can be sure the data will stay with research group as students and postdocs come and go. To request Shared Drive, first make sure you have an EliApps account (see above) then send us a request to research.computing@yale.edu. We will work with ITS on your behalf to enable the feature. Although they don't have a total size quota, there are limits for Google Shared Drives . Some are listed below. Warning To keep file counts low (and for easier data retrieval) we highly recommended that you archive your data using zip or tar . Limit type Limit Number of files and folders 400,000 Daily upload cap 750 GB Max individual file size 5 TB Max number of nested folders 20 Local File Access You can upload and access your data using the web portal and sync data with your local machines via the Google File Stream software. For sync with your local machine, install Google File Stream . Select the Download button on the left side under \u201cBusiness\u201d and authenticate with your EliApps account. You will see Google Drive mounted as an additional drive on your machine. Rclone You can also transfer data using the command line utility Rclone . Rclone can be used to transfer data to any Google Drive account. Globus Google Drive Connector The Globus connector is configured to only allow data to be uploaded into EliApps (Yale's GSuite for Education) Google Drive accounts. If you don't have an EliApps account, request one as described above. Set Up Your Endpoint To set up your Globus Google Drive endpoint, click on the following link: Setup Globus Google Drive Endpoint Log into Globus, if needed. The first time you create an endpoint, you will be presented with a permissions approval page. If you are ok with the Connector manipulating your files through Globus (which is required), click the Allow button. The next page should say \"Create a shared endpoint\". Click on \"Yale Google Drive Gateway (Google Drive)\". Again, the first time you create an endpoint, you will be asked to register your Google EliApps account with Globus. Put in your EliApps account (either your email address if you are an EliApps user, or <netid>@yale.edu if you are no-email EliApps user) and submit the form. You will then be asked on a series of Google pages to select or login into your EliApps account and then approve Globus to write to your Google Drive. You will then be redirected back to Globus to fill out a form to \"Create a Shared Endpoint\". The only required field are (all others can be left blank): \"Credentials\" - should be prefilled with your yale account \"Endpoint Display Name\" - this is the name of your endpoint and the name you will use to search for the endpoint in the Globus transfer interface. We recommend including your netid or name in the endpoint so you can uniquely identify it, such as \" Google Drive\" After filling out the form, click the \"Create Endpoint\" button. If your endpoint was successfully created, you should see a page with a green checkmark and three links. Click on the middle link to start transferring data to or from your Google Drive! Using Your Endpoint On the Globus Transfer page, select an endpoint for each side of your transfer. To transfer to or from your Google Drive, simply search in the Endpoint field for the name of the Endpoint you created above (e.g. \" Google Drive\"). There are \"rate limits\" to how much data and how many files you can transfer in any 24 hours period. If you are archiving data to Google Drive, it is much better to first compress folders that contain lots of small files (e.g. using tar ) before transferring. If you have hit your rate limit, Globus should automatically resume the transfer during the next 24 hour period. If you click the \"sync\" checkbox in the Transfer Setting window on the Globus page, Globus should resume the transfer where it left off when it hit the limit. In our testing, we have seen up to 10MB/s upload and 100MB/s download speeds. Managing Your Endpoint To manage your endpoint, such as delete the endpoint, rename it, or share it with additional people (be aware, they will be able to access your Google Drive), go to Manage Endpoint on the Globus website.","title":"Google Drive"},{"location":"data/google-drive/#google-drive","text":"Through Yale Google Apps for Education (EliApps), researchers have free Google Drive storage with very few limits on storage. The Globus Google Drive connector allows you to create a Globus endpoint that allows you to use the Globus infrastructure to transfer data into your Google Drive account. As always, no sensitive data (e.g. ePHI, HIPAA) is allowed in Google Drive storage.","title":"Google Drive"},{"location":"data/google-drive/#eliapps","text":"If your Yale email account is already an EliApps account (Gmail), then you are all set. If your Yale email is in Microsoft Office365, send an email to the ITS helpdesk requesting a \"no-email EliApps account\". Once it is created you can login to Google Drive using your EliApps account name, which will be of the form netid@yale.edu . The Globus connector is configured to only allow data to be uploaded into EliApps Google Drive accounts.","title":"EliApps"},{"location":"data/google-drive/#google-shared-drives-formerly-team-drive","text":"Shared Drives is an additional feature for EliApps that is available by request only (at the moment). A Shared Drive is a Google Drive space that solves a lot of ownership and permissions issues present with traditional shared Google Drive folder. Once you create a Shared Drive, e.g. for a project or research group, any data placed in that Drive are owned by the drive and the permission (which accounts can own or access the data) can be easily managed from the Shared Drive interface by drive owners. With Shared Drive, you can be sure the data will stay with research group as students and postdocs come and go. To request Shared Drive, first make sure you have an EliApps account (see above) then send us a request to research.computing@yale.edu. We will work with ITS on your behalf to enable the feature. Although they don't have a total size quota, there are limits for Google Shared Drives . Some are listed below. Warning To keep file counts low (and for easier data retrieval) we highly recommended that you archive your data using zip or tar . Limit type Limit Number of files and folders 400,000 Daily upload cap 750 GB Max individual file size 5 TB Max number of nested folders 20","title":"Google Shared Drives (formerly Team Drive)"},{"location":"data/google-drive/#local-file-access","text":"You can upload and access your data using the web portal and sync data with your local machines via the Google File Stream software. For sync with your local machine, install Google File Stream . Select the Download button on the left side under \u201cBusiness\u201d and authenticate with your EliApps account. You will see Google Drive mounted as an additional drive on your machine.","title":"Local File Access"},{"location":"data/google-drive/#rclone","text":"You can also transfer data using the command line utility Rclone . Rclone can be used to transfer data to any Google Drive account.","title":"Rclone"},{"location":"data/google-drive/#globus-google-drive-connector","text":"The Globus connector is configured to only allow data to be uploaded into EliApps (Yale's GSuite for Education) Google Drive accounts. If you don't have an EliApps account, request one as described above.","title":"Globus Google Drive Connector"},{"location":"data/google-drive/#set-up-your-endpoint","text":"To set up your Globus Google Drive endpoint, click on the following link: Setup Globus Google Drive Endpoint Log into Globus, if needed. The first time you create an endpoint, you will be presented with a permissions approval page. If you are ok with the Connector manipulating your files through Globus (which is required), click the Allow button. The next page should say \"Create a shared endpoint\". Click on \"Yale Google Drive Gateway (Google Drive)\". Again, the first time you create an endpoint, you will be asked to register your Google EliApps account with Globus. Put in your EliApps account (either your email address if you are an EliApps user, or <netid>@yale.edu if you are no-email EliApps user) and submit the form. You will then be asked on a series of Google pages to select or login into your EliApps account and then approve Globus to write to your Google Drive. You will then be redirected back to Globus to fill out a form to \"Create a Shared Endpoint\". The only required field are (all others can be left blank): \"Credentials\" - should be prefilled with your yale account \"Endpoint Display Name\" - this is the name of your endpoint and the name you will use to search for the endpoint in the Globus transfer interface. We recommend including your netid or name in the endpoint so you can uniquely identify it, such as \" Google Drive\" After filling out the form, click the \"Create Endpoint\" button. If your endpoint was successfully created, you should see a page with a green checkmark and three links. Click on the middle link to start transferring data to or from your Google Drive!","title":"Set Up Your Endpoint"},{"location":"data/google-drive/#using-your-endpoint","text":"On the Globus Transfer page, select an endpoint for each side of your transfer. To transfer to or from your Google Drive, simply search in the Endpoint field for the name of the Endpoint you created above (e.g. \" Google Drive\"). There are \"rate limits\" to how much data and how many files you can transfer in any 24 hours period. If you are archiving data to Google Drive, it is much better to first compress folders that contain lots of small files (e.g. using tar ) before transferring. If you have hit your rate limit, Globus should automatically resume the transfer during the next 24 hour period. If you click the \"sync\" checkbox in the Transfer Setting window on the Globus page, Globus should resume the transfer where it left off when it hit the limit. In our testing, we have seen up to 10MB/s upload and 100MB/s download speeds.","title":"Using Your Endpoint"},{"location":"data/google-drive/#managing-your-endpoint","text":"To manage your endpoint, such as delete the endpoint, rename it, or share it with additional people (be aware, they will be able to access your Google Drive), go to Manage Endpoint on the Globus website.","title":"Managing Your Endpoint"}]}