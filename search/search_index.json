{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The Yale Center for Research Computing provides support for research computing at Yale University. Our most active area for support is High Performance Computing, however we also support other computational intensive research such as Geo-Spatial Computation and Data Visualization. In addition, we work with faculty and research groups across disciplines to design and maintain cost-effective computing capabilities. Get Help To best serve the research community, we hold office hours and are also available through a support tracking system. Troubleshooting Login Issues If you are experiencing issues logging into one of the clusters, please first check the current System Status for known issues and use the Troubleshoot Login guide to address common issues before seeking additional assistance. In Person The YCRC is located on the second floor of 160 Saint Ronan Street . Visitor parking is available upon request. There are also Red, Orange, Blue and Green shuttle stops within a block of our center. Specialist Day Hours Office Areas of Focus Kaylea Nelson, Ph.D. Mon 10am-Noon 225 Grace / Omega / Milgram , Astronomy, G&G, MPI, Python Tom Langford, Ph.D. Mon 2-4pm 225 Grace / Omega / Milgram , Physics, Python Jason Ignatius Tues 2-4pm 216 Farnam / Ruddle , Life Sciences, Bioinformatics, PacBio Ben Evans, Ph.D. Wed 10am-Noon 224 Farnam / Ruddle , Life Sciences, Bioinformatics, Python Rob Bjornson, Ph.D. Thurs 2-4pm 216 Farnam / Ruddle , Life Sciences, Bioinformatics, Python Andy Sherman, Ph.D. Fri 1:30-3:30pm 217 Grace / Omega , MPI, GPUs, Matlab Steve Weston via email Grace / Omega / Milgram , R, Python, Deep learning Giuseppe Amatulli, Ph.D. via email Geo-Computation, GIS and Remote Sensing, Forestry Email To submit requests, concerns or questions via email please send to hpc@yale.edu and your communication will be logged directly with our system. An automated response will be sent back with a tracking number. Replies to your communication will be addressed from YCRC Support and will include the tracking number for reference. Any additional communication, as either replies from you or initiated by us, will be tracked in the system and can easily be referenced by the tracking number. Web Interface Alternatively, a web interface is available for both initiating and tracking a request or concern. After navigating to the website, you can select to log a new ticket or track an existing one. After your selection you will be able to login with CAS. If creating a new ticket, a new ticket form will appear where you can log your request or concern. If selecting an existing issue, a list of open items, assigned to you, will appear. Status of your request or concern can be viewed by selecting the tracking number. If you have any questions or feedback, we would be happy to hear from you.","title":"Introduction"},{"location":"#introduction","text":"The Yale Center for Research Computing provides support for research computing at Yale University. Our most active area for support is High Performance Computing, however we also support other computational intensive research such as Geo-Spatial Computation and Data Visualization. In addition, we work with faculty and research groups across disciplines to design and maintain cost-effective computing capabilities.","title":"Introduction"},{"location":"#get-help","text":"To best serve the research community, we hold office hours and are also available through a support tracking system. Troubleshooting Login Issues If you are experiencing issues logging into one of the clusters, please first check the current System Status for known issues and use the Troubleshoot Login guide to address common issues before seeking additional assistance.","title":"Get Help"},{"location":"#in-person","text":"The YCRC is located on the second floor of 160 Saint Ronan Street . Visitor parking is available upon request. There are also Red, Orange, Blue and Green shuttle stops within a block of our center. Specialist Day Hours Office Areas of Focus Kaylea Nelson, Ph.D. Mon 10am-Noon 225 Grace / Omega / Milgram , Astronomy, G&G, MPI, Python Tom Langford, Ph.D. Mon 2-4pm 225 Grace / Omega / Milgram , Physics, Python Jason Ignatius Tues 2-4pm 216 Farnam / Ruddle , Life Sciences, Bioinformatics, PacBio Ben Evans, Ph.D. Wed 10am-Noon 224 Farnam / Ruddle , Life Sciences, Bioinformatics, Python Rob Bjornson, Ph.D. Thurs 2-4pm 216 Farnam / Ruddle , Life Sciences, Bioinformatics, Python Andy Sherman, Ph.D. Fri 1:30-3:30pm 217 Grace / Omega , MPI, GPUs, Matlab Steve Weston via email Grace / Omega / Milgram , R, Python, Deep learning Giuseppe Amatulli, Ph.D. via email Geo-Computation, GIS and Remote Sensing, Forestry","title":"In Person"},{"location":"#email","text":"To submit requests, concerns or questions via email please send to hpc@yale.edu and your communication will be logged directly with our system. An automated response will be sent back with a tracking number. Replies to your communication will be addressed from YCRC Support and will include the tracking number for reference. Any additional communication, as either replies from you or initiated by us, will be tracked in the system and can easily be referenced by the tracking number.","title":"Email"},{"location":"#web-interface","text":"Alternatively, a web interface is available for both initiating and tracking a request or concern. After navigating to the website, you can select to log a new ticket or track an existing one. After your selection you will be able to login with CAS. If creating a new ticket, a new ticket form will appear where you can log your request or concern. If selecting an existing issue, a list of open items, assigned to you, will appear. Status of your request or concern can be viewed by selecting the tracking number. If you have any questions or feedback, we would be happy to hear from you.","title":"Web Interface"},{"location":"national-hpcs/","text":"National HPCs Beyond Yale\u2019s on campus clusters, there are a number of ways for researchers to obtain compute resources (both cycles and storage) at national facilities. Yale researchers may use the Data Management Planning Tool ( DMPtool ) to create, review, and share data management plans that are in accordance with institutional and funder requirements. XSEDE Quarterly | Application & Info Startup Allocations are readily available on XSEDE resources (typically approved within 2 weeks) for benchmarking and planning runs. For even lower commitment allocations (e.g. to just explore the resource), YCRC staff members have \"Campus Champions\" allocations on all XSEDE resources that can be shared upon request. Send an email to hpc@yale.edu for access. XSEDE resources include the following. Up to date information is available at xsede.org : Stampede2: traditional compute and Phis Jetstream: Science Gateways Bridges: traditional compute and GPUs Comet: traditional compute and GPUs XStream: GPU cluster Department of Energy NERSC, Argonne Leadership Computing Facility (ALCF), Oak Ridge Leadership Computing Facility (OLCF) INCITE Due in June | Application & Info ALCC Due in June | Application & Info ANL Director\u2019s Discretionary Rolling submission | Application & Info 3-6 month duration. Expectation is that you are using it to gather data for ALCC or INCITE proposal OLCF Director\u2019s Discretionary Rolling submission | Application & Info NCSA: Blue Waters PRAC Due in November | Application & Info Blue Water\u2019s Innovation Allocations Rolling submission | Application & Info Open Science Grid (OSG) Rolling Submission | Application & Info The OSG facilitates access to distributed high throughput computing for research in the US. The resources accessible through the OSG are contributed by the community, organized by the OSG, and governed by the OSG consortium.","title":"National HPCs"},{"location":"national-hpcs/#national-hpcs","text":"Beyond Yale\u2019s on campus clusters, there are a number of ways for researchers to obtain compute resources (both cycles and storage) at national facilities. Yale researchers may use the Data Management Planning Tool ( DMPtool ) to create, review, and share data management plans that are in accordance with institutional and funder requirements.","title":"National HPCs"},{"location":"national-hpcs/#xsede","text":"Quarterly | Application & Info Startup Allocations are readily available on XSEDE resources (typically approved within 2 weeks) for benchmarking and planning runs. For even lower commitment allocations (e.g. to just explore the resource), YCRC staff members have \"Campus Champions\" allocations on all XSEDE resources that can be shared upon request. Send an email to hpc@yale.edu for access. XSEDE resources include the following. Up to date information is available at xsede.org : Stampede2: traditional compute and Phis Jetstream: Science Gateways Bridges: traditional compute and GPUs Comet: traditional compute and GPUs XStream: GPU cluster","title":"XSEDE"},{"location":"national-hpcs/#department-of-energy","text":"NERSC, Argonne Leadership Computing Facility (ALCF), Oak Ridge Leadership Computing Facility (OLCF)","title":"Department of Energy"},{"location":"national-hpcs/#incite","text":"Due in June | Application & Info","title":"INCITE"},{"location":"national-hpcs/#alcc","text":"Due in June | Application & Info","title":"ALCC"},{"location":"national-hpcs/#anl-directors-discretionary","text":"Rolling submission | Application & Info 3-6 month duration. Expectation is that you are using it to gather data for ALCC or INCITE proposal","title":"ANL Director\u2019s Discretionary"},{"location":"national-hpcs/#olcf-directors-discretionary","text":"Rolling submission | Application & Info","title":"OLCF Director\u2019s Discretionary"},{"location":"national-hpcs/#ncsa-blue-waters","text":"","title":"NCSA: Blue Waters"},{"location":"national-hpcs/#prac","text":"Due in November | Application & Info","title":"PRAC"},{"location":"national-hpcs/#blue-waters-innovation-allocations","text":"Rolling submission | Application & Info","title":"Blue Water\u2019s Innovation Allocations"},{"location":"national-hpcs/#open-science-grid-osg","text":"Rolling Submission | Application & Info The OSG facilitates access to distributed high throughput computing for research in the US. The resources accessible through the OSG are contributed by the community, organized by the OSG, and governed by the OSG consortium.","title":"Open Science Grid (OSG)"},{"location":"online-tutorials/","text":"Online Tutorials Linux/Unix and Command Line Introduction to Linux Note: You can learn more about most commands you come across by typing \"man [command]\" into the terminal. Practical Introduction to Linux , ( Video ) *Recommended Unix for Beginners Interactive command-line bootcamp Introduction to Bash awk (text extraction/parsing) Examples of awk command usage Full awk tutorial grep Grep is useful for searching command line output for a certain phrase or regular expression Basic Usage In-depth guide sed Sed (Stream EDitor) is a useful tool for making substitutions in a text file. The syntax sed uses for substitutions is common in Linux (for example, the same syntax is used in the VIM text editor) Basic Usage Brief guide to sed In-depth guide to sed SSH (connecting to the clusters) Connecting to the Yale clusters Transfer files to/from the cluster Basics of SSH Bashrc Basics Bashrc vs bash_profile How create and extract a tar or tar.gz archive The command you are looking for is likely: tar xvf archive.tar , but see the following for details. Creating and extracting from a tar file How to Compile and Link Libraries How Programs Work on Linux Dual Booting Windows and Linux Dual Boot Linux Mint and Windows Dual Boot Ubuntu and Windows Python Intro to Python Fantastic resource for anyone interested in Python multiprocessing Quick Tutorial: Python Multiprocessing Parallel Programming with Python mpi4py Documentation for mpi4py Yale Bootcamp and course material R Intro to R Brief Intro to R Thorough intro to R foreach Using the foreach package, by Steve Weston foreach + dompi Introduction to doMPI","title":"Online Tutorials"},{"location":"online-tutorials/#online-tutorials","text":"","title":"Online Tutorials"},{"location":"online-tutorials/#linuxunix-and-command-line","text":"","title":"Linux/Unix and Command Line"},{"location":"online-tutorials/#introduction-to-linux","text":"Note: You can learn more about most commands you come across by typing \"man [command]\" into the terminal. Practical Introduction to Linux , ( Video ) *Recommended Unix for Beginners Interactive command-line bootcamp Introduction to Bash","title":"Introduction to Linux"},{"location":"online-tutorials/#awk-text-extractionparsing","text":"Examples of awk command usage Full awk tutorial","title":"awk (text extraction/parsing)"},{"location":"online-tutorials/#grep","text":"Grep is useful for searching command line output for a certain phrase or regular expression Basic Usage In-depth guide","title":"grep"},{"location":"online-tutorials/#sed","text":"Sed (Stream EDitor) is a useful tool for making substitutions in a text file. The syntax sed uses for substitutions is common in Linux (for example, the same syntax is used in the VIM text editor) Basic Usage Brief guide to sed In-depth guide to sed","title":"sed"},{"location":"online-tutorials/#ssh-connecting-to-the-clusters","text":"Connecting to the Yale clusters Transfer files to/from the cluster Basics of SSH","title":"SSH (connecting to the clusters)"},{"location":"online-tutorials/#bashrc","text":"Basics Bashrc vs bash_profile","title":"Bashrc"},{"location":"online-tutorials/#how-create-and-extract-a-tar-or-targz-archive","text":"The command you are looking for is likely: tar xvf archive.tar , but see the following for details. Creating and extracting from a tar file","title":"How create and extract a tar or tar.gz archive"},{"location":"online-tutorials/#how-to-compile-and-link-libraries","text":"How Programs Work on Linux","title":"How to Compile and Link Libraries"},{"location":"online-tutorials/#dual-booting-windows-and-linux","text":"Dual Boot Linux Mint and Windows Dual Boot Ubuntu and Windows","title":"Dual Booting Windows and Linux"},{"location":"online-tutorials/#python","text":"","title":"Python"},{"location":"online-tutorials/#intro-to-python","text":"Fantastic resource for anyone interested in Python","title":"Intro to Python"},{"location":"online-tutorials/#multiprocessing","text":"Quick Tutorial: Python Multiprocessing Parallel Programming with Python","title":"multiprocessing"},{"location":"online-tutorials/#mpi4py","text":"Documentation for mpi4py Yale Bootcamp and course material","title":"mpi4py"},{"location":"online-tutorials/#r","text":"","title":"R"},{"location":"online-tutorials/#intro-to-r","text":"Brief Intro to R Thorough intro to R","title":"Intro to R"},{"location":"online-tutorials/#foreach","text":"Using the foreach package, by Steve Weston","title":"foreach"},{"location":"online-tutorials/#foreach-dompi","text":"Introduction to doMPI","title":"foreach + dompi"},{"location":"user-group/","text":"YCRC User Group The YCRC User Group is a community of researchers at Yale who utilize computing resources and technology to enable their research. The User Group holds monthly meetings, each on a distinct topic of interest to the community. During meeting there are opportunities for members of the research community to teach and learn from their peers in a mix of panel discussions, presentations, lightning talks, working groups and informal discussions. User Group meetings are held monthly in the YCRC Auditorium on the first Wednesday of each month at 4:00 pm. You can join the User Group mailing list and forum at https://groups.io/g/ycrcusergroup .","title":"YCRC User Group"},{"location":"user-group/#ycrc-user-group","text":"The YCRC User Group is a community of researchers at Yale who utilize computing resources and technology to enable their research. The User Group holds monthly meetings, each on a distinct topic of interest to the community. During meeting there are opportunities for members of the research community to teach and learn from their peers in a mix of panel discussions, presentations, lightning talks, working groups and informal discussions. User Group meetings are held monthly in the YCRC Auditorium on the first Wednesday of each month at 4:00 pm. You can join the User Group mailing list and forum at https://groups.io/g/ycrcusergroup .","title":"YCRC User Group"},{"location":"clusters-at-yale/","text":"Getting Started HPC Clusters Broadly speaking, a compute cluster is a collection of networked computers which we call nodes. Our clusters are only accessible to researchers remotely; your gateway to the cluster is the login node . From this node, you will be able to view your files and dispatch jobs to one or several other nodes across the cluster configured for computation, called compute nodes . The tool we use to submit these jobs is called a job scheduler . All compute nodes on a cluster mount a shared filesystem ; a file server or set of servers that keeps track of all files on a large array of disks, so that you can access and edit your data from any compute node. Detailed information about each of our clusters is available here . Request an Account The first step in gaining access to our clusters is to request an account. There are several HPC clusters available at Yale. There is no charge for using these clusters. To understand which cluster is appropriate for you and to request an account, visit the account request page . Log in All of Yale's clusters are accessed via a protocol called secure shell (ssh). Once you have an account, look at our SSH instructions to log on to the system. If you want to access the clusters from outside Yale, you must use the Yale VPN. Schedule a Job On our clusters, you control your jobs using a job scheduling system called Slurm that dedicates and manages compute resources for you. Schedulers are usually used in one of two ways. For testing and small jobs you may want to run a job interactively . This way you can directly interact with the compute node(s) in real time to make sure your jobs will behave as expected. The other way, which is the preferred way for long-running jobs, involves writing your job commands in a script and submitting that to the job scheduler. Please see our Slurm documentation or attend the HPC bootcamp for more details. Linux A basically familiarity with Linux commands is required for interacting with the clusters. We periodically run an Intro to Linux Bootcamp to get you started. There are also many excellent beginner tutorials available for free online, including the following: Unix Tutorial for Beginners Interactive Command Line Bootcamp Move Your Files You will likely find it necessary to copy files between your local machines and the clusters. Just as with logging in, there are different ways to do this, depending on your local operating system. See the documentation on transferring data for more information. Use Software To best serve the diverse needs of all our researhcers, we use a module system to manage the most commonly used software. This allows you to swap between different applications and versions of those applications with relative ease and focus on getting your work done. See the Modules documentation in our User Guide for more information. We also provide assistance for installing less commonly used packages. See our Applications & Software documentation for more details. Rules of the Road Before you begin using the cluster, here are some important things to remember: Do not run jobs or do real work on the login node. Always allocate a compute node and run programs there. Never give your password or ssh key to anyone else. Do not store any protected or regulated data on the cluster (e.g. PHI data) Use of the clusters is also governed by our official guidelines . Hands on Training We offer several courses that will assist you with your work on our clusters. They range from orientation for absolute beginners to advanced topics on application-specific optimization. Please peruse our catalog of training to see what is available. Get Additional Help If you have additional questions/comments, please contact the YCRC team . If possible, please include the following information: Your netid Cluster Queue/partition name Job ID(s) Error messages Command used to submit the job(s) Path(s) to scripts called by the submission command Path(s) to output files from your jobs","title":"Getting Started"},{"location":"clusters-at-yale/#getting-started","text":"","title":"Getting Started"},{"location":"clusters-at-yale/#hpc-clusters","text":"Broadly speaking, a compute cluster is a collection of networked computers which we call nodes. Our clusters are only accessible to researchers remotely; your gateway to the cluster is the login node . From this node, you will be able to view your files and dispatch jobs to one or several other nodes across the cluster configured for computation, called compute nodes . The tool we use to submit these jobs is called a job scheduler . All compute nodes on a cluster mount a shared filesystem ; a file server or set of servers that keeps track of all files on a large array of disks, so that you can access and edit your data from any compute node. Detailed information about each of our clusters is available here .","title":"HPC Clusters"},{"location":"clusters-at-yale/#request-an-account","text":"The first step in gaining access to our clusters is to request an account. There are several HPC clusters available at Yale. There is no charge for using these clusters. To understand which cluster is appropriate for you and to request an account, visit the account request page .","title":"Request an Account"},{"location":"clusters-at-yale/#log-in","text":"All of Yale's clusters are accessed via a protocol called secure shell (ssh). Once you have an account, look at our SSH instructions to log on to the system. If you want to access the clusters from outside Yale, you must use the Yale VPN.","title":"Log in"},{"location":"clusters-at-yale/#schedule-a-job","text":"On our clusters, you control your jobs using a job scheduling system called Slurm that dedicates and manages compute resources for you. Schedulers are usually used in one of two ways. For testing and small jobs you may want to run a job interactively . This way you can directly interact with the compute node(s) in real time to make sure your jobs will behave as expected. The other way, which is the preferred way for long-running jobs, involves writing your job commands in a script and submitting that to the job scheduler. Please see our Slurm documentation or attend the HPC bootcamp for more details.","title":"Schedule a Job"},{"location":"clusters-at-yale/#linux","text":"A basically familiarity with Linux commands is required for interacting with the clusters. We periodically run an Intro to Linux Bootcamp to get you started. There are also many excellent beginner tutorials available for free online, including the following: Unix Tutorial for Beginners Interactive Command Line Bootcamp","title":"Linux"},{"location":"clusters-at-yale/#move-your-files","text":"You will likely find it necessary to copy files between your local machines and the clusters. Just as with logging in, there are different ways to do this, depending on your local operating system. See the documentation on transferring data for more information.","title":"Move Your Files"},{"location":"clusters-at-yale/#use-software","text":"To best serve the diverse needs of all our researhcers, we use a module system to manage the most commonly used software. This allows you to swap between different applications and versions of those applications with relative ease and focus on getting your work done. See the Modules documentation in our User Guide for more information. We also provide assistance for installing less commonly used packages. See our Applications & Software documentation for more details.","title":"Use Software"},{"location":"clusters-at-yale/#rules-of-the-road","text":"Before you begin using the cluster, here are some important things to remember: Do not run jobs or do real work on the login node. Always allocate a compute node and run programs there. Never give your password or ssh key to anyone else. Do not store any protected or regulated data on the cluster (e.g. PHI data) Use of the clusters is also governed by our official guidelines .","title":"Rules of the Road"},{"location":"clusters-at-yale/#hands-on-training","text":"We offer several courses that will assist you with your work on our clusters. They range from orientation for absolute beginners to advanced topics on application-specific optimization. Please peruse our catalog of training to see what is available.","title":"Hands on Training"},{"location":"clusters-at-yale/#get-additional-help","text":"If you have additional questions/comments, please contact the YCRC team . If possible, please include the following information: Your netid Cluster Queue/partition name Job ID(s) Error messages Command used to submit the job(s) Path(s) to scripts called by the submission command Path(s) to output files from your jobs","title":"Get Additional Help"},{"location":"clusters-at-yale/troubleshoot/","text":"Troubleshoot Login If you are having trouble logging in, please check the following: Check the status page to make sure that your cluster is online: System Status . Accounts are only created for you on the clusters as you requested them. To get access to additional clusters, submit another account request . Verify that you are attempting to ssh to the correct hostname. Please see the cluster page for a list of login addresses. Verify that your ssh keys are setup correctly. Make sure your public key is uploaded to the ssh key uploader . If you are on Windows, make sure you have pointed MobaXterm to your private ssh key and if you are on macOS or Linux, your private key needs to be in ${HOME}/.ssh . We use ssh keys to authenticate logins to the clusters, and not NetID passwords. If you are asked for a \"passphrase\" upon logging in, this is the ssh key passphrase you configured when first creating your key. If you have forgotten your passphrase, you will need to create and upload a new ssh key pair (see our SSH Guide ). Make sure you are accessing the cluster from either Yale's campus networks (ethernet or YaleSecure for wireless) or Yale's VPN if you are off-campus. The home directory should only be write-able by your NetID. If you recently modified the permissions to your home directory, contact us at hpc@yale.edu and we can fix the permissions for you. If you get an error like \"could not resolve hostname\" make sure that you are using the Yale DNS servers (130.132.1.9,10,11). External DNS servers do not list our clusters. If you are using Ruddle, they require Duo MFA on your smartphone. If Duo is not working for you, try testing it on this ITS site: http://access.yale.edu/mfa . If that doesn't work, contact the Yale Help Desk. If all of the above check out ok, please contact us and we will investigate further. Please include your netid and the cluster you are attempting to connect to in your email. \"REMOTE HOST IDENTIFICATION HAS CHANGED!\" Warning If you are seeing the following error: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! .... Offending key in /home/user/.ssh/known_hosts:34 ... This error means that the host keys on the cluster have changed. This may be the result of system upgrades on the cluster. It could also mean someone is trying to intercept your ssh session. Please contact us if you receive this error. If the host keys have indeed changed on the server you are connecting to, you can edit ~/.ssh/known_hosts and remove the offending line. In the example above, line 34 in ~/.ssh/known_hosts would have to be deleted before you re-connect.","title":"Troubleshoot Login"},{"location":"clusters-at-yale/troubleshoot/#troubleshoot-login","text":"If you are having trouble logging in, please check the following: Check the status page to make sure that your cluster is online: System Status . Accounts are only created for you on the clusters as you requested them. To get access to additional clusters, submit another account request . Verify that you are attempting to ssh to the correct hostname. Please see the cluster page for a list of login addresses. Verify that your ssh keys are setup correctly. Make sure your public key is uploaded to the ssh key uploader . If you are on Windows, make sure you have pointed MobaXterm to your private ssh key and if you are on macOS or Linux, your private key needs to be in ${HOME}/.ssh . We use ssh keys to authenticate logins to the clusters, and not NetID passwords. If you are asked for a \"passphrase\" upon logging in, this is the ssh key passphrase you configured when first creating your key. If you have forgotten your passphrase, you will need to create and upload a new ssh key pair (see our SSH Guide ). Make sure you are accessing the cluster from either Yale's campus networks (ethernet or YaleSecure for wireless) or Yale's VPN if you are off-campus. The home directory should only be write-able by your NetID. If you recently modified the permissions to your home directory, contact us at hpc@yale.edu and we can fix the permissions for you. If you get an error like \"could not resolve hostname\" make sure that you are using the Yale DNS servers (130.132.1.9,10,11). External DNS servers do not list our clusters. If you are using Ruddle, they require Duo MFA on your smartphone. If Duo is not working for you, try testing it on this ITS site: http://access.yale.edu/mfa . If that doesn't work, contact the Yale Help Desk. If all of the above check out ok, please contact us and we will investigate further. Please include your netid and the cluster you are attempting to connect to in your email.","title":"Troubleshoot Login"},{"location":"clusters-at-yale/troubleshoot/#remote-host-identification-has-changed-warning","text":"If you are seeing the following error: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! .... Offending key in /home/user/.ssh/known_hosts:34 ... This error means that the host keys on the cluster have changed. This may be the result of system upgrades on the cluster. It could also mean someone is trying to intercept your ssh session. Please contact us if you receive this error. If the host keys have indeed changed on the server you are connecting to, you can edit ~/.ssh/known_hosts and remove the offending line. In the example above, line 34 in ~/.ssh/known_hosts would have to be deleted before you re-connect.","title":"\"REMOTE HOST IDENTIFICATION HAS CHANGED!\" Warning"},{"location":"clusters-at-yale/access/","text":"Log on to the Clusters We use ssh with ssh key pairs to log in to the clusters, e.g. ssh netid@clustername.hpc.yale.edu If you have a public key and are familiar with key pairs, upload your ssh key below. Please allow up to ten minutes for the key to propagate before logging in. Upload your SSH key here (only accessible on campus or through the Yale VPN) Troubleshoot Login For additional information, see below. Off Campus Access to the Clusters Graphical Interfaces: X11 Forwarding and VNC Connect from macOS and Linux Generate Your Key Pair A key pair is required to connect to a cluster. A key pair consists of a private key and a public key. The private key remains on your desktop/laptop and should never be shared with anyone. Your public key is installed in ~/.ssh/authorized_keys on the cluster. In order for someone to access your account on the cluster, they must possess your private key and its associated passphrase. To generate a new key pair, first open a terminal/xterm session. If you are on macOS, open Applications -> Utilities -> Terminal. Generate your public and private ssh keys. Type the following into the terminal window: ssh-keygen Your terminal should respond: Generating public/private rsa key pair. Enter file in which to save the key (/home/#yourusername#/.ssh/id_rsa): Press Enter to accept the default value. Your terminal should respond: Enter passphrase (empty for no passphrase): Choose a secure passphrase. Your passphrase will prevent access to your account in the event your private key is stolen. The response will be: Enter same passphrase again: Enter the passphrase again. The key pair is generated and written to a directory called .ssh in your home directory. The public key is stored in ~/.ssh/id_rsa.pub . If you forget your passphrase, it cannot be recovered. Instead, you will need to generate and upload a new ssh key pair. Next, install your public ssh key on the cluster. Run the following command in a terminal: cat ~/.ssh/id_rsa.pub Copy and paste the output to Yale HPC ssh key installer (only accessible on campus or through the Yale VPN). It may take up to 15 minutes after uploading for your key to be pushed to the clusters. Note that you should never send the private key file to anyone! Connect Once your public key has been installed, you may use ssh in a terminal to access the appropriate cluster. You need to know 2 things to log into a cluster. The hostname of the cluster login node Your netid You can find the hostnames of the cluster login nodes here. Open a terminal window and connect to the login node using the syntax: ssh netid@login-node For example, if your netid is ra359 and you wish to log into the Grace cluster: ssh ra359@grace.hpc.yale.edu Check out our Sample Linux/Mac SSH Configuration for tips on maintaining connections and adding tab complete to your ssh commands. Mac: Store Passphrase and Use SSH Agent Forwarding By default, macOS won't always remember your ssh key passphase and keep your ssh key in the agent for SSH agent forwarding. In order to not repeatedly enter your passphrase and enable agent forwarding, enter the following command on your local machine (just once): ssh-add -K ~/.ssh/[your-private-key] and add the following to your ~/.ssh/config file (create this file if it doesn't exist). Host * UseKeychain yes AddKeystoAgent yes ForwardAgent yes Connect from Windows We recommend using MobaXterm to connect to the clusters. You can download, extract & install MobaXterm from this page . We recommend using the \"Installer Edition\", but make sure to extract the zip file before running the installer. Generate Your Key Pair To get up and running, generate an ssh keypair if you haven't already: From the top menu choose Tools -> MobaKeyGen (SSH key generator). Leave all defaults and click the generate button. Wiggle your mouse. Save your public key as id_rsa.pub. Save your private key as id_rsa.ppk (this one is secret, don't give it to other people ). Copy the text of your public key and paste it into the text box after you log into the SSH key uploader . Your key will be synced out to the clusters in a few minutes. Connect To make a new connection to one of the clusters From the top menu select Sessions -> New Session. Click the SSH icon in the top left. Enter the cluster login node address (e.g. farnam.hpc.yale.edu) as the Remote Host. Check \"Specify Username\" and Enter your netID as the the username. Click the \"Advanced SSH Settings\" tab and check the \"Use private key box\", then click the file icon / magnifying glass to choose where you saved your private key (id_rsa.ppk). Click OK. In the future, your session should be saved in the sessions bar on the left in the main window.","title":"Log on to the Clusters"},{"location":"clusters-at-yale/access/#log-on-to-the-clusters","text":"We use ssh with ssh key pairs to log in to the clusters, e.g. ssh netid@clustername.hpc.yale.edu If you have a public key and are familiar with key pairs, upload your ssh key below. Please allow up to ten minutes for the key to propagate before logging in. Upload your SSH key here (only accessible on campus or through the Yale VPN) Troubleshoot Login For additional information, see below. Off Campus Access to the Clusters Graphical Interfaces: X11 Forwarding and VNC","title":"Log on to the Clusters"},{"location":"clusters-at-yale/access/#connect-from-macos-and-linux","text":"","title":"Connect from macOS and Linux"},{"location":"clusters-at-yale/access/#generate-your-key-pair","text":"A key pair is required to connect to a cluster. A key pair consists of a private key and a public key. The private key remains on your desktop/laptop and should never be shared with anyone. Your public key is installed in ~/.ssh/authorized_keys on the cluster. In order for someone to access your account on the cluster, they must possess your private key and its associated passphrase. To generate a new key pair, first open a terminal/xterm session. If you are on macOS, open Applications -> Utilities -> Terminal. Generate your public and private ssh keys. Type the following into the terminal window: ssh-keygen Your terminal should respond: Generating public/private rsa key pair. Enter file in which to save the key (/home/#yourusername#/.ssh/id_rsa): Press Enter to accept the default value. Your terminal should respond: Enter passphrase (empty for no passphrase): Choose a secure passphrase. Your passphrase will prevent access to your account in the event your private key is stolen. The response will be: Enter same passphrase again: Enter the passphrase again. The key pair is generated and written to a directory called .ssh in your home directory. The public key is stored in ~/.ssh/id_rsa.pub . If you forget your passphrase, it cannot be recovered. Instead, you will need to generate and upload a new ssh key pair. Next, install your public ssh key on the cluster. Run the following command in a terminal: cat ~/.ssh/id_rsa.pub Copy and paste the output to Yale HPC ssh key installer (only accessible on campus or through the Yale VPN). It may take up to 15 minutes after uploading for your key to be pushed to the clusters. Note that you should never send the private key file to anyone!","title":"Generate Your Key Pair"},{"location":"clusters-at-yale/access/#connect","text":"Once your public key has been installed, you may use ssh in a terminal to access the appropriate cluster. You need to know 2 things to log into a cluster. The hostname of the cluster login node Your netid You can find the hostnames of the cluster login nodes here. Open a terminal window and connect to the login node using the syntax: ssh netid@login-node For example, if your netid is ra359 and you wish to log into the Grace cluster: ssh ra359@grace.hpc.yale.edu Check out our Sample Linux/Mac SSH Configuration for tips on maintaining connections and adding tab complete to your ssh commands.","title":"Connect"},{"location":"clusters-at-yale/access/#mac-store-passphrase-and-use-ssh-agent-forwarding","text":"By default, macOS won't always remember your ssh key passphase and keep your ssh key in the agent for SSH agent forwarding. In order to not repeatedly enter your passphrase and enable agent forwarding, enter the following command on your local machine (just once): ssh-add -K ~/.ssh/[your-private-key] and add the following to your ~/.ssh/config file (create this file if it doesn't exist). Host * UseKeychain yes AddKeystoAgent yes ForwardAgent yes","title":"Mac: Store Passphrase and Use SSH Agent Forwarding"},{"location":"clusters-at-yale/access/#connect-from-windows","text":"We recommend using MobaXterm to connect to the clusters. You can download, extract & install MobaXterm from this page . We recommend using the \"Installer Edition\", but make sure to extract the zip file before running the installer.","title":"Connect from Windows"},{"location":"clusters-at-yale/access/#generate-your-key-pair_1","text":"To get up and running, generate an ssh keypair if you haven't already: From the top menu choose Tools -> MobaKeyGen (SSH key generator). Leave all defaults and click the generate button. Wiggle your mouse. Save your public key as id_rsa.pub. Save your private key as id_rsa.ppk (this one is secret, don't give it to other people ). Copy the text of your public key and paste it into the text box after you log into the SSH key uploader . Your key will be synced out to the clusters in a few minutes.","title":"Generate Your Key Pair"},{"location":"clusters-at-yale/access/#connect_1","text":"To make a new connection to one of the clusters From the top menu select Sessions -> New Session. Click the SSH icon in the top left. Enter the cluster login node address (e.g. farnam.hpc.yale.edu) as the Remote Host. Check \"Specify Username\" and Enter your netID as the the username. Click the \"Advanced SSH Settings\" tab and check the \"Use private key box\", then click the file icon / magnifying glass to choose where you saved your private key (id_rsa.ppk). Click OK. In the future, your session should be saved in the sessions bar on the left in the main window.","title":"Connect"},{"location":"clusters-at-yale/access/mfa/","text":"Multi-factor Authentication To improve security, access to Ruddle requires both a public key and multi-factor authentication (MFA). We use the same MFA (Duo) as is used elsewhere at Yale. To get set up with Duo, see these instructions. You will need upload your ssh public key to our site . For more info on how to use ssh, please see SSH instructions . Tip Setting up a config file lets you re-uses your authenticated sessions. Once you've set up Duo and your key is registered, you can finally log in. Use ssh to connect to your cluster of choice, and you will be prompted for a passcode or to select a notification option. We recommend choosing Duo Push (option 1). If you chose this option you should receive a notification on your phone. Once approved, you should be allowed to continue to log in. Note You can set up more than one phone for Duo. For example, you can set up your smartphone plus your office landline. That way, if you forget or lose your phone, you can still authenticate. For instructions on how to add additional phones go here . File Transfer Clients and Duo MFA Some file transfer clients attempt new and sometimes multiple concurrent connections to transfer files for you. When this happens, you will be asked to Duo authenticate for each connection. Here are some application-specific tips on how to avoid having to authenticate for each new connection: FileZilla File->SiteManager New Site In host (new site will be highlighted, change to your cluster name) In the general tab on the right side, put: Host: yourcluster.hpc.yale.edu Change protocol to SFTP set user to your netid set login type to \"Key file\" browse to and select your putty key file set login type to \"Interactive\" Go to transfer settings tab check limit number of simultaneous connections Make sure maximum is 1 click ok Now to connect, file->sitemanager select the site you created click connect Other Client Recommendations Cyberduck (Windows/macOS) Troubleshoot MFA If you are having problems initially registering Duo, please contact the Yale ITS Helpdesk . If you have successfully used MFA connect to a cluster before, but cannot now, first please check the following: Verify that your ssh client is using the correct login node Verify you are attempting to connect from a Yale machine or via the proper VPN If all of this is true, please contact us at hpc@yale.edu . Include the following information (and anything else you think is helpful): Your netid Have you ever successfully used ssh and Duo to connect to a cluster? How long have you been having problems? Where are you trying to connect from? (fully qualified hostname/IP, if possible) Are you using a VPN? What is the error message you see?","title":"Multi-factor Authentication"},{"location":"clusters-at-yale/access/mfa/#multi-factor-authentication","text":"To improve security, access to Ruddle requires both a public key and multi-factor authentication (MFA). We use the same MFA (Duo) as is used elsewhere at Yale. To get set up with Duo, see these instructions. You will need upload your ssh public key to our site . For more info on how to use ssh, please see SSH instructions . Tip Setting up a config file lets you re-uses your authenticated sessions. Once you've set up Duo and your key is registered, you can finally log in. Use ssh to connect to your cluster of choice, and you will be prompted for a passcode or to select a notification option. We recommend choosing Duo Push (option 1). If you chose this option you should receive a notification on your phone. Once approved, you should be allowed to continue to log in. Note You can set up more than one phone for Duo. For example, you can set up your smartphone plus your office landline. That way, if you forget or lose your phone, you can still authenticate. For instructions on how to add additional phones go here .","title":"Multi-factor Authentication"},{"location":"clusters-at-yale/access/mfa/#file-transfer-clients-and-duo-mfa","text":"Some file transfer clients attempt new and sometimes multiple concurrent connections to transfer files for you. When this happens, you will be asked to Duo authenticate for each connection. Here are some application-specific tips on how to avoid having to authenticate for each new connection:","title":"File Transfer Clients and Duo MFA"},{"location":"clusters-at-yale/access/mfa/#filezilla","text":"File->SiteManager New Site In host (new site will be highlighted, change to your cluster name) In the general tab on the right side, put: Host: yourcluster.hpc.yale.edu Change protocol to SFTP set user to your netid set login type to \"Key file\" browse to and select your putty key file set login type to \"Interactive\" Go to transfer settings tab check limit number of simultaneous connections Make sure maximum is 1 click ok Now to connect, file->sitemanager select the site you created click connect","title":"FileZilla"},{"location":"clusters-at-yale/access/mfa/#other-client-recommendations","text":"Cyberduck (Windows/macOS)","title":"Other Client Recommendations"},{"location":"clusters-at-yale/access/mfa/#troubleshoot-mfa","text":"If you are having problems initially registering Duo, please contact the Yale ITS Helpdesk . If you have successfully used MFA connect to a cluster before, but cannot now, first please check the following: Verify that your ssh client is using the correct login node Verify you are attempting to connect from a Yale machine or via the proper VPN If all of this is true, please contact us at hpc@yale.edu . Include the following information (and anything else you think is helpful): Your netid Have you ever successfully used ssh and Duo to connect to a cluster? How long have you been having problems? Where are you trying to connect from? (fully qualified hostname/IP, if possible) Are you using a VPN? What is the error message you see?","title":"Troubleshoot MFA"},{"location":"clusters-at-yale/access/sample-config/","text":"Example .ssh/config The following configuration is an example ssh client configuration file (linux and mac only) specific to our clusters. It allows you to use tab completion of the clusters, sans the .hpc.yale.edu suffixes (i.e. ssh farnam works). It will also allow you to re-use and multiplex authenticated sessions. Practically this means clusters that require Duo MFA will not need you to re-authenticate, as you use the same ssh connection to host multiple sessions. If you attempt to close your first connection with others running, it will wait until all others are closed. For this to work, you need to create the directory ~/.ssh/tmp : mkdir -p ~/.ssh/tmp Then take the text below and replace NETID with your Yale netid. Lines that begin with # will be ignored. Save this file as ~/.ssh/config (or add it to the file that already exists). #re-use your connections with multi-factor authentication (Ruddle) ControlMaster auto ControlPath ~/.ssh/tmp/%h_%p_%r # If you use a ssh key that is named something other than id_rsa, # specify your private key like this: # IdentityFile ~/.ssh/other_key_rsa #uncomment the ForwardX11 options to enable X11 Forwarding by default (no -Y necessary) Host farnam HostName farnam.hpc.yale.edu User NETID # ForwardX11 yes Host ruddle HostName ruddle.hpc.yale.edu User NETID # ForwardX11 yes Host grace HostName grace.hpc.yale.edu User NETID # ForwardX11 yes Host omega Hostname omega.hpc.yale.edu User NETID # ForwardX11 yes For more info, run: man ssh_config","title":"Example .ssh/config"},{"location":"clusters-at-yale/access/sample-config/#example-sshconfig","text":"The following configuration is an example ssh client configuration file (linux and mac only) specific to our clusters. It allows you to use tab completion of the clusters, sans the .hpc.yale.edu suffixes (i.e. ssh farnam works). It will also allow you to re-use and multiplex authenticated sessions. Practically this means clusters that require Duo MFA will not need you to re-authenticate, as you use the same ssh connection to host multiple sessions. If you attempt to close your first connection with others running, it will wait until all others are closed. For this to work, you need to create the directory ~/.ssh/tmp : mkdir -p ~/.ssh/tmp Then take the text below and replace NETID with your Yale netid. Lines that begin with # will be ignored. Save this file as ~/.ssh/config (or add it to the file that already exists). #re-use your connections with multi-factor authentication (Ruddle) ControlMaster auto ControlPath ~/.ssh/tmp/%h_%p_%r # If you use a ssh key that is named something other than id_rsa, # specify your private key like this: # IdentityFile ~/.ssh/other_key_rsa #uncomment the ForwardX11 options to enable X11 Forwarding by default (no -Y necessary) Host farnam HostName farnam.hpc.yale.edu User NETID # ForwardX11 yes Host ruddle HostName ruddle.hpc.yale.edu User NETID # ForwardX11 yes Host grace HostName grace.hpc.yale.edu User NETID # ForwardX11 yes Host omega Hostname omega.hpc.yale.edu User NETID # ForwardX11 yes For more info, run: man ssh_config","title":"Example .ssh/config"},{"location":"clusters-at-yale/access/vnc/","text":"VNC As an alternative to X11 Forwarding, using VNC to access the cluster is another way to run graphically intensive applications. Setup vncserver on a Cluster Connect to the cluster with X11 forwarding enabled. If on Linux or Mac, ssh -Y netid@cluster , or if on Windows, follow our X11 forwarding guide . Start an interactive job on cluster with the --x11 flag (see Slurm for more information). For this description, we\u2019ll assume you were given node c04n03: srun --pty --x11 -p interactive bash On that node, run the VNCserver. You\u2019ll see something like: c04n03$ vncserver New 'c31n02.grace.hpc.yale.internal:1 (kln26)' desktop is c31n02.grace.hpc.yale.internal:1 Creating default startup script /home/fas/hpcprog/kln26/.vnc/xstartup Starting applications specified in /home/fas/hpcprog/kln26/.vnc/xstartup Log file is /home/fas/hpcprog/kln26/.vnc/c31n02.grace.hpc.yale.internal:1.log The :1 means that your DISPLAY is :1. You\u2019ll need that later, so note it. The first time you run \"vncserver\", you\u2019ll also be asked to select a password for allowing access. On MacOS, if connecting with TurboVNC throws a security exception such as \"javax.net.ssl.SSLHandshakeException\", try adding the SecurityTypes option when starting vncserver on the cluster: vncserver -SecurityTypes VNC,OTP,UnixLogin,None Connect from your local machine (laptop/desktop) macOs/Linux From a shell on your local machine, run the following ssh command: ssh -Y -L7777:c04n03:5901 YourNetID@cluster_login_node This will set up a tunnel from your local port 7777 to port 5901 on c04n03. You will need to customize this command to your situation. The 5901 is for display :1. In general, you should put 5900+DISPLAY. The 7777 is arbitrary; any number above 3000 will likely work. You\u2019ll need the number you chose for the next step. On your local machine, start the vncviewer application. Depending on your local operating system, you may need to install this. We recommend TurboVNC for Mac. When you start the viewer, you\u2019ll need to tell it which host and port to attach to. You want to specify the local end of the tunnel. In the above case, that would be localhost::7777. Exactly how you specify this will depend a bit on which viewer you use. E.g: vncviewer localhost::7777 You should be prompted for the password you set when you started the server. Now you are in a GUI environment and can run IGV or any other rich GUI application. /home/bioinfo/software/IGV/IGV_2.2.0/igv.sh Windows In MobaXterm, create a new Session (available in the menu bar) and then select the VNC session. To fill out the VNC Session setup, click the \"Network settings\" tab and check the box for \"Connect through SSH gateway (jump host). Then fill out the boxes as follows: Remote hostname or IP Address: name of the node running your VNC server (e.g. c01n01) Port: 5900 + the DISPLAY number from above (e.g. 5901 for DISPLAY = 1 ) Gateway SSH server: ssh address of the cluster (e.g. grace.hpc.yale.edu) Port: 22 (should be default) User: netid Use private key: check this box and click to point to your private key file you use to connect to the cluster When you are done, click OK. If promoted for a password for \"localhost\", provide the vncserver password you specified in the previous step. If the VNC server looks very pixelated and your mouse movements seem laggy, try clicking the \"Toggle scaling\" button at the top of the VNC window. Example Configuration: Clean Up When you are all finished, you can kill the vncserver by doing this in the same shell you used to start it (replace :1 by your display number): vncserver -kill :1","title":"VNC"},{"location":"clusters-at-yale/access/vnc/#vnc","text":"As an alternative to X11 Forwarding, using VNC to access the cluster is another way to run graphically intensive applications.","title":"VNC"},{"location":"clusters-at-yale/access/vnc/#setup-vncserver-on-a-cluster","text":"Connect to the cluster with X11 forwarding enabled. If on Linux or Mac, ssh -Y netid@cluster , or if on Windows, follow our X11 forwarding guide . Start an interactive job on cluster with the --x11 flag (see Slurm for more information). For this description, we\u2019ll assume you were given node c04n03: srun --pty --x11 -p interactive bash On that node, run the VNCserver. You\u2019ll see something like: c04n03$ vncserver New 'c31n02.grace.hpc.yale.internal:1 (kln26)' desktop is c31n02.grace.hpc.yale.internal:1 Creating default startup script /home/fas/hpcprog/kln26/.vnc/xstartup Starting applications specified in /home/fas/hpcprog/kln26/.vnc/xstartup Log file is /home/fas/hpcprog/kln26/.vnc/c31n02.grace.hpc.yale.internal:1.log The :1 means that your DISPLAY is :1. You\u2019ll need that later, so note it. The first time you run \"vncserver\", you\u2019ll also be asked to select a password for allowing access. On MacOS, if connecting with TurboVNC throws a security exception such as \"javax.net.ssl.SSLHandshakeException\", try adding the SecurityTypes option when starting vncserver on the cluster: vncserver -SecurityTypes VNC,OTP,UnixLogin,None","title":"Setup vncserver on a Cluster"},{"location":"clusters-at-yale/access/vnc/#connect-from-your-local-machine-laptopdesktop","text":"","title":"Connect from your local machine (laptop/desktop)"},{"location":"clusters-at-yale/access/vnc/#macoslinux","text":"From a shell on your local machine, run the following ssh command: ssh -Y -L7777:c04n03:5901 YourNetID@cluster_login_node This will set up a tunnel from your local port 7777 to port 5901 on c04n03. You will need to customize this command to your situation. The 5901 is for display :1. In general, you should put 5900+DISPLAY. The 7777 is arbitrary; any number above 3000 will likely work. You\u2019ll need the number you chose for the next step. On your local machine, start the vncviewer application. Depending on your local operating system, you may need to install this. We recommend TurboVNC for Mac. When you start the viewer, you\u2019ll need to tell it which host and port to attach to. You want to specify the local end of the tunnel. In the above case, that would be localhost::7777. Exactly how you specify this will depend a bit on which viewer you use. E.g: vncviewer localhost::7777 You should be prompted for the password you set when you started the server. Now you are in a GUI environment and can run IGV or any other rich GUI application. /home/bioinfo/software/IGV/IGV_2.2.0/igv.sh","title":"macOs/Linux"},{"location":"clusters-at-yale/access/vnc/#windows","text":"In MobaXterm, create a new Session (available in the menu bar) and then select the VNC session. To fill out the VNC Session setup, click the \"Network settings\" tab and check the box for \"Connect through SSH gateway (jump host). Then fill out the boxes as follows: Remote hostname or IP Address: name of the node running your VNC server (e.g. c01n01) Port: 5900 + the DISPLAY number from above (e.g. 5901 for DISPLAY = 1 ) Gateway SSH server: ssh address of the cluster (e.g. grace.hpc.yale.edu) Port: 22 (should be default) User: netid Use private key: check this box and click to point to your private key file you use to connect to the cluster When you are done, click OK. If promoted for a password for \"localhost\", provide the vncserver password you specified in the previous step. If the VNC server looks very pixelated and your mouse movements seem laggy, try clicking the \"Toggle scaling\" button at the top of the VNC window. Example Configuration:","title":"Windows"},{"location":"clusters-at-yale/access/vnc/#clean-up","text":"When you are all finished, you can kill the vncserver by doing this in the same shell you used to start it (replace :1 by your display number): vncserver -kill :1","title":"Clean Up"},{"location":"clusters-at-yale/access/vpn/","text":"Access from Off Campus (VPN) Yale's clusters can only be accessed on the Yale network. Therefore, in order to access a cluster from off campus, you will need to first connect to Yale's VPN. We recommend the Cisco AnyConnect VPN Client, which can be downloaded from the ITS Software Library . More information about Yale's VPN can be found on the ITS website . Connect via VPN You will need to connect via the VPN client using the profile \"access.yale.edu\". Multi-factor Authentication (MFA) Authentication for the VPN requires multi-factor authentication via Duo in addition to your normal Yale credentials (netid and password). After you select \"Connect\" in the above dialog box, you will be presented with a new prompt to enter your netid, password and an MFA method. Depending on what you choose you will be prompted to authenticate via a second authentication method. If you type \"push\", simply tap \"Approve\" on your mobile device. If you type \"sms\" you will receive a text message with your passcode. Enter the passcode you received to authenticate. If you type \"phone\", follow the prompts when you are called. Once you successfully authenticate with MFA, you will be connected to the VPN and should be able to log in the clusters via SSH as usual. More information about MFA at Yale can be found on the ITS website .","title":"Access from Off Campus (VPN)"},{"location":"clusters-at-yale/access/vpn/#access-from-off-campus-vpn","text":"Yale's clusters can only be accessed on the Yale network. Therefore, in order to access a cluster from off campus, you will need to first connect to Yale's VPN. We recommend the Cisco AnyConnect VPN Client, which can be downloaded from the ITS Software Library . More information about Yale's VPN can be found on the ITS website .","title":"Access from Off Campus (VPN)"},{"location":"clusters-at-yale/access/vpn/#connect-via-vpn","text":"You will need to connect via the VPN client using the profile \"access.yale.edu\".","title":"Connect via VPN"},{"location":"clusters-at-yale/access/vpn/#multi-factor-authentication-mfa","text":"Authentication for the VPN requires multi-factor authentication via Duo in addition to your normal Yale credentials (netid and password). After you select \"Connect\" in the above dialog box, you will be presented with a new prompt to enter your netid, password and an MFA method. Depending on what you choose you will be prompted to authenticate via a second authentication method. If you type \"push\", simply tap \"Approve\" on your mobile device. If you type \"sms\" you will receive a text message with your passcode. Enter the passcode you received to authenticate. If you type \"phone\", follow the prompts when you are called. Once you successfully authenticate with MFA, you will be connected to the VPN and should be able to log in the clusters via SSH as usual. More information about MFA at Yale can be found on the ITS website .","title":"Multi-factor Authentication (MFA)"},{"location":"clusters-at-yale/access/x11/","text":"Graphical Interfaces (X11) To use a graphical interface on the clusters, your connection needs to be set up for X11 forwarding, which will transmit the graphical window from the cluster back to your local machine. A simple test to see if your setup is working is to run the command xclock . You should see a simple analog clock window pop up. On macOS Download and Install X-Quartz v2.7.8 (newer versions will not work with our clusters). Log out and log back in to reset some variables When using ssh to log in to the clusters, use the -Y option to enable X11 forwarding. Example: ssh -Y rdb9@ruddle.hpc.yale.edu On Windows Our recommended software, MobaXterm , for connecting to the clusters from Windows is configured for X11 forwarding out of the box and should require no additional configuration or software. Quick Test A quick and simple test to check if X11 forwarding is working is to run the command xclock in the session you expect to be forwarding. After a short delay, you should see a window with a simple clock pop up. Submit an X11 enabled Job Once configured, you'll usually want to use X11 forwarding on a compute node to do your work. To allocate a simple interactive session with X11 forwarding: srun --x11 --pty -p interactive bash For more Slurm options, see our Slurm documentation .","title":"Graphical Interfaces (X11)"},{"location":"clusters-at-yale/access/x11/#graphical-interfaces-x11","text":"To use a graphical interface on the clusters, your connection needs to be set up for X11 forwarding, which will transmit the graphical window from the cluster back to your local machine. A simple test to see if your setup is working is to run the command xclock . You should see a simple analog clock window pop up.","title":"Graphical Interfaces (X11)"},{"location":"clusters-at-yale/access/x11/#on-macos","text":"Download and Install X-Quartz v2.7.8 (newer versions will not work with our clusters). Log out and log back in to reset some variables When using ssh to log in to the clusters, use the -Y option to enable X11 forwarding. Example: ssh -Y rdb9@ruddle.hpc.yale.edu","title":"On macOS"},{"location":"clusters-at-yale/access/x11/#on-windows","text":"Our recommended software, MobaXterm , for connecting to the clusters from Windows is configured for X11 forwarding out of the box and should require no additional configuration or software.","title":"On Windows"},{"location":"clusters-at-yale/access/x11/#quick-test","text":"A quick and simple test to check if X11 forwarding is working is to run the command xclock in the session you expect to be forwarding. After a short delay, you should see a window with a simple clock pop up.","title":"Quick Test"},{"location":"clusters-at-yale/access/x11/#submit-an-x11-enabled-job","text":"Once configured, you'll usually want to use X11 forwarding on a compute node to do your work. To allocate a simple interactive session with X11 forwarding: srun --x11 --pty -p interactive bash For more Slurm options, see our Slurm documentation .","title":"Submit an X11 enabled Job"},{"location":"clusters-at-yale/applications/","text":"Overview The YCRC will install and manage commonly used software. These software are available as modules, which allow you to add or remove different combinations and versions of software to your environment as needed. See our module guide for more info. You can run module avail to page through all available software once you log in. You should also feel free to install things for yourself. For Python environments, we recommend using a virtual environment in Anaconda Python . For R enviroments, we recommend using install.packages() to manage your own package needs. For all other software, we encourage users to attempt to install their own software into their directories. Here are instructions for common software procedures. Make Cmake Singularity : create containers and port Docker containers to the clusters We provide guides for certain software packages and languages as well. If you run into issues with your software installations, the YCRC staff can assist at hpc@yale.edu.","title":"Overview"},{"location":"clusters-at-yale/applications/#overview","text":"The YCRC will install and manage commonly used software. These software are available as modules, which allow you to add or remove different combinations and versions of software to your environment as needed. See our module guide for more info. You can run module avail to page through all available software once you log in. You should also feel free to install things for yourself. For Python environments, we recommend using a virtual environment in Anaconda Python . For R enviroments, we recommend using install.packages() to manage your own package needs. For all other software, we encourage users to attempt to install their own software into their directories. Here are instructions for common software procedures. Make Cmake Singularity : create containers and port Docker containers to the clusters We provide guides for certain software packages and languages as well. If you run into issues with your software installations, the YCRC staff can assist at hpc@yale.edu.","title":"Overview"},{"location":"clusters-at-yale/applications/compile/","text":"Build Software How to get software you need up and running on the clusters. caveat emptor We recommend either use existing software modules , Anaconda , Singularity , or pre-compiled software where available. However, there are cases where compiling applications is necessary or desired. This can be because the pre-compiled version isn't readily available/compatible or because compiling applications on the cluster will make an appreciable difference in performance. It is also the case that many R packages are compiled at install time. When compiling applications on the clusters, it is important to consider the ways in which you expect to run the application you are endeavoring to get working. If you want to be able to run jobs calling your application any node on the cluster, you will need to target the oldest hardware available so that newer optimizations are not used that will fail on some nodes. If your application is already quite specialized (e.g. needs GPUs or brand-new CPU instructions), you will want to compile it natively for the subset of compute nodes on which your jobs will run. This decision is often a trade-off between faster individual jobs or jobs that can run on more nodes at once. Each of the cluster pages (see the clusters index for a list) has a \"Compute Node Configurations\" section where nodes are roughly listed from oldest to newest. Illegal Instruction Instructions You may find that software compiled on newer compute nodes will fail with the error Illegal instruction (core dumped) . This includes R/Python libraries that include code that compiles from source. To remedy this issue make sure to always either: Build or install software on the oldest available nodes (right now those are m620 on Farnam and nx360i on Grace ). Require that your jobs running the software in question request similar hardware to their build environment. If your software needs newer instructions using avx2 as a constraint will probably work. Either way, you will want to control where your jobs run with job constraints . Conventions Local Install Because you don't have admin/root/sudo privileges on the clusters, you won't be able to use sudo and a package manager like apt , yum , etc.; You will need to adapt install instructions to allow for what is called a local or user install. If you prefer or require this method, you should create a singularity container image (see our Singularity guide ), then run it on the cluster. For things to work smoothly you will need to choose and stick with a prefix, or path to your installed applications and libraries. We recommend this be either in your home or project directory, something like ~/software or /path/to/project/software . Make sure you have created it before continuing. Tip If you choose a project directory prefix, it will be easier to share your applications with lab mates or other cluster users. Just make sure to use the true path (the one returned by mydirectories ). Once you've chosen a prefix you will want to add any directory with executables you want to run to your PATH environment variable, and any directores with libraries that your application(s) link to your LD_LIBRARY_PATH environment variable. Each of these tell your shell where to look when you call your application without specifying an absolute path to it. To set these variables permanently, add the following to the end of your ~/.bashrc file: # local installs export MY_PREFIX = ~/software export PATH = $MY_PREFIX /bin: $PATH export LD_LIBRARY_PATH = $MY_PREFIX /lib: $LD_LIBRARY_PATH For the remainder of the guide we'll use the $MY_PREFIX variable to refer to the prefix. See below or your application's install instructions for exactly how to specify your prefix at build/install time. Dependencies You will need to develop a build strategy that works for you and stay consistent. If you're happy using libraries and toolchains that are already available on the cluster as dependencies (recommended), feel free to create module collections that serve as your environments. If you prefer to completely build your own software tree, that is ok too. Whichever route you choose, try to stick with the same version of dependencies (e.g. MPI, zlib, numpy) and compiler you're using (e.g. GCC, intel). We find that unless absolutely necessary, the newest version of a compiler or library might not be the most compatible with a wide array of scientific software so you may want to step back a few versions or try using what was available at the time your application was being developed. Autotools ( configure / make ) If your application includes instructions to run ./bootstrap , ./autogen.sh , ./configure or make , it is using the GNU Build System . configure If you are instructed to run ./configure to generate a Makefile, specify your prefix with the --prefix option. This creates a file, usually named Makefile that is a recipe for make to use to build your application. export MY_PREFIX = ~/software ./configure --prefix = $MY_PREFIX make install If your configure ran properly, make install should properly place your application in your prefix directory. If there is no install target specified for your application, you can either run make and copy the application to your $MY_PREFIX/bin directory or build it somewhere in $MY_PREFIX and add its relevant paths to your PATH and/or LD_LIBRARY_PATH environment variables in your ~/.bashrc file as shown in the local install section. CMake CMake is a popular cross-platform build system. On a linux system, CMake will create a Makefile in a step analogous to ./configure . It is common to create a build directory then run the cmake and make commands from there. Below is what installing to your $MY_DIRECTORY prefix might look like with CMake. CMake instructions also tend to link together the build process onto on line with && , which tells your shell to only continue to the next command if the previous one exited without error. export MY_PREFIX = ~/software mkdir build && cd build && cmake -DCMAKE_INSTALL_PREFIX = $MY_PREFIX .. && make && make install","title":"Build Software"},{"location":"clusters-at-yale/applications/compile/#build-software","text":"How to get software you need up and running on the clusters.","title":"Build Software"},{"location":"clusters-at-yale/applications/compile/#caveat-emptor","text":"We recommend either use existing software modules , Anaconda , Singularity , or pre-compiled software where available. However, there are cases where compiling applications is necessary or desired. This can be because the pre-compiled version isn't readily available/compatible or because compiling applications on the cluster will make an appreciable difference in performance. It is also the case that many R packages are compiled at install time. When compiling applications on the clusters, it is important to consider the ways in which you expect to run the application you are endeavoring to get working. If you want to be able to run jobs calling your application any node on the cluster, you will need to target the oldest hardware available so that newer optimizations are not used that will fail on some nodes. If your application is already quite specialized (e.g. needs GPUs or brand-new CPU instructions), you will want to compile it natively for the subset of compute nodes on which your jobs will run. This decision is often a trade-off between faster individual jobs or jobs that can run on more nodes at once. Each of the cluster pages (see the clusters index for a list) has a \"Compute Node Configurations\" section where nodes are roughly listed from oldest to newest.","title":"caveat emptor"},{"location":"clusters-at-yale/applications/compile/#illegal-instruction-instructions","text":"You may find that software compiled on newer compute nodes will fail with the error Illegal instruction (core dumped) . This includes R/Python libraries that include code that compiles from source. To remedy this issue make sure to always either: Build or install software on the oldest available nodes (right now those are m620 on Farnam and nx360i on Grace ). Require that your jobs running the software in question request similar hardware to their build environment. If your software needs newer instructions using avx2 as a constraint will probably work. Either way, you will want to control where your jobs run with job constraints .","title":"Illegal Instruction Instructions"},{"location":"clusters-at-yale/applications/compile/#conventions","text":"","title":"Conventions"},{"location":"clusters-at-yale/applications/compile/#local-install","text":"Because you don't have admin/root/sudo privileges on the clusters, you won't be able to use sudo and a package manager like apt , yum , etc.; You will need to adapt install instructions to allow for what is called a local or user install. If you prefer or require this method, you should create a singularity container image (see our Singularity guide ), then run it on the cluster. For things to work smoothly you will need to choose and stick with a prefix, or path to your installed applications and libraries. We recommend this be either in your home or project directory, something like ~/software or /path/to/project/software . Make sure you have created it before continuing. Tip If you choose a project directory prefix, it will be easier to share your applications with lab mates or other cluster users. Just make sure to use the true path (the one returned by mydirectories ). Once you've chosen a prefix you will want to add any directory with executables you want to run to your PATH environment variable, and any directores with libraries that your application(s) link to your LD_LIBRARY_PATH environment variable. Each of these tell your shell where to look when you call your application without specifying an absolute path to it. To set these variables permanently, add the following to the end of your ~/.bashrc file: # local installs export MY_PREFIX = ~/software export PATH = $MY_PREFIX /bin: $PATH export LD_LIBRARY_PATH = $MY_PREFIX /lib: $LD_LIBRARY_PATH For the remainder of the guide we'll use the $MY_PREFIX variable to refer to the prefix. See below or your application's install instructions for exactly how to specify your prefix at build/install time.","title":"Local Install"},{"location":"clusters-at-yale/applications/compile/#dependencies","text":"You will need to develop a build strategy that works for you and stay consistent. If you're happy using libraries and toolchains that are already available on the cluster as dependencies (recommended), feel free to create module collections that serve as your environments. If you prefer to completely build your own software tree, that is ok too. Whichever route you choose, try to stick with the same version of dependencies (e.g. MPI, zlib, numpy) and compiler you're using (e.g. GCC, intel). We find that unless absolutely necessary, the newest version of a compiler or library might not be the most compatible with a wide array of scientific software so you may want to step back a few versions or try using what was available at the time your application was being developed.","title":"Dependencies"},{"location":"clusters-at-yale/applications/compile/#autotools-configuremake","text":"If your application includes instructions to run ./bootstrap , ./autogen.sh , ./configure or make , it is using the GNU Build System .","title":"Autotools (configure/make)"},{"location":"clusters-at-yale/applications/compile/#configure","text":"If you are instructed to run ./configure to generate a Makefile, specify your prefix with the --prefix option. This creates a file, usually named Makefile that is a recipe for make to use to build your application. export MY_PREFIX = ~/software ./configure --prefix = $MY_PREFIX","title":"configure"},{"location":"clusters-at-yale/applications/compile/#make-install","text":"If your configure ran properly, make install should properly place your application in your prefix directory. If there is no install target specified for your application, you can either run make and copy the application to your $MY_PREFIX/bin directory or build it somewhere in $MY_PREFIX and add its relevant paths to your PATH and/or LD_LIBRARY_PATH environment variables in your ~/.bashrc file as shown in the local install section.","title":"make install"},{"location":"clusters-at-yale/applications/compile/#cmake","text":"CMake is a popular cross-platform build system. On a linux system, CMake will create a Makefile in a step analogous to ./configure . It is common to create a build directory then run the cmake and make commands from there. Below is what installing to your $MY_DIRECTORY prefix might look like with CMake. CMake instructions also tend to link together the build process onto on line with && , which tells your shell to only continue to the next command if the previous one exited without error. export MY_PREFIX = ~/software mkdir build && cd build && cmake -DCMAKE_INSTALL_PREFIX = $MY_PREFIX .. && make && make install","title":"CMake"},{"location":"clusters-at-yale/applications/easybuild/","text":"Toolchains and EasyBuild We use a build and installation framework called EasyBuild that connects the software we compile and maintain on the clusters with the module system that makes it available to you. Toolchains When we install software, we use pre-defined build environments called toolchains. These are modules that include core compilers and libraries (e.g. GCC , OpenMPI , zlib ). We do this for two main reasons. One is to try to keep our build process simpler. The other is so that you can load two different modules for software built with the same toolchain and expect everything to work. The two common toolchains you will interact with are foss and intel . Each of these have module versions corresponding to the year they were built. Toolchain name and version information is appended to the name of a module so it is clear to us and to the module system what should be compatible. An example would be Python/2.7.12-foss-2016b , where the software name is Python , version 2.7.12 , built with the foss toolchain version 2016b . The easiest way to see what software a toolchain includes is to load it and then list loaded modules. [be59@farnam2 ~]$ module load foss/2016b [be59@farnam2 ~]$ module list Currently Loaded Modules: 1) StdEnv (S) 7) OpenMPI/1.10.3-GCC-5.4.0-2.26 2) GCCcore/5.4.0 8) OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 3) binutils/2.26-GCCcore-5.4.0 9) gompi/2016b 4) GCC/5.4.0-2.26 10) FFTW/3.3.4-gompi-2016b 5) numactl/2.0.11-GCC-5.4.0-2.26 11) ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1 6) hwloc/1.11.3-GCC-5.4.0-2.26 12) foss/2016b Where: S: Module is Sticky, requires --force to unload or purge The takeaway here is that you should try to use modules that match their foss or intel identifiers. Environment Variables If you ever want to refer to the directory where the software from a module is stored, you can use the environment variable $EBROOTMODULENAME where modulename is the name of the module in all caps with no spaces. This can be useful for finding the executables, libraries, or readme files that are included with the software: [be59@farnam2 ~]$ module load SAMtools/1.9-foss-2016b [be59@farnam2 ~]$ ls $EBROOTSAMTOOLS bin easybuild include lib share [be59@farnam2 ~]$ ls $EBROOTSAMTOOLS/bin ace2sam interpolate_sam.pl md5sum-lite r2plot.lua seq_cache_populate.pl wgsim blast2sam.pl maq2sam-long novo2sam.pl sam2vcf.pl soap2sam.pl wgsim_eval.pl bowtie2sam.pl maq2sam-short plot-bamstats samtools varfilter.py zoom2sam.pl export2sam.pl md5fa psl2sam.pl samtools.pl vcfutils.lua","title":"Toolchains and EasyBuild"},{"location":"clusters-at-yale/applications/easybuild/#toolchains-and-easybuild","text":"We use a build and installation framework called EasyBuild that connects the software we compile and maintain on the clusters with the module system that makes it available to you.","title":"Toolchains and EasyBuild"},{"location":"clusters-at-yale/applications/easybuild/#toolchains","text":"When we install software, we use pre-defined build environments called toolchains. These are modules that include core compilers and libraries (e.g. GCC , OpenMPI , zlib ). We do this for two main reasons. One is to try to keep our build process simpler. The other is so that you can load two different modules for software built with the same toolchain and expect everything to work. The two common toolchains you will interact with are foss and intel . Each of these have module versions corresponding to the year they were built. Toolchain name and version information is appended to the name of a module so it is clear to us and to the module system what should be compatible. An example would be Python/2.7.12-foss-2016b , where the software name is Python , version 2.7.12 , built with the foss toolchain version 2016b . The easiest way to see what software a toolchain includes is to load it and then list loaded modules. [be59@farnam2 ~]$ module load foss/2016b [be59@farnam2 ~]$ module list Currently Loaded Modules: 1) StdEnv (S) 7) OpenMPI/1.10.3-GCC-5.4.0-2.26 2) GCCcore/5.4.0 8) OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 3) binutils/2.26-GCCcore-5.4.0 9) gompi/2016b 4) GCC/5.4.0-2.26 10) FFTW/3.3.4-gompi-2016b 5) numactl/2.0.11-GCC-5.4.0-2.26 11) ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1 6) hwloc/1.11.3-GCC-5.4.0-2.26 12) foss/2016b Where: S: Module is Sticky, requires --force to unload or purge The takeaway here is that you should try to use modules that match their foss or intel identifiers.","title":"Toolchains"},{"location":"clusters-at-yale/applications/easybuild/#environment-variables","text":"If you ever want to refer to the directory where the software from a module is stored, you can use the environment variable $EBROOTMODULENAME where modulename is the name of the module in all caps with no spaces. This can be useful for finding the executables, libraries, or readme files that are included with the software: [be59@farnam2 ~]$ module load SAMtools/1.9-foss-2016b [be59@farnam2 ~]$ ls $EBROOTSAMTOOLS bin easybuild include lib share [be59@farnam2 ~]$ ls $EBROOTSAMTOOLS/bin ace2sam interpolate_sam.pl md5sum-lite r2plot.lua seq_cache_populate.pl wgsim blast2sam.pl maq2sam-long novo2sam.pl sam2vcf.pl soap2sam.pl wgsim_eval.pl bowtie2sam.pl maq2sam-short plot-bamstats samtools varfilter.py zoom2sam.pl export2sam.pl md5fa psl2sam.pl samtools.pl vcfutils.lua","title":"Environment Variables"},{"location":"clusters-at-yale/applications/modules/","text":"Load Software with Modules There are many software packages installed on the Yale clusters in addition to those installed in the standard system directories. This software has been specifically installed for use on our clusters at the request of our users. In order to manage these additional packages, the clusters use \"module files\". These module files allow you to easily specify which versions of which packages you want use. List All Loaded Modules The module list command displays all of the module files that are currently loaded in your environment: module list You may notice a module named StdEnv loaded by default. This module sets up your cluster environment; purging this module will make the modules we have installed unavailable. Find Available Modules To list all available module files, execute: module avail You can also list all module files whose name contains a specified string. For example, to find all Python module files, use: module avail python Tip You can get a brief description of a module and the url to the software's homepage by running: module help <modulename> If you don't find a commonly used software you require in the list of modules feel free to send us a software installation request to hpc@yale.edu. Otherwise, check out our installation guides to install it for yourself. Load and Unload Modules The module load command is used to modify your environment so you can use a specified software package. For example, if you found and want to load Python version 2.7.13 , execute the command: module load Python/2.7.13-foss-2016b This modifies the PATH environment variable (as well as a few others) so that your default Python interpreter is version 2.7.13 : [ be59@farnam2 ~ ] $ python --version Python 2 .7.13 You don't have to specify the version. For example, if you want the default module version of R, use: module load R You can also unload a module that you've previously loaded: module unload R Warning Mixing and matching certain software can be tricky due to the way we build our software and modules. In short, make sure that the foss or intel in your module names match if they are present. For more information, see our EasyBuild page . Module Collections It can be a pain to have to enter a long list of module load commands every time you log on to the cluster. Module collections allow you to create saved environments that remember a set list of modules to load together. This method is particularly useful if you have two or more module sets that may conflict with one another. To create a saved environment, simply load all of your desired modules and then type module save This will save this set of modules as your default set. To save a set as a non-default, just assign the environment a name: module save environment_name To load all the modules in the set, enter module restore module restore environment_name to load the default or specified environment, respectively. To modify an environment, restore the environment, make the desired changes by loading and/or unloading modules and save it to the same name. To get a list of your environments, run: module savelist More Information You can view documentation while on the cluster using the command: man module There is even more information at the offical lmod website .","title":"Load Software with Modules"},{"location":"clusters-at-yale/applications/modules/#load-software-with-modules","text":"There are many software packages installed on the Yale clusters in addition to those installed in the standard system directories. This software has been specifically installed for use on our clusters at the request of our users. In order to manage these additional packages, the clusters use \"module files\". These module files allow you to easily specify which versions of which packages you want use.","title":"Load Software with Modules"},{"location":"clusters-at-yale/applications/modules/#list-all-loaded-modules","text":"The module list command displays all of the module files that are currently loaded in your environment: module list You may notice a module named StdEnv loaded by default. This module sets up your cluster environment; purging this module will make the modules we have installed unavailable.","title":"List All Loaded Modules"},{"location":"clusters-at-yale/applications/modules/#find-available-modules","text":"To list all available module files, execute: module avail You can also list all module files whose name contains a specified string. For example, to find all Python module files, use: module avail python Tip You can get a brief description of a module and the url to the software's homepage by running: module help <modulename> If you don't find a commonly used software you require in the list of modules feel free to send us a software installation request to hpc@yale.edu. Otherwise, check out our installation guides to install it for yourself.","title":"Find Available Modules"},{"location":"clusters-at-yale/applications/modules/#load-and-unload-modules","text":"The module load command is used to modify your environment so you can use a specified software package. For example, if you found and want to load Python version 2.7.13 , execute the command: module load Python/2.7.13-foss-2016b This modifies the PATH environment variable (as well as a few others) so that your default Python interpreter is version 2.7.13 : [ be59@farnam2 ~ ] $ python --version Python 2 .7.13 You don't have to specify the version. For example, if you want the default module version of R, use: module load R You can also unload a module that you've previously loaded: module unload R Warning Mixing and matching certain software can be tricky due to the way we build our software and modules. In short, make sure that the foss or intel in your module names match if they are present. For more information, see our EasyBuild page .","title":"Load and Unload Modules"},{"location":"clusters-at-yale/applications/modules/#module-collections","text":"It can be a pain to have to enter a long list of module load commands every time you log on to the cluster. Module collections allow you to create saved environments that remember a set list of modules to load together. This method is particularly useful if you have two or more module sets that may conflict with one another. To create a saved environment, simply load all of your desired modules and then type module save This will save this set of modules as your default set. To save a set as a non-default, just assign the environment a name: module save environment_name To load all the modules in the set, enter module restore module restore environment_name to load the default or specified environment, respectively. To modify an environment, restore the environment, make the desired changes by loading and/or unloading modules and save it to the same name. To get a list of your environments, run: module savelist","title":"Module Collections"},{"location":"clusters-at-yale/applications/modules/#more-information","text":"You can view documentation while on the cluster using the command: man module There is even more information at the offical lmod website .","title":"More Information"},{"location":"clusters-at-yale/applications/new-modules/","text":"New Module System During the December 10-12, 2018 Milgram scheduled maintenance, we will be switching the default module collection. This upgrade will make the cluster more consistent with our other clusters' software environments and allow us to resolve software installation requests more quickly. The old software collection will be available for the time being (see below), but all new software installations will go into the new collection. Key Differences Module Names The new module collection is built using a tool called \"EasyBuild\", which uses a system of \"toolchains\" to more transparently constrain software dependencies and compatibility. See this page for more information on EasyBuild and toolchains. As such, modules will no longer be prefixed with a category such as \"Apps\" or \"Langs\", but instead will have a suffix describing its toolchain (if applicable). # old: module load Apps/Octave/4.2.1 # new: module load Octave/4.2.1-foss-2016b You will need to update your scripts to load modules using their new names. Note that these are new installations of the same software (not redirects to the old installations), so please let us know if anything isn't working as expected. Python and R With the growth of users on our clusters, installing and maintaining central versions of Python and R with all the requested packages has become extremely complicated and unsustainable. As such, our recommendation is now to install a personal Python environment using conda and required R packages using the built-in \"install.packages()\" which will install the packages of your choosing into your home directory. If you run into any difficulty with these environments, we are happy to help--just email us at hpc@yale.edu. To Try and/or Switch to the New Collection To try the new collection and see if all the software you require is available (email us if anything is missing!), run the following command: source /apps/bin/try_new_modules.sh This will switch your module collection (visible via \"module avail\") to the new one for that session. To revert, simply log out and log back in. To switch permanently, run \"module save\" after running the above command. To Use Old Software To use any of the old software, run the following commands on the Milgram login node: module use /gpfs/milgram/apps/hpc/Modules module save Then you should see all the old software if you run \"module avail\". To then remove the old software collection from your list, run: module unuse /gpfs/milgram/apps/hpc/Modules module save We will be deprecating the old software at a TBD date, so we encourage you to contact us if anything is missing or not working for you in the new collection.","title":"New Module System"},{"location":"clusters-at-yale/applications/new-modules/#new-module-system","text":"During the December 10-12, 2018 Milgram scheduled maintenance, we will be switching the default module collection. This upgrade will make the cluster more consistent with our other clusters' software environments and allow us to resolve software installation requests more quickly. The old software collection will be available for the time being (see below), but all new software installations will go into the new collection.","title":"New Module System"},{"location":"clusters-at-yale/applications/new-modules/#key-differences","text":"","title":"Key Differences"},{"location":"clusters-at-yale/applications/new-modules/#module-names","text":"The new module collection is built using a tool called \"EasyBuild\", which uses a system of \"toolchains\" to more transparently constrain software dependencies and compatibility. See this page for more information on EasyBuild and toolchains. As such, modules will no longer be prefixed with a category such as \"Apps\" or \"Langs\", but instead will have a suffix describing its toolchain (if applicable). # old: module load Apps/Octave/4.2.1 # new: module load Octave/4.2.1-foss-2016b You will need to update your scripts to load modules using their new names. Note that these are new installations of the same software (not redirects to the old installations), so please let us know if anything isn't working as expected.","title":"Module Names"},{"location":"clusters-at-yale/applications/new-modules/#python-and-r","text":"With the growth of users on our clusters, installing and maintaining central versions of Python and R with all the requested packages has become extremely complicated and unsustainable. As such, our recommendation is now to install a personal Python environment using conda and required R packages using the built-in \"install.packages()\" which will install the packages of your choosing into your home directory. If you run into any difficulty with these environments, we are happy to help--just email us at hpc@yale.edu.","title":"Python and R"},{"location":"clusters-at-yale/applications/new-modules/#to-try-andor-switch-to-the-new-collection","text":"To try the new collection and see if all the software you require is available (email us if anything is missing!), run the following command: source /apps/bin/try_new_modules.sh This will switch your module collection (visible via \"module avail\") to the new one for that session. To revert, simply log out and log back in. To switch permanently, run \"module save\" after running the above command.","title":"To Try and/or Switch to the New Collection"},{"location":"clusters-at-yale/applications/new-modules/#to-use-old-software","text":"To use any of the old software, run the following commands on the Milgram login node: module use /gpfs/milgram/apps/hpc/Modules module save Then you should see all the old software if you run \"module avail\". To then remove the old software collection from your list, run: module unuse /gpfs/milgram/apps/hpc/Modules module save We will be deprecating the old software at a TBD date, so we encourage you to contact us if anything is missing or not working for you in the new collection.","title":"To Use Old Software"},{"location":"clusters-at-yale/clusters/","text":"HPC Resources Compute We maintain and support 5 compute clusters, with roughly 29,000 cores total. Please click on cluster names for more information. To download a Word document that describes our facilities, equipment, and other resources for HPC and research computing, click here . Cluster Name Approx. Core Count Login Address Monitor Dashboard Purpose Grace 13,500 grace.hpc.yale.edu cluster.ycrc.yale.edu/grace general Farnam 5,200 farnam.hpc.yale.edu cluster.ycrc.yale.edu/farnam medical/life science Omega 6,500 omega.hpc.yale.edu cluster.ycrc.yale.edu/omega highly parallel, tightly coupled Ruddle 3,800 ruddle.hpc.yale.edu cluster.ycrc.yale.edu/ruddle Yale Center for Genome Analysis Milgram 1,600 milgram.hpc.yale.edu monitor1.milgram.hpc.yale.internal:4001 (on HIPAA VPN) HIPAA Storage We maintain several high performance storage systems which amount to about 9 PB total. Listed below are these shared filesystems and the clusters where they are available. We distinguish where clusters store their home directories with an asterisk. The directory /home will always contain the home directory of the cluster you are on. Filesystem Size Mounting Clusters /gpfs/loomis 2.6 PB Grace , Omega , Farnam /gpfs/ysm 1.5 PB Grace, Omega, Farnam* /gpfs/slayman 1.0 PB Grace, Omega, Farnam /gpfs/ycga 2.0 PB Ruddle* /ycga-ba 1.1 PB Ruddle /gpfs/milgram 1.1 PB Milgram*","title":"HPC Resources"},{"location":"clusters-at-yale/clusters/#hpc-resources","text":"","title":"HPC Resources"},{"location":"clusters-at-yale/clusters/#compute","text":"We maintain and support 5 compute clusters, with roughly 29,000 cores total. Please click on cluster names for more information. To download a Word document that describes our facilities, equipment, and other resources for HPC and research computing, click here . Cluster Name Approx. Core Count Login Address Monitor Dashboard Purpose Grace 13,500 grace.hpc.yale.edu cluster.ycrc.yale.edu/grace general Farnam 5,200 farnam.hpc.yale.edu cluster.ycrc.yale.edu/farnam medical/life science Omega 6,500 omega.hpc.yale.edu cluster.ycrc.yale.edu/omega highly parallel, tightly coupled Ruddle 3,800 ruddle.hpc.yale.edu cluster.ycrc.yale.edu/ruddle Yale Center for Genome Analysis Milgram 1,600 milgram.hpc.yale.edu monitor1.milgram.hpc.yale.internal:4001 (on HIPAA VPN) HIPAA","title":"Compute"},{"location":"clusters-at-yale/clusters/#storage","text":"We maintain several high performance storage systems which amount to about 9 PB total. Listed below are these shared filesystems and the clusters where they are available. We distinguish where clusters store their home directories with an asterisk. The directory /home will always contain the home directory of the cluster you are on. Filesystem Size Mounting Clusters /gpfs/loomis 2.6 PB Grace , Omega , Farnam /gpfs/ysm 1.5 PB Grace, Omega, Farnam* /gpfs/slayman 1.0 PB Grace, Omega, Farnam /gpfs/ycga 2.0 PB Ruddle* /ycga-ba 1.1 PB Ruddle /gpfs/milgram 1.1 PB Milgram*","title":"Storage"},{"location":"clusters-at-yale/clusters/farnam/","text":"Farnam The Farnam Cluster is named for Louise Whitman Farnam , the first woman to graduate from the Yale School of Medicine, class of 1916. Farnam is a shared-use resource for the Yale School of Medicine (YSM). It consists of a variety of compute nodes networked over ethernet and mounts several shared filesystems. Hardware Farnam is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. GPUs listed can be requested with the --gres flag, e.g. --gres=gpu:gtx1080ti:2 would request 2 GeForce GTX 1080Ti GPUs per node. See the Request Compute Resources page for more info. Warning Care should be taken if when scheduling your job if you are running programs/libraries optimized for specific hardware. See the guide on how to compile software for specific guidance. Compute Node Configurations Count Node Type CPU CPU Cores RAM GPU Features 59 Dell PowerEdge M620 2x E5-2670 16 121G sandybridge, sse4_2, avx, E5-2670 14 Dell PowerEdge M915 4x AMD Opteron 6276 32 507G bulldozer, sse4_2, avx, opteron-6276 117 Lenovo nx360h 2x E5-2660 v3 20 121G haswell, v3, sse4_2, avx, avx2, E5-2660_v3 5 Lenovo nx360h w/GPUs 2x E5-2660 v3 20 121G 2x k80 (2GPUs/k80) haswell, v3, sse4_2, avx, avx2, E5-2660_v3 38 Lenovo nx360b 2x E5-2680 v4 28 246G broadwell, v4, sse4_2, avx, avx2, E5-2680_v4 3 Lenovo nx360b w/GPUs 2x E5-2660 v4 28 246G 2x p100 broadwell, v4, sse4_2, avx, avx2, E5-2660_v4 2 Lenovo sd530 2x Gold 6132 28 184G skylake, sse4_2, avx, avx2, avx512, 6132 1 Lenovo sd530 2x Gold 6132 28 751G skylake, sse4_2, avx, avx2, avx512, 6132 1 Thinkmate GPX XT4 (gpu_devel) 2x E5-2623 v4 8 58G 4x gtx1080ti broadwell, v4, sse4_2, avx, avx2, E5-2623_v4 20 Thinkmate GPX XT4 2x E5-2637 v4 8 121G 4x gtx1080ti broadwell, v4, sse4_2, avx, avx2, E5-2637_v4 1 Thinkmate GPX XT4 2x E5-2637 v4 8 121G 4x titanv broadwell, v4, sse4_2, avx, avx2, E5-2637_v4 2 Lenovo 3850X6 4x E7-4809 v3 32 1507G haswell, v3, sse4_2, avx, avx2, E7-4809_v3 1 Lenovo 3850X6 4x E7-4820 v4 40 1507G broadwell, v3, sse4_2, avx, avx2, E7-4820_v4 Slurm Partitions Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . Public Partitions The general partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The bigmem partition contains our largest memory nodes; only jobs that cannot be satisfied by general should run here. The gpu_devel partition is a single node meant for testing or compiling GPU accelerated code, and the gpu partition is where normal GPU jobs should run. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime default/max Node type (number) general* 400 CPUs, 2560 G RAM 200 CPUs, 1280 G RAM 1d/30d m620 (34), nx360h (94) interactive 20 CPUs, 256 G RAM 1d/2d m620 (34), nx360h (94) bigmem 2 jobs, 32 CPUs, 1532 G RAM 1d/7d m915 (9), 3850X6 (2) gpu_devel 1 job 10min/2hr GPX XT4 gtx1080ti (1) gpu 32 CPUs, 256 G RAM 1d/2d nx360h k80 (2), GPX XT4 gtx1080ti (10) scavenge 800 CPUs, 5120 G RAM 1d/7d all * default Private Partitions Private partitions contain nodes acquired by specific research groups. Full access to these partitions is granted at the discretion of the owner. Contact us if your group would like to purchase nodes. Partition Walltime default/max Node type (number) pi_breaker 1d/14d nx360b (24) pi_cryoem 1d/\u221e GPX XT4 gtx1080ti (10) pi_deng 1d/14d nx360b (1) pi_gerstein 1d/14d m915 (2), nx360b (11), sd530 (3), 3850X6 (1) pi_gerstein_gpu 1d/14d nx360h k80 (3), nx360b p100 (2), GPX XT4 titanv (1) pi_gruen 1d/14d nx360b (1) pi_jadi 1d/14d nx360b (2) pi_kleinstein 1d/14d m915 (1), nx360h (3) pi_krauthammer 1d/14d nx360h (1) pi_ma 1d/14d nx360h (2) pi_ohern 1d/14d m620 (6), nx360h (3) pi_sigworth 1d/14d nx360h (1) pi_sindelar 1d/14d m620 (4), m915 (1), nx360h (1) pi_strobel 1d/14d m915 (1) pi_townsend 1d/14d nx360h (5) pi_zhao 1d/14d m620 (17), m915 (1) Public Datasets We host datasets of general interest in a loosely organized directory tree in /gpfs/ysm/datasets : \u251c\u2500\u2500 cryoem \u251c\u2500\u2500 db \u2502 \u251c\u2500\u2500 annovar \u2502 \u2514\u2500\u2500 blast \u2514\u2500\u2500 genomes \u251c\u2500\u2500 1000Genomes \u251c\u2500\u2500 10xgenomics \u251c\u2500\u2500 Aedes_aegypti \u251c\u2500\u2500 Chelonoidis_nigra \u251c\u2500\u2500 Danio_rerio \u251c\u2500\u2500 hisat2 \u251c\u2500\u2500 Homo_sapiens \u251c\u2500\u2500 Mus_musculus \u251c\u2500\u2500 PhiX \u2514\u2500\u2500 Saccharomyces_cerevisiae If you would like us to host a dataset or questions about what is currently available, please email hpc@yale.edu. Storage Farnam has access to a number of GPFS filesystems. /gpfs/ysm is Farnam's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/ysm/home 125G/user 500,000 Yes project /gpfs/ysm/project 4T/group 5,000,000 No scratch60 /gpfs/ysm/scratch60 10T/group 5,000,000 No","title":"Farnam"},{"location":"clusters-at-yale/clusters/farnam/#farnam","text":"The Farnam Cluster is named for Louise Whitman Farnam , the first woman to graduate from the Yale School of Medicine, class of 1916. Farnam is a shared-use resource for the Yale School of Medicine (YSM). It consists of a variety of compute nodes networked over ethernet and mounts several shared filesystems.","title":"Farnam"},{"location":"clusters-at-yale/clusters/farnam/#hardware","text":"Farnam is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. GPUs listed can be requested with the --gres flag, e.g. --gres=gpu:gtx1080ti:2 would request 2 GeForce GTX 1080Ti GPUs per node. See the Request Compute Resources page for more info. Warning Care should be taken if when scheduling your job if you are running programs/libraries optimized for specific hardware. See the guide on how to compile software for specific guidance.","title":"Hardware"},{"location":"clusters-at-yale/clusters/farnam/#compute-node-configurations","text":"Count Node Type CPU CPU Cores RAM GPU Features 59 Dell PowerEdge M620 2x E5-2670 16 121G sandybridge, sse4_2, avx, E5-2670 14 Dell PowerEdge M915 4x AMD Opteron 6276 32 507G bulldozer, sse4_2, avx, opteron-6276 117 Lenovo nx360h 2x E5-2660 v3 20 121G haswell, v3, sse4_2, avx, avx2, E5-2660_v3 5 Lenovo nx360h w/GPUs 2x E5-2660 v3 20 121G 2x k80 (2GPUs/k80) haswell, v3, sse4_2, avx, avx2, E5-2660_v3 38 Lenovo nx360b 2x E5-2680 v4 28 246G broadwell, v4, sse4_2, avx, avx2, E5-2680_v4 3 Lenovo nx360b w/GPUs 2x E5-2660 v4 28 246G 2x p100 broadwell, v4, sse4_2, avx, avx2, E5-2660_v4 2 Lenovo sd530 2x Gold 6132 28 184G skylake, sse4_2, avx, avx2, avx512, 6132 1 Lenovo sd530 2x Gold 6132 28 751G skylake, sse4_2, avx, avx2, avx512, 6132 1 Thinkmate GPX XT4 (gpu_devel) 2x E5-2623 v4 8 58G 4x gtx1080ti broadwell, v4, sse4_2, avx, avx2, E5-2623_v4 20 Thinkmate GPX XT4 2x E5-2637 v4 8 121G 4x gtx1080ti broadwell, v4, sse4_2, avx, avx2, E5-2637_v4 1 Thinkmate GPX XT4 2x E5-2637 v4 8 121G 4x titanv broadwell, v4, sse4_2, avx, avx2, E5-2637_v4 2 Lenovo 3850X6 4x E7-4809 v3 32 1507G haswell, v3, sse4_2, avx, avx2, E7-4809_v3 1 Lenovo 3850X6 4x E7-4820 v4 40 1507G broadwell, v3, sse4_2, avx, avx2, E7-4820_v4","title":"Compute Node Configurations"},{"location":"clusters-at-yale/clusters/farnam/#slurm-partitions","text":"Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm .","title":"Slurm Partitions"},{"location":"clusters-at-yale/clusters/farnam/#public-partitions","text":"The general partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The bigmem partition contains our largest memory nodes; only jobs that cannot be satisfied by general should run here. The gpu_devel partition is a single node meant for testing or compiling GPU accelerated code, and the gpu partition is where normal GPU jobs should run. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime default/max Node type (number) general* 400 CPUs, 2560 G RAM 200 CPUs, 1280 G RAM 1d/30d m620 (34), nx360h (94) interactive 20 CPUs, 256 G RAM 1d/2d m620 (34), nx360h (94) bigmem 2 jobs, 32 CPUs, 1532 G RAM 1d/7d m915 (9), 3850X6 (2) gpu_devel 1 job 10min/2hr GPX XT4 gtx1080ti (1) gpu 32 CPUs, 256 G RAM 1d/2d nx360h k80 (2), GPX XT4 gtx1080ti (10) scavenge 800 CPUs, 5120 G RAM 1d/7d all * default","title":"Public Partitions"},{"location":"clusters-at-yale/clusters/farnam/#private-partitions","text":"Private partitions contain nodes acquired by specific research groups. Full access to these partitions is granted at the discretion of the owner. Contact us if your group would like to purchase nodes. Partition Walltime default/max Node type (number) pi_breaker 1d/14d nx360b (24) pi_cryoem 1d/\u221e GPX XT4 gtx1080ti (10) pi_deng 1d/14d nx360b (1) pi_gerstein 1d/14d m915 (2), nx360b (11), sd530 (3), 3850X6 (1) pi_gerstein_gpu 1d/14d nx360h k80 (3), nx360b p100 (2), GPX XT4 titanv (1) pi_gruen 1d/14d nx360b (1) pi_jadi 1d/14d nx360b (2) pi_kleinstein 1d/14d m915 (1), nx360h (3) pi_krauthammer 1d/14d nx360h (1) pi_ma 1d/14d nx360h (2) pi_ohern 1d/14d m620 (6), nx360h (3) pi_sigworth 1d/14d nx360h (1) pi_sindelar 1d/14d m620 (4), m915 (1), nx360h (1) pi_strobel 1d/14d m915 (1) pi_townsend 1d/14d nx360h (5) pi_zhao 1d/14d m620 (17), m915 (1)","title":"Private Partitions"},{"location":"clusters-at-yale/clusters/farnam/#public-datasets","text":"We host datasets of general interest in a loosely organized directory tree in /gpfs/ysm/datasets : \u251c\u2500\u2500 cryoem \u251c\u2500\u2500 db \u2502 \u251c\u2500\u2500 annovar \u2502 \u2514\u2500\u2500 blast \u2514\u2500\u2500 genomes \u251c\u2500\u2500 1000Genomes \u251c\u2500\u2500 10xgenomics \u251c\u2500\u2500 Aedes_aegypti \u251c\u2500\u2500 Chelonoidis_nigra \u251c\u2500\u2500 Danio_rerio \u251c\u2500\u2500 hisat2 \u251c\u2500\u2500 Homo_sapiens \u251c\u2500\u2500 Mus_musculus \u251c\u2500\u2500 PhiX \u2514\u2500\u2500 Saccharomyces_cerevisiae If you would like us to host a dataset or questions about what is currently available, please email hpc@yale.edu.","title":"Public Datasets"},{"location":"clusters-at-yale/clusters/farnam/#storage","text":"Farnam has access to a number of GPFS filesystems. /gpfs/ysm is Farnam's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/ysm/home 125G/user 500,000 Yes project /gpfs/ysm/project 4T/group 5,000,000 No scratch60 /gpfs/ysm/scratch60 10T/group 5,000,000 No","title":"Storage"},{"location":"clusters-at-yale/clusters/grace/","text":"Grace The Grace cluster is is named for the computer scientist and United States Navy Rear Admiral Grace Murray Hopper , who received her Ph.D. in Mathematics from Yale in 1934. Grace is a shared-use resource for the Faculty of Arts and Sciences (FAS). It consists of a variety of compute nodes networked over low-latency InfiniBand and mounts several shared filesystems. Hardware Grace is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. GPUs listed can be requested with the --gres flag, e.g. --gres=gpu:p100:1 would request one Tesla P100 GPU per node. See the Request Compute Resources page for more info. Warning Care should be taken if when scheduling your job if you are running programs/libraries optimized for specific hardware. See the guide on how to compile software for specific guidance. Compute Node Configurations Count Node Type CPU CPU Cores RAM GPU Features 80 IBM nx360i 2x E5-2660 v2 20 121G ivybridge, v2, sse4_2, avx, E5-2660_v2 136 Lenovo nx360h 2x E5-2660 v3 20 121G haswell, v3, sse4_2, avx, avx2, E5-2660_v3 20 Lenovo nx360h 2x E5-2660 v3 20 247G haswell, v3, sse4_2, avx, avx2, E5-2660_v3 6 Lenovo nx360h w/ GPUs 2x E5-2660 v3 20 121G 2x k80 (2GPUs/k80) haswell, v3, sse4_2, avx, avx2, E5-2660_v3 8 Lenovo nx360h w/ GPUs 2x E5-2660 v3 20 247G 1x k80 (2GPUs/k80) haswell, v3, sse4_2, avx, avx2, E5-2660_v3 161 Lenovo nx360b 2x E5-2660 v4 28 247G broadwell, v4, sse4_2, avx, avx2, E5-2660_v4 7 Lenovo nx360b 2x E5-2660 v4 28 247 1x p100 broadwell, v4, sse4_2, avx, avx2, E5-2660_v4 53 Lenovo sd530 2x Gold 6136 24 90G skylake, sse4_2, avx, avx2, avx512, 6136 1 Lenovo sd530 2x Gold 6136 24 176G skylake, sse4_2, avx, avx2, avx512, 6126 1 Lenovo sd530 w/ GPUs 2x Gold 6136 24 90G 2x v100 skylake, sse4_2, avx, avx2, avx512, 6136 1 Lenovo sd530 2x Gold 6136 24 751G skylake, sse4_2, avx, avx2, avx512, 6136 1 IBM x3850i 4x E7-4820 v2 32 1003G ivybridge, v2, sse4_2, avx, E7-4820_v2 1 Lenovo x3850h 4x E7-4809 v2 32 2011G haswell, v3, sse4_2, avx, avx2, E7-4809_v3 4 Lenovo x3850b 4x E7-4820 v4 40 1507G broadwell, v4, sse4_2, avx, avx2, E7-4820_v4 1 Thinkmate GPX XT4 2x E5-2637 v4 8 121G 4x gtx1080ti broadwell, v4, sse4_2, avx, avx2, E5-2637_v4 9 Penguin XE2118GT 2x Gold 6136 24 183G 4x p100 skylake, sse4_2, avx, avx2, avx512, 6136 Slurm Partitions Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . Public Partitions The day partition is where most batch jobs should run, and is the default if you don't specify a partition. The week partition is smaller, but allows for longer jobs. The interactive partition should only be used for testing or compiling software. The bigmem partition contains our largest memory node; only jobs that cannot be satisfied by general should run here. The gpu_devel partition is a single node meant for testing or compiling GPU accelerated code, and the gpu partition is where normal GPU jobs should run. The mpi partition is reserved for tightly-coupled parallel and access is by special request only. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime default/max Node type (number) day* 900 CPUs 640 CPUs 1h/1d nx360i (49), nx360h (32), nx360b (72) week 250 CPUs 100 CPUs 1h/7d nx360h (48), nx360b (7) interactive 1 job, 4 CPUs, 32 G RAM 1h/6h nx360i (2) bigmem 40 CPUs, 1500 G RAM 1h/1d x3850b (1) gpu_devel 1 job, 10 CPUs, 60 G RAM 10min/4hr nx360h k80 (1) gpu 32 CPUs, 256 G RAM 6 nodes 1h/1d nx360b p100 (6), nx360h k80 (3), sd530 v100 (1) mpi** 18 nodes 1h/1d sd530 (35) scavenge 6400 CPUs 1h/1d all * default ** The mpi partition is reserved for tightly-coupled parallel programs that make efficient use of multiple nodes. Contact us at hpc@yale.edu for access if your workload fits this description. Private Partitions Private partitions contain nodes acquired by specific research groups. Full access to these partitions is granted at the discretion of the owner. Contact us if your group would like to purchase nodes. Partition Walltime default/max Node type (number) pi_altonji 1d/28d nx360h (2) pi_anticevic 1d/100d nx360h (16), nx360b (20) pi_anticevic_bigmem 1d/100d x3850h (1) pi_anticevic_fs 1d/100d nx360h (3) pi_anticevic_gpu 1d/100d nx360h k80 (8) pi_balou 1d/28d nx360b (30) pi_berry 1d/28d nx360h (1) pi_cowles 1d/28d nx360h (14) pi_cowles_nopreempt 1d/28d nx360h (10) pi_gelernter 1d/28d nx360b (1) pi_gerstein 1d/28d x3850i (1), nx360h (32) pi_glahn 1d/100d nx360h (1) pi_hammes_schiffer 1d/28d sd530 (17), GPX XT4 gtx1080ti (1) pi_holland 1d/28d nx360h (2) pi_jetz 1d/28d nx360b (2) pi_kaminski 1d/28d nx360h (8) pi_mak 1d/28d nx360h (8) pi_manohar 1d/180d nx360b (8), x3850b (2), nx360b p100 (1) pi_ohern 1d/28d nx360i (16), nx360b (3), XE2118GT p100 (9) pi_owen_miller 1d/28d nx360b (5) pi_poland 1d/28d nx360b (10) pi_seto 1d/28d nx360b (1), sd530(3) pi_som 1d/28d nx360i (4) pi_tsmith 1d/28d nx360h (1) Storage Grace has access to a number of GPFS filesystems. /gpfs/loomis is Grace's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/loomis/home.grace 100G/user 200,000 Yes project /gpfs/loomis/project 1T/group 5,000,000 No scratch60 /gpfs/loomis/scratch60 20T/group 5,000,000 No","title":"Grace"},{"location":"clusters-at-yale/clusters/grace/#grace","text":"The Grace cluster is is named for the computer scientist and United States Navy Rear Admiral Grace Murray Hopper , who received her Ph.D. in Mathematics from Yale in 1934. Grace is a shared-use resource for the Faculty of Arts and Sciences (FAS). It consists of a variety of compute nodes networked over low-latency InfiniBand and mounts several shared filesystems.","title":"Grace"},{"location":"clusters-at-yale/clusters/grace/#hardware","text":"Grace is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. GPUs listed can be requested with the --gres flag, e.g. --gres=gpu:p100:1 would request one Tesla P100 GPU per node. See the Request Compute Resources page for more info. Warning Care should be taken if when scheduling your job if you are running programs/libraries optimized for specific hardware. See the guide on how to compile software for specific guidance.","title":"Hardware"},{"location":"clusters-at-yale/clusters/grace/#compute-node-configurations","text":"Count Node Type CPU CPU Cores RAM GPU Features 80 IBM nx360i 2x E5-2660 v2 20 121G ivybridge, v2, sse4_2, avx, E5-2660_v2 136 Lenovo nx360h 2x E5-2660 v3 20 121G haswell, v3, sse4_2, avx, avx2, E5-2660_v3 20 Lenovo nx360h 2x E5-2660 v3 20 247G haswell, v3, sse4_2, avx, avx2, E5-2660_v3 6 Lenovo nx360h w/ GPUs 2x E5-2660 v3 20 121G 2x k80 (2GPUs/k80) haswell, v3, sse4_2, avx, avx2, E5-2660_v3 8 Lenovo nx360h w/ GPUs 2x E5-2660 v3 20 247G 1x k80 (2GPUs/k80) haswell, v3, sse4_2, avx, avx2, E5-2660_v3 161 Lenovo nx360b 2x E5-2660 v4 28 247G broadwell, v4, sse4_2, avx, avx2, E5-2660_v4 7 Lenovo nx360b 2x E5-2660 v4 28 247 1x p100 broadwell, v4, sse4_2, avx, avx2, E5-2660_v4 53 Lenovo sd530 2x Gold 6136 24 90G skylake, sse4_2, avx, avx2, avx512, 6136 1 Lenovo sd530 2x Gold 6136 24 176G skylake, sse4_2, avx, avx2, avx512, 6126 1 Lenovo sd530 w/ GPUs 2x Gold 6136 24 90G 2x v100 skylake, sse4_2, avx, avx2, avx512, 6136 1 Lenovo sd530 2x Gold 6136 24 751G skylake, sse4_2, avx, avx2, avx512, 6136 1 IBM x3850i 4x E7-4820 v2 32 1003G ivybridge, v2, sse4_2, avx, E7-4820_v2 1 Lenovo x3850h 4x E7-4809 v2 32 2011G haswell, v3, sse4_2, avx, avx2, E7-4809_v3 4 Lenovo x3850b 4x E7-4820 v4 40 1507G broadwell, v4, sse4_2, avx, avx2, E7-4820_v4 1 Thinkmate GPX XT4 2x E5-2637 v4 8 121G 4x gtx1080ti broadwell, v4, sse4_2, avx, avx2, E5-2637_v4 9 Penguin XE2118GT 2x Gold 6136 24 183G 4x p100 skylake, sse4_2, avx, avx2, avx512, 6136","title":"Compute Node Configurations"},{"location":"clusters-at-yale/clusters/grace/#slurm-partitions","text":"Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm .","title":"Slurm Partitions"},{"location":"clusters-at-yale/clusters/grace/#public-partitions","text":"The day partition is where most batch jobs should run, and is the default if you don't specify a partition. The week partition is smaller, but allows for longer jobs. The interactive partition should only be used for testing or compiling software. The bigmem partition contains our largest memory node; only jobs that cannot be satisfied by general should run here. The gpu_devel partition is a single node meant for testing or compiling GPU accelerated code, and the gpu partition is where normal GPU jobs should run. The mpi partition is reserved for tightly-coupled parallel and access is by special request only. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime default/max Node type (number) day* 900 CPUs 640 CPUs 1h/1d nx360i (49), nx360h (32), nx360b (72) week 250 CPUs 100 CPUs 1h/7d nx360h (48), nx360b (7) interactive 1 job, 4 CPUs, 32 G RAM 1h/6h nx360i (2) bigmem 40 CPUs, 1500 G RAM 1h/1d x3850b (1) gpu_devel 1 job, 10 CPUs, 60 G RAM 10min/4hr nx360h k80 (1) gpu 32 CPUs, 256 G RAM 6 nodes 1h/1d nx360b p100 (6), nx360h k80 (3), sd530 v100 (1) mpi** 18 nodes 1h/1d sd530 (35) scavenge 6400 CPUs 1h/1d all * default ** The mpi partition is reserved for tightly-coupled parallel programs that make efficient use of multiple nodes. Contact us at hpc@yale.edu for access if your workload fits this description.","title":"Public Partitions"},{"location":"clusters-at-yale/clusters/grace/#private-partitions","text":"Private partitions contain nodes acquired by specific research groups. Full access to these partitions is granted at the discretion of the owner. Contact us if your group would like to purchase nodes. Partition Walltime default/max Node type (number) pi_altonji 1d/28d nx360h (2) pi_anticevic 1d/100d nx360h (16), nx360b (20) pi_anticevic_bigmem 1d/100d x3850h (1) pi_anticevic_fs 1d/100d nx360h (3) pi_anticevic_gpu 1d/100d nx360h k80 (8) pi_balou 1d/28d nx360b (30) pi_berry 1d/28d nx360h (1) pi_cowles 1d/28d nx360h (14) pi_cowles_nopreempt 1d/28d nx360h (10) pi_gelernter 1d/28d nx360b (1) pi_gerstein 1d/28d x3850i (1), nx360h (32) pi_glahn 1d/100d nx360h (1) pi_hammes_schiffer 1d/28d sd530 (17), GPX XT4 gtx1080ti (1) pi_holland 1d/28d nx360h (2) pi_jetz 1d/28d nx360b (2) pi_kaminski 1d/28d nx360h (8) pi_mak 1d/28d nx360h (8) pi_manohar 1d/180d nx360b (8), x3850b (2), nx360b p100 (1) pi_ohern 1d/28d nx360i (16), nx360b (3), XE2118GT p100 (9) pi_owen_miller 1d/28d nx360b (5) pi_poland 1d/28d nx360b (10) pi_seto 1d/28d nx360b (1), sd530(3) pi_som 1d/28d nx360i (4) pi_tsmith 1d/28d nx360h (1)","title":"Private Partitions"},{"location":"clusters-at-yale/clusters/grace/#storage","text":"Grace has access to a number of GPFS filesystems. /gpfs/loomis is Grace's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/loomis/home.grace 100G/user 200,000 Yes project /gpfs/loomis/project 1T/group 5,000,000 No scratch60 /gpfs/loomis/scratch60 20T/group 5,000,000 No","title":"Storage"},{"location":"clusters-at-yale/clusters/milgram-workstations/","text":"Milgram Workstations Host Name Lab Location cannon1.milgram.hpc.yale.internal Cannon SSS Hall cannon2.milgram.hpc.yale.internal Cannon SSS Hall casey1.milgram.hpc.yale.internal Casey SSS Hall chang1.milgram.hpc.yale.internal Chang Dunham Lab cl1.milgram.hpc.yale.internal Chun SSS Hall cl2.milgram.hpc.yale.internal Chun SSS Hall cl3.milgram.hpc.yale.internal Chun SSS Hall crockett1.milgram.hpc.yale.internal Crockett Dunham Lab gee1.milgram.hpc.yale.internal Gee Kirtland Hall gee2.milgram.hpc.yale.internal Gee Kirtland Hall hl1.milgram.hpc.yale.internal Holmes SSS Hall hl2.milgram.hpc.yale.internal Holmes SSS Hall joormann1.milgram.hpc.yale.internal Joorman Kirtland Hall","title":"Milgram Workstations"},{"location":"clusters-at-yale/clusters/milgram-workstations/#milgram-workstations","text":"Host Name Lab Location cannon1.milgram.hpc.yale.internal Cannon SSS Hall cannon2.milgram.hpc.yale.internal Cannon SSS Hall casey1.milgram.hpc.yale.internal Casey SSS Hall chang1.milgram.hpc.yale.internal Chang Dunham Lab cl1.milgram.hpc.yale.internal Chun SSS Hall cl2.milgram.hpc.yale.internal Chun SSS Hall cl3.milgram.hpc.yale.internal Chun SSS Hall crockett1.milgram.hpc.yale.internal Crockett Dunham Lab gee1.milgram.hpc.yale.internal Gee Kirtland Hall gee2.milgram.hpc.yale.internal Gee Kirtland Hall hl1.milgram.hpc.yale.internal Holmes SSS Hall hl2.milgram.hpc.yale.internal Holmes SSS Hall joormann1.milgram.hpc.yale.internal Joorman Kirtland Hall","title":"Milgram Workstations"},{"location":"clusters-at-yale/clusters/milgram/","text":"Milgram Milgram is named for Dr. Stanley Milgram, a psychologist who researched the behavioral motivations behind social awareness in individuals and obedience to authority figures. He conducted several famous experiments during his professorship at Yale University including the lost-letter experiment, the small-world experiment, and the Milgram experiment on obedience to authority figures. Milgram is a HIPAA aligned Department of Psychology cluster intended for use on projects that may involve sensitive data. This applies to both storage and computation. If you have any questions about this policy, please contact us . Note Connections to Milgram can only be made from the HIPAA VPN ( access.yale.edu/hipaa ). See our VPN page for setup instructions. If your group has a workstation (see list ), you can connect using one of those. Hardware Milgram is made up of a couple kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. Warning Care should be taken if when scheduling your job if you are running programs/libraries optimized for specific hardware. See the guide on how to compile software for specific guidance. Compute Node Configurations Count Node Type CPU CPU Cores RAM Features 12 Dell R730 2x E5-2660 v3 20 121G haswell, v3, sse4_2, avx, avx2, E5-2660_v3 48 Lenovo nx360b 2x E5-2660 v4 28 250G broadwell, v4, sse4_2, avx, avx2, E5-2660_v4 Slurm Partitions Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The short partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The long and verylong partitions are meant for jobs with projected walltimes that are too long to run in short. For courses using the cluster we set aside the education partition. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime default/max Node type (number) short* 1158 CPUs, 10176 G RAM 772 CPUs, 6784 G RAM 1h/6h nx360b (48), R730 (9) interactive 1 job, 4 CPUs, 20 G RAM 1h/6h R730 (1) long 1188 CPUs, 5940 G RAM 1h/2d nx360b (48), R730 (9) verylong 792 CPUs, 3960 G RAM 1h/7d nx360b (48), R730 (9) education 1h/6h R730 (2) scavenge none nx360b (48), R730 (9) * default Storage /gpfs/milgram is Milgram's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/milgram/home 20G/user 500,000 Yes project /gpfs/milgram/project varies varies No scratch60 /gpfs/milgram/scratch60 varies 5,000,000 No","title":"Milgram"},{"location":"clusters-at-yale/clusters/milgram/#milgram","text":"Milgram is named for Dr. Stanley Milgram, a psychologist who researched the behavioral motivations behind social awareness in individuals and obedience to authority figures. He conducted several famous experiments during his professorship at Yale University including the lost-letter experiment, the small-world experiment, and the Milgram experiment on obedience to authority figures. Milgram is a HIPAA aligned Department of Psychology cluster intended for use on projects that may involve sensitive data. This applies to both storage and computation. If you have any questions about this policy, please contact us . Note Connections to Milgram can only be made from the HIPAA VPN ( access.yale.edu/hipaa ). See our VPN page for setup instructions. If your group has a workstation (see list ), you can connect using one of those.","title":"Milgram"},{"location":"clusters-at-yale/clusters/milgram/#hardware","text":"Milgram is made up of a couple kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. Warning Care should be taken if when scheduling your job if you are running programs/libraries optimized for specific hardware. See the guide on how to compile software for specific guidance.","title":"Hardware"},{"location":"clusters-at-yale/clusters/milgram/#compute-node-configurations","text":"Count Node Type CPU CPU Cores RAM Features 12 Dell R730 2x E5-2660 v3 20 121G haswell, v3, sse4_2, avx, avx2, E5-2660_v3 48 Lenovo nx360b 2x E5-2660 v4 28 250G broadwell, v4, sse4_2, avx, avx2, E5-2660_v4","title":"Compute Node Configurations"},{"location":"clusters-at-yale/clusters/milgram/#slurm-partitions","text":"Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The short partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The long and verylong partitions are meant for jobs with projected walltimes that are too long to run in short. For courses using the cluster we set aside the education partition. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition Group Limits User Limits Walltime default/max Node type (number) short* 1158 CPUs, 10176 G RAM 772 CPUs, 6784 G RAM 1h/6h nx360b (48), R730 (9) interactive 1 job, 4 CPUs, 20 G RAM 1h/6h R730 (1) long 1188 CPUs, 5940 G RAM 1h/2d nx360b (48), R730 (9) verylong 792 CPUs, 3960 G RAM 1h/7d nx360b (48), R730 (9) education 1h/6h R730 (2) scavenge none nx360b (48), R730 (9) * default","title":"Slurm Partitions"},{"location":"clusters-at-yale/clusters/milgram/#storage","text":"/gpfs/milgram is Milgram's primary filesystem where home, project, and scratch60 directories are located. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/milgram/home 20G/user 500,000 Yes project /gpfs/milgram/project varies varies No scratch60 /gpfs/milgram/scratch60 varies 5,000,000 No","title":"Storage"},{"location":"clusters-at-yale/clusters/omega/","text":"Omega Warning All of Omega's scratch space is read-and-delete-only as of Dec 3, 2018. The scratch space will be purged and all data permanently deleted on Feb 1, 2019. Omega has now served Yale\u2019s research community well for more than 2 years past the normal end-of-life for similar clusters. Most of its components are no longer under vendor warranty, and parts are sometimes difficult to obtain, so we are forced to support it on a best-effort basis. Last year, we developed a multi-year plan to replace Omega, which began with moving Omega\u2019s shared resources to our Grace cluster, for which we acquired new commons nodes. We plan to continue to support groups with dedicated node allocations and other users running tightly-coupled parallel jobs on Omega until Mid 2019. The mpi partition on Grace contains the replacement nodes the remainder of Omega. Please test your workload on those nodes are your convenience. We will provide ample warning before the final Omega decommission. Clean Out Omega Data All Omega files are now stored solely on the Loomis GPFS system. For groups that have migrated their workloads entirely to Grace or Farnam, their Omega data is now available from Grace and Farnam for copying and clean-up until Feb 2019. See Cleaning Out Omega Data for instructions on retrieving your data. Hardware The cluster is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. Compute Node Configurations Count Node Type CPU CPU Cores RAM Features 668 HP ProLiant BL460c X5560 8 32G nehalem,sse4_2,X5560 32 HP ProLiant BL460c X5560 8 44G nehalem,sse4_2,X5560,extramem 4 HP ProLiant SL250s E5-2650 16 121G sandybridge,sse4_2,avx,E5-2650 60 HP ProLiant SL230s E5-2620 12 59G sandybridge,sse4_2,avx,E5-2620 Slurm Partitions Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . Public Partitions The day partition is where most batch jobs should run, and is the default if you don't specify a partition. The week partition is smaller, but allows for longer jobs. The interactive partition should only be used for testing or compiling software. The bigmem partition contains our largest memory node; only jobs that cannot be satisfied by day should run here. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition User Limits Walltime default/max Node type (number) day* 128 nodes 1h/1d BL460c (218) week 64 nodes 1h/7d BL460c (46), BL460c-extramem (16) interactive 1 job, 8 CPUs, 1 node 1h/4h BL460c (2) shared** 1h/1d BL460c (2) bigmem 1h/1d SL250s (4) scavenge 1h/7d all * default partition ** The shared partition is for jobs that require less than a full node of cores Private Partitions Private partitions contain nodes acquired by specific research groups. Full access to these partitions is granted at the discretion of the owner. Contact us if your group would like to purchase nodes. Partition Walltime default/max Node type (number) astro 1h/28d BL460c (112), BL460c-extramem (16) geo 1h/7d BL460c (207) hep 1h/7d BL460c (47) esi 1h/28d SL230s (60) Storage /gpfs/loomis is Omega's primary filesystem where home, and scratch60 directories are located. You can also access Grace's project space (if you have a Grace account) from Omega. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/loomis/home.omega 300G/group 500,000 Yes scratch60 /gpfs/loomis/scratch60 20T/group 5,000,000 No","title":"Omega"},{"location":"clusters-at-yale/clusters/omega/#omega","text":"Warning All of Omega's scratch space is read-and-delete-only as of Dec 3, 2018. The scratch space will be purged and all data permanently deleted on Feb 1, 2019. Omega has now served Yale\u2019s research community well for more than 2 years past the normal end-of-life for similar clusters. Most of its components are no longer under vendor warranty, and parts are sometimes difficult to obtain, so we are forced to support it on a best-effort basis. Last year, we developed a multi-year plan to replace Omega, which began with moving Omega\u2019s shared resources to our Grace cluster, for which we acquired new commons nodes. We plan to continue to support groups with dedicated node allocations and other users running tightly-coupled parallel jobs on Omega until Mid 2019. The mpi partition on Grace contains the replacement nodes the remainder of Omega. Please test your workload on those nodes are your convenience. We will provide ample warning before the final Omega decommission.","title":"Omega"},{"location":"clusters-at-yale/clusters/omega/#clean-out-omega-data","text":"All Omega files are now stored solely on the Loomis GPFS system. For groups that have migrated their workloads entirely to Grace or Farnam, their Omega data is now available from Grace and Farnam for copying and clean-up until Feb 2019. See Cleaning Out Omega Data for instructions on retrieving your data.","title":"Clean Out Omega Data"},{"location":"clusters-at-yale/clusters/omega/#hardware","text":"The cluster is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs.","title":"Hardware"},{"location":"clusters-at-yale/clusters/omega/#compute-node-configurations","text":"Count Node Type CPU CPU Cores RAM Features 668 HP ProLiant BL460c X5560 8 32G nehalem,sse4_2,X5560 32 HP ProLiant BL460c X5560 8 44G nehalem,sse4_2,X5560,extramem 4 HP ProLiant SL250s E5-2650 16 121G sandybridge,sse4_2,avx,E5-2650 60 HP ProLiant SL230s E5-2620 12 59G sandybridge,sse4_2,avx,E5-2620","title":"Compute Node Configurations"},{"location":"clusters-at-yale/clusters/omega/#slurm-partitions","text":"Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm .","title":"Slurm Partitions"},{"location":"clusters-at-yale/clusters/omega/#public-partitions","text":"The day partition is where most batch jobs should run, and is the default if you don't specify a partition. The week partition is smaller, but allows for longer jobs. The interactive partition should only be used for testing or compiling software. The bigmem partition contains our largest memory node; only jobs that cannot be satisfied by day should run here. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition User Limits Walltime default/max Node type (number) day* 128 nodes 1h/1d BL460c (218) week 64 nodes 1h/7d BL460c (46), BL460c-extramem (16) interactive 1 job, 8 CPUs, 1 node 1h/4h BL460c (2) shared** 1h/1d BL460c (2) bigmem 1h/1d SL250s (4) scavenge 1h/7d all * default partition ** The shared partition is for jobs that require less than a full node of cores","title":"Public Partitions"},{"location":"clusters-at-yale/clusters/omega/#private-partitions","text":"Private partitions contain nodes acquired by specific research groups. Full access to these partitions is granted at the discretion of the owner. Contact us if your group would like to purchase nodes. Partition Walltime default/max Node type (number) astro 1h/28d BL460c (112), BL460c-extramem (16) geo 1h/7d BL460c (207) hep 1h/7d BL460c (47) esi 1h/28d SL230s (60)","title":"Private Partitions"},{"location":"clusters-at-yale/clusters/omega/#storage","text":"/gpfs/loomis is Omega's primary filesystem where home, and scratch60 directories are located. You can also access Grace's project space (if you have a Grace account) from Omega. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/loomis/home.omega 300G/group 500,000 Yes scratch60 /gpfs/loomis/scratch60 20T/group 5,000,000 No","title":"Storage"},{"location":"clusters-at-yale/clusters/ruddle/","text":"Ruddle Ruddle is named for Frank Ruddle , a Yale geneticist who was a pioneer in genetic engineering and the study of developmental genetics. Ruddle is intended for use only on projects related to the Yale Center for Genome Analysis ; Please do not use this cluster for other projects. If you have any questions about this policy, please contact us . Hardware Ruddle is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. Warning Care should be taken if when scheduling your job if you are running programs/libraries optimized for specific hardware. See the guide on how to compile software for specific guidance. Compute Node Configurations Count Node Type CPU CPU Cores RAM Features 12 Dell PowerEdge M915 4x AMD Opteron 6276 32 507G bulldozer, sse4_2, avx, opteron-6276 155 Lenovo nx360h 2x E5-2660 v3 20 121G haswell, v3, sse4_2, avx, avx2, E5-2660_v3 2 Lenovo 3850X6 4x E7-4809 v3 32 1507G haswell, v3, sse4_2, avx, avx2, E7-4809_v3 Slurm Partitions Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The general partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The bigmem partition contains our largest memory nodes; only jobs that cannot be satisfied by general should run here. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition User Limits Walltime default/max Node type (number) general* 300 CPUs, 1800 G RAM 7d/30d nx360h (155) interactive 20 CPUs, 256 G RAM 1d/2d nx360h (155) bigmem 32 CPUs, 1507 G RAM 1d/7d m915 (12), 3850X6 (2) scavenge 800 CPUs, 5120 G RAM 1d/7d all * default Access Sequencing Data To avoid duplication of data and to save space that counts against your quotas, we suggest that you make soft links to your sequencing data rather than copying them: ln -s /path/to/sequece_data /path/to/your_link Tip Original sequence data are archived pursuant to the YCGA retention policy. For long-running projects we recommend you keep a personal backup of your sequence files. If you need to retrieve archived sequencing data, please see our guide on how to do so . To find the location of the sequence files on the storage, look at the URL that you were sent from YCGA. fullPath Starts With Root Path on Ruddle gpfs_illumina/sequencer /gpfs/ycga/illumina/sequencer ba_sequencers /ycga-ba/ba_sequencers sequencers /gpfs/ycga/sequencers/panfs/sequencers For example, if the sample link you received is: http://sysg1.cs.yale.edu:2011/gen?fullPath=sequencers2/sequencerV/runs/131107_D00306_0096... etc The path on the cluster to the data is: /gpfs/ycga/sequencers/panfs/sequencers2/sequencerV/runs/131107_D00306_0096... etc Public Datasets We host datasets of general interest in a loosely organized directory tree in /gpfs/ycga/datasets : \u251c\u2500\u2500 annovar \u2502 \u2514\u2500\u2500 humandb \u251c\u2500\u2500 db \u2502 \u2514\u2500\u2500 blast \u251c\u2500\u2500 genomes \u2502 \u251c\u2500\u2500 Aedes_aegypti \u2502 \u251c\u2500\u2500 Bos_taurus \u2502 \u251c\u2500\u2500 Chelonoidis_nigra \u2502 \u251c\u2500\u2500 Danio_rerio \u2502 \u251c\u2500\u2500 Gallus_gallus \u2502 \u251c\u2500\u2500 hisat2 \u2502 \u251c\u2500\u2500 Homo_sapiens \u2502 \u251c\u2500\u2500 Macaca_mulatta \u2502 \u251c\u2500\u2500 Monodelphis_domestica \u2502 \u251c\u2500\u2500 Mus_musculus \u2502 \u251c\u2500\u2500 PhiX \u2502 \u2514\u2500\u2500 tmp \u2514\u2500\u2500 hisat2 \u2514\u2500\u2500 mouse If you would like us to host a dataset or questions about what is currently available, please email hpc@yale.edu. Storage Ruddle has access to two filesystems. /gpfs/ycga is Ruddle's filesystem where home, project, and scratch60 directories are located. /ycga-ba stores legacy data. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily.. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/ycga/home 125G/user 500,000 Yes project /gpfs/ycga/project 4T/group 5,000,000 No scratch60 /gpfs/ycga/scratch60 10T/group 5,000,000 No","title":"Ruddle"},{"location":"clusters-at-yale/clusters/ruddle/#ruddle","text":"Ruddle is named for Frank Ruddle , a Yale geneticist who was a pioneer in genetic engineering and the study of developmental genetics. Ruddle is intended for use only on projects related to the Yale Center for Genome Analysis ; Please do not use this cluster for other projects. If you have any questions about this policy, please contact us .","title":"Ruddle"},{"location":"clusters-at-yale/clusters/ruddle/#hardware","text":"Ruddle is made up of several kinds of compute nodes. The Features column below lists the features that can be used to request different node types using the --constraints flag (see our Slurm documentation for more details). The RAM listed below is the amount of memory available for jobs. Warning Care should be taken if when scheduling your job if you are running programs/libraries optimized for specific hardware. See the guide on how to compile software for specific guidance.","title":"Hardware"},{"location":"clusters-at-yale/clusters/ruddle/#compute-node-configurations","text":"Count Node Type CPU CPU Cores RAM Features 12 Dell PowerEdge M915 4x AMD Opteron 6276 32 507G bulldozer, sse4_2, avx, opteron-6276 155 Lenovo nx360h 2x E5-2660 v3 20 121G haswell, v3, sse4_2, avx, avx2, E5-2660_v3 2 Lenovo 3850X6 4x E7-4809 v3 32 1507G haswell, v3, sse4_2, avx, avx2, E7-4809_v3","title":"Compute Node Configurations"},{"location":"clusters-at-yale/clusters/ruddle/#slurm-partitions","text":"Nodes on the clusters are organized into partitions, to which you submit your jobs with Slurm . The general partition is where most batch jobs should run, and is the default if you don't specify a partition. The interactive partition is dedicated to jobs with which you need ongoing interaction. The bigmem partition contains our largest memory nodes; only jobs that cannot be satisfied by general should run here. The scavenge partition allows you to run preemptable jobs on more resources than normally allowed. For more information about scavenge, see the Scavenge documentation . The limits listed below are for all running jobs combined. Per-node limits are bound by the node types, as described in the hardware table. Partition User Limits Walltime default/max Node type (number) general* 300 CPUs, 1800 G RAM 7d/30d nx360h (155) interactive 20 CPUs, 256 G RAM 1d/2d nx360h (155) bigmem 32 CPUs, 1507 G RAM 1d/7d m915 (12), 3850X6 (2) scavenge 800 CPUs, 5120 G RAM 1d/7d all * default","title":"Slurm Partitions"},{"location":"clusters-at-yale/clusters/ruddle/#access-sequencing-data","text":"To avoid duplication of data and to save space that counts against your quotas, we suggest that you make soft links to your sequencing data rather than copying them: ln -s /path/to/sequece_data /path/to/your_link Tip Original sequence data are archived pursuant to the YCGA retention policy. For long-running projects we recommend you keep a personal backup of your sequence files. If you need to retrieve archived sequencing data, please see our guide on how to do so . To find the location of the sequence files on the storage, look at the URL that you were sent from YCGA. fullPath Starts With Root Path on Ruddle gpfs_illumina/sequencer /gpfs/ycga/illumina/sequencer ba_sequencers /ycga-ba/ba_sequencers sequencers /gpfs/ycga/sequencers/panfs/sequencers For example, if the sample link you received is: http://sysg1.cs.yale.edu:2011/gen?fullPath=sequencers2/sequencerV/runs/131107_D00306_0096... etc The path on the cluster to the data is: /gpfs/ycga/sequencers/panfs/sequencers2/sequencerV/runs/131107_D00306_0096... etc","title":"Access Sequencing Data"},{"location":"clusters-at-yale/clusters/ruddle/#public-datasets","text":"We host datasets of general interest in a loosely organized directory tree in /gpfs/ycga/datasets : \u251c\u2500\u2500 annovar \u2502 \u2514\u2500\u2500 humandb \u251c\u2500\u2500 db \u2502 \u2514\u2500\u2500 blast \u251c\u2500\u2500 genomes \u2502 \u251c\u2500\u2500 Aedes_aegypti \u2502 \u251c\u2500\u2500 Bos_taurus \u2502 \u251c\u2500\u2500 Chelonoidis_nigra \u2502 \u251c\u2500\u2500 Danio_rerio \u2502 \u251c\u2500\u2500 Gallus_gallus \u2502 \u251c\u2500\u2500 hisat2 \u2502 \u251c\u2500\u2500 Homo_sapiens \u2502 \u251c\u2500\u2500 Macaca_mulatta \u2502 \u251c\u2500\u2500 Monodelphis_domestica \u2502 \u251c\u2500\u2500 Mus_musculus \u2502 \u251c\u2500\u2500 PhiX \u2502 \u2514\u2500\u2500 tmp \u2514\u2500\u2500 hisat2 \u2514\u2500\u2500 mouse If you would like us to host a dataset or questions about what is currently available, please email hpc@yale.edu.","title":"Public Datasets"},{"location":"clusters-at-yale/clusters/ruddle/#storage","text":"Ruddle has access to two filesystems. /gpfs/ycga is Ruddle's filesystem where home, project, and scratch60 directories are located. /ycga-ba stores legacy data. For more details on the different storage spaces, see our Cluster Storage documentation. You can check your current storage usage & limits by running the getquota command. Note that the per-user usage breakdown only update once daily.. Warning Files stored in scratch60 are purged if they are older than 60 days. You will receive an email alert one week before they are deleted. Partition Root Directory Storage File Count Backups home /gpfs/ycga/home 125G/user 500,000 Yes project /gpfs/ycga/project 4T/group 5,000,000 No scratch60 /gpfs/ycga/scratch60 10T/group 5,000,000 No","title":"Storage"},{"location":"clusters-at-yale/data/","text":"Overview Each cluster has a small amount of space dedicated to home directories, meant for notes and scripts. There are also project and scratch spaces for larger and more numerous files. Each group is given a free allocation in each of these storage locations (see the individual cluster pages for details). Contact us at hpc@yale.edu about purchasing additional cluster storage if your needs exceed your free allocation. Other Storage Options If you or your group finds the HPC storage do not accommodate your needs, please see the off-cluster research data storage page for other options.","title":"Overview"},{"location":"clusters-at-yale/data/#overview","text":"Each cluster has a small amount of space dedicated to home directories, meant for notes and scripts. There are also project and scratch spaces for larger and more numerous files. Each group is given a free allocation in each of these storage locations (see the individual cluster pages for details). Contact us at hpc@yale.edu about purchasing additional cluster storage if your needs exceed your free allocation.","title":"Overview"},{"location":"clusters-at-yale/data/#other-storage-options","text":"If you or your group finds the HPC storage do not accommodate your needs, please see the off-cluster research data storage page for other options.","title":"Other Storage Options"},{"location":"clusters-at-yale/data/archived-sequencing/","text":"Ruddle Archived Sequence Data Retrieve Data from the Archive In the sequencing archive on Ruddle , a directory exists for each run, holding one or more tar files. There is a main tar file, plus a tar file for each project directory. Most users only need the project tar file corresponding to their data. Although the archive actually exists on tape, you can treat it as a regular directory tree. Many operations such as ls , cd , etc. are very fast, since directory structures and file metadata are on a disk cache. However, when you actually read the contents of files the tape is mounted and the file is read into a disk cache. Archived runs are stored in the following locations. Original location Archive location /panfs/sequencers /SAY/archive/YCGA-729009-YCGA/archive/panfs/sequencers /ycga-ba/ba_sequencers /SAY/archive/YCGA-729009-YCGA/archive/ycga-ba/ba_sequencers /gpfs/ycga/sequencers/illumina/sequencers /SAY/archive/YCGA-729009-YCGA/archive/ycga-gpfs/sequencers/illumina/sequencers You can directly copy or untar the project tarfile into a scratch directory. cd ~/scratch60/somedir tar \u2013xvf /SAY/archive/YCGA-729009-YCGA/archive/path/to/file.tar Inside the project tar files are the fastq files, which have been compressed using quip . If your pipeline cannot read quip files directly, you will need to uncompress them before using them. module load Quip quip \u2013d M20_ACAGTG_L008_R1_009.fastq.qp For your convenience, we have a tool, restore , that will download a tar file, untar it, and uncompress all quip files. module load ycga-public restore \u2013t /SAY/archive/YCGA-729009-YCGA/archive/path/to/file.tar If you have trouble locating your files, you can use the utility locateRun , using any substring of the original run name. locateRun is in the same module as restore. locateRun C9374AN Restore spends most of the time running quip. You can parallelize and thereby speed up that process using the -n flag. restore \u2013n 20 ... Tip When retrieving data, run untar/unquip as a job on a compute node, not a login node and make sure to allocate sufficient resources to your job, e.g. \u2013c 20 --mem=100G .","title":"Ruddle Archived Sequence Data"},{"location":"clusters-at-yale/data/archived-sequencing/#ruddle-archived-sequence-data","text":"","title":"Ruddle Archived Sequence Data"},{"location":"clusters-at-yale/data/archived-sequencing/#retrieve-data-from-the-archive","text":"In the sequencing archive on Ruddle , a directory exists for each run, holding one or more tar files. There is a main tar file, plus a tar file for each project directory. Most users only need the project tar file corresponding to their data. Although the archive actually exists on tape, you can treat it as a regular directory tree. Many operations such as ls , cd , etc. are very fast, since directory structures and file metadata are on a disk cache. However, when you actually read the contents of files the tape is mounted and the file is read into a disk cache. Archived runs are stored in the following locations. Original location Archive location /panfs/sequencers /SAY/archive/YCGA-729009-YCGA/archive/panfs/sequencers /ycga-ba/ba_sequencers /SAY/archive/YCGA-729009-YCGA/archive/ycga-ba/ba_sequencers /gpfs/ycga/sequencers/illumina/sequencers /SAY/archive/YCGA-729009-YCGA/archive/ycga-gpfs/sequencers/illumina/sequencers You can directly copy or untar the project tarfile into a scratch directory. cd ~/scratch60/somedir tar \u2013xvf /SAY/archive/YCGA-729009-YCGA/archive/path/to/file.tar Inside the project tar files are the fastq files, which have been compressed using quip . If your pipeline cannot read quip files directly, you will need to uncompress them before using them. module load Quip quip \u2013d M20_ACAGTG_L008_R1_009.fastq.qp For your convenience, we have a tool, restore , that will download a tar file, untar it, and uncompress all quip files. module load ycga-public restore \u2013t /SAY/archive/YCGA-729009-YCGA/archive/path/to/file.tar If you have trouble locating your files, you can use the utility locateRun , using any substring of the original run name. locateRun is in the same module as restore. locateRun C9374AN Restore spends most of the time running quip. You can parallelize and thereby speed up that process using the -n flag. restore \u2013n 20 ... Tip When retrieving data, run untar/unquip as a job on a compute node, not a login node and make sure to allocate sufficient resources to your job, e.g. \u2013c 20 --mem=100G .","title":"Retrieve Data from the Archive"},{"location":"clusters-at-yale/data/cluster-storage/","text":"Cluster Storage Each research group is provided with storage space for research data on the GPFS parallel filesytems on the clusters. The storage is separated into three tiers: home, project, and scratch. You can monitor your storage usage by running the getquota command on a cluster. Warning The only storage backed up on every cluster is home . HPC Storage Locations Home Home storage is a small amount of space to store your scripts, notes, final products (e.g. figures), etc. Home storage is backed up daily. Project In general, project storage is intended to be the primary storage location for HPC research data in active use. 60-Day Scratch ( scratch60 ) Use this space to keep intermediate files that can be regenerated/reconstituted if necessary. Files older than 60 days will be deleted automatically . This space is not backed up, and you may be asked to delete files younger than 60 days old if this space fills up. HPC Storage Best Practices Prevent Large Numbers of Small Files Parallel fileystems, like the ones attached to our clusters, perform poorly with very large numbers of small files. For this reason, there are file count limits on all accounts to provide a safety net against excessive file creation. However, we expect users to manage their own file counts by altering workflows to reduce file creation, deleting unneeded files, and compressing (using tar ) collections of files no longer in use. Backups and Snapshots Retrieve Data from Home Backups Contact us at hpc@yale.edu with your netid and the list of files/directories you would like restored. Retrieve Data from Snapshots (Farnam & Milgram) Farnam and Milgram all run snapshots nightly on portions of their filesystems so that you can retrieve mistakenly modified or removed files for yourself. As long as your files existed in the form you want them in before the most recent midnight, they can probably be recovered. Snapshot directory structure mirrors the files that are being tracked with a prefix, listed in the table below. File set Snapshot Prefix /gpfs/ysm /gpfs/ysm/.snapshots /gpfs/slaymanpi/gerstein /gpfs/slayman/pi/gerstein/.snapshots /gpfs/milgram/home /gpfs/milgram/home/.snapshots /gpfs/milgram/project /gpfs/milgram/project/groupname/.snapshots For example, if you wanted to recover the most recent snapshot of the file /gpfs/ysm/home/rdb9/scripts/doit.sh , its path would be /gpfs/ysm/.snapshots/$(date +%Y%m%d-0000)/home/rdb9/scripts/doit.sh . Similarly, the file /gpfs/milgram/project/bjornson/rdb9/doit.sh (a file in the bjornson group's project directory owned by rdb9) could be recovered at /gpfs/milgram/project/bjornson/.snapshots/$(date +%Y%m%d-0000)/rdb9/doit.sh . Info Because of the way snapshots are stored, sizes will not be correctly reported until you copy your files/directories back out of the .snapshots directory.","title":"Cluster Storage"},{"location":"clusters-at-yale/data/cluster-storage/#cluster-storage","text":"Each research group is provided with storage space for research data on the GPFS parallel filesytems on the clusters. The storage is separated into three tiers: home, project, and scratch. You can monitor your storage usage by running the getquota command on a cluster. Warning The only storage backed up on every cluster is home .","title":"Cluster Storage"},{"location":"clusters-at-yale/data/cluster-storage/#hpc-storage-locations","text":"","title":"HPC Storage Locations"},{"location":"clusters-at-yale/data/cluster-storage/#home","text":"Home storage is a small amount of space to store your scripts, notes, final products (e.g. figures), etc. Home storage is backed up daily.","title":"Home"},{"location":"clusters-at-yale/data/cluster-storage/#project","text":"In general, project storage is intended to be the primary storage location for HPC research data in active use.","title":"Project"},{"location":"clusters-at-yale/data/cluster-storage/#60-day-scratch-scratch60","text":"Use this space to keep intermediate files that can be regenerated/reconstituted if necessary. Files older than 60 days will be deleted automatically . This space is not backed up, and you may be asked to delete files younger than 60 days old if this space fills up.","title":"60-Day Scratch (scratch60)"},{"location":"clusters-at-yale/data/cluster-storage/#hpc-storage-best-practices","text":"","title":"HPC Storage Best Practices"},{"location":"clusters-at-yale/data/cluster-storage/#prevent-large-numbers-of-small-files","text":"Parallel fileystems, like the ones attached to our clusters, perform poorly with very large numbers of small files. For this reason, there are file count limits on all accounts to provide a safety net against excessive file creation. However, we expect users to manage their own file counts by altering workflows to reduce file creation, deleting unneeded files, and compressing (using tar ) collections of files no longer in use.","title":"Prevent Large Numbers of Small Files"},{"location":"clusters-at-yale/data/cluster-storage/#backups-and-snapshots","text":"","title":"Backups and Snapshots"},{"location":"clusters-at-yale/data/cluster-storage/#retrieve-data-from-home-backups","text":"Contact us at hpc@yale.edu with your netid and the list of files/directories you would like restored.","title":"Retrieve Data from Home Backups"},{"location":"clusters-at-yale/data/cluster-storage/#retrieve-data-from-snapshots-farnam-milgram","text":"Farnam and Milgram all run snapshots nightly on portions of their filesystems so that you can retrieve mistakenly modified or removed files for yourself. As long as your files existed in the form you want them in before the most recent midnight, they can probably be recovered. Snapshot directory structure mirrors the files that are being tracked with a prefix, listed in the table below. File set Snapshot Prefix /gpfs/ysm /gpfs/ysm/.snapshots /gpfs/slaymanpi/gerstein /gpfs/slayman/pi/gerstein/.snapshots /gpfs/milgram/home /gpfs/milgram/home/.snapshots /gpfs/milgram/project /gpfs/milgram/project/groupname/.snapshots For example, if you wanted to recover the most recent snapshot of the file /gpfs/ysm/home/rdb9/scripts/doit.sh , its path would be /gpfs/ysm/.snapshots/$(date +%Y%m%d-0000)/home/rdb9/scripts/doit.sh . Similarly, the file /gpfs/milgram/project/bjornson/rdb9/doit.sh (a file in the bjornson group's project directory owned by rdb9) could be recovered at /gpfs/milgram/project/bjornson/.snapshots/$(date +%Y%m%d-0000)/rdb9/doit.sh . Info Because of the way snapshots are stored, sizes will not be correctly reported until you copy your files/directories back out of the .snapshots directory.","title":"Retrieve Data from Snapshots (Farnam &amp; Milgram)"},{"location":"clusters-at-yale/data/omega-data/","text":"Clean Out Omega Data All Omega files are now stored solely on the Loomis GPFS system. Omega data can be accessed from Grace and Farnam for copying and clean-up until February 1, 2019. If you have data on Omega that you would like to keep, you will need to move it to a new location before February 1, 2019, after which it will be permanently deleted. Omega data can be accessed from the other clusters at the following paths until that date: /gpfs/loomis/home.omega/<metagroup>/<group>/<netid> /gpfs/loomis/scratch.omega/<metagroup>/<group>/<netid> You can run the mydirectories command from the clusters for the proper paths of your directories. Note that Omega logins have been disable for most users, so you will need follow the instructions below to access your data. Those users who still have access to Omega, your scratch space will become read-and-delete-only on Monday, December 3, 2018. To continue to compute, please redirect your job output to your Grace project or scratch60 spaces. Gaussian users, see below for tips on redirecting Gaussian's output. If you are a PI and would like access to data owned by any former group members, please send us an email at hpc@yale.edu and we can assist you. Review Usage From either Grace or Farnam, you can run the groupquota script with the -c omega flag to get a report of your group's storage usage on Omega. getquota -c omega If you don't have an account on Grace or Farnam, you can email us at hpc@yale.edu to get a list of usage and see below for instructions on retrieving your data. Transferring Data from Omega to Grace or Farnam Since the Loomis is available on Grace and Farnam, there is no need to use Globus or rsync that data between transfer nodes. Instead, from the cluster where you will be storing the data moving forward, you can run an rsync directly between the filesets (directory trees). Once you have copied your data from Omega to your other cluster, please delete the data from the Omega directories . For example, if you would like to copy your data into a new directory in your project space: >mkdir ~/project/data_from_omega rsync -avz /gpfs/loomis/scratch.omega/fas/hpcprog/kln26/simulation100 ~/project/data_from_omega # verify data has been copied successfully to ~/project/data_from_omega, then rm -r /gpfs/loomis/scratch.omega/fas/hpcprog/kln26/simulation100 Again, please delete any data you have copied off the Omega storage spaces or determined you no longer need. Transferring Data from Omega off of Cluster Storage If you had an account on Omega but don't use either of Grace or Farnam, you can use Globus to transfer data from the yale#omega endpoint. Please see our data transfer documentation for instructions on transferring to local storage. Please see our Google Drive documentation for instructions on transferring to Google Drive. Again, please delete any data you have copied off the Omega storage spaces or determined you no longer need, which you should be able to do from the Globus interface.","title":"Clean Out Omega Data"},{"location":"clusters-at-yale/data/omega-data/#clean-out-omega-data","text":"All Omega files are now stored solely on the Loomis GPFS system. Omega data can be accessed from Grace and Farnam for copying and clean-up until February 1, 2019. If you have data on Omega that you would like to keep, you will need to move it to a new location before February 1, 2019, after which it will be permanently deleted. Omega data can be accessed from the other clusters at the following paths until that date: /gpfs/loomis/home.omega/<metagroup>/<group>/<netid> /gpfs/loomis/scratch.omega/<metagroup>/<group>/<netid> You can run the mydirectories command from the clusters for the proper paths of your directories. Note that Omega logins have been disable for most users, so you will need follow the instructions below to access your data. Those users who still have access to Omega, your scratch space will become read-and-delete-only on Monday, December 3, 2018. To continue to compute, please redirect your job output to your Grace project or scratch60 spaces. Gaussian users, see below for tips on redirecting Gaussian's output. If you are a PI and would like access to data owned by any former group members, please send us an email at hpc@yale.edu and we can assist you.","title":"Clean Out Omega Data"},{"location":"clusters-at-yale/data/omega-data/#review-usage","text":"From either Grace or Farnam, you can run the groupquota script with the -c omega flag to get a report of your group's storage usage on Omega. getquota -c omega If you don't have an account on Grace or Farnam, you can email us at hpc@yale.edu to get a list of usage and see below for instructions on retrieving your data.","title":"Review Usage"},{"location":"clusters-at-yale/data/omega-data/#transferring-data-from-omega-to-grace-or-farnam","text":"Since the Loomis is available on Grace and Farnam, there is no need to use Globus or rsync that data between transfer nodes. Instead, from the cluster where you will be storing the data moving forward, you can run an rsync directly between the filesets (directory trees). Once you have copied your data from Omega to your other cluster, please delete the data from the Omega directories . For example, if you would like to copy your data into a new directory in your project space: >mkdir ~/project/data_from_omega rsync -avz /gpfs/loomis/scratch.omega/fas/hpcprog/kln26/simulation100 ~/project/data_from_omega # verify data has been copied successfully to ~/project/data_from_omega, then rm -r /gpfs/loomis/scratch.omega/fas/hpcprog/kln26/simulation100 Again, please delete any data you have copied off the Omega storage spaces or determined you no longer need.","title":"Transferring Data from Omega to Grace or Farnam"},{"location":"clusters-at-yale/data/omega-data/#transferring-data-from-omega-off-of-cluster-storage","text":"If you had an account on Omega but don't use either of Grace or Farnam, you can use Globus to transfer data from the yale#omega endpoint. Please see our data transfer documentation for instructions on transferring to local storage. Please see our Google Drive documentation for instructions on transferring to Google Drive. Again, please delete any data you have copied off the Omega storage spaces or determined you no longer need, which you should be able to do from the Globus interface.","title":"Transferring Data from Omega off of Cluster Storage"},{"location":"clusters-at-yale/data/permissions/","text":"Manage Permissions for Sharing Home Directories Do not give your home directory group write permissions. This will break your ability to log into the cluster. If you need to share something in your home directory, either move it your project directory or ask hpc@yale.edu for assistance. Shared Group Directories Upon request we can setup directories for sharing scripts or data across your research group. These directories can either have read-only permissions for the group (so no one accidentally modifies something) or read and write permissions for shared data directories. If interested, email us at hpc@yale.edu to request such a directory. Share With Specific Users or Other Groups It can be very useful to create shared directories that can be read and written by multiple users, or all members of a group. The linux command setfacl is useful for this, but can be complicated to use. Here are some simple scenarios. We recommend that you create a shared directory somewhere in your project or scratch60 directories, rather than home . Warning Your ~/project and ~/scratch60 directories are actually symlinks (shortcuts) to elsewhere on the filesystem. Either run mydirectories or readlink - f dirname (replace dirname with the one you're interested in) to get their true paths. Give these true to people you are sharing with. Share a Directory with All Members of a Group To share a new directory called shared with group othergroup : mkdir shared setfacl -d -m g:othergroup:rwx shared Share a Directory with a Particular Person To share a new directory called shared with a person with netid aa111 : mkdir shared setfacl -d -m u:aa111:rwx shared","title":"Manage Permissions for Sharing"},{"location":"clusters-at-yale/data/permissions/#manage-permissions-for-sharing","text":"","title":"Manage Permissions for Sharing"},{"location":"clusters-at-yale/data/permissions/#home-directories","text":"Do not give your home directory group write permissions. This will break your ability to log into the cluster. If you need to share something in your home directory, either move it your project directory or ask hpc@yale.edu for assistance.","title":"Home Directories"},{"location":"clusters-at-yale/data/permissions/#shared-group-directories","text":"Upon request we can setup directories for sharing scripts or data across your research group. These directories can either have read-only permissions for the group (so no one accidentally modifies something) or read and write permissions for shared data directories. If interested, email us at hpc@yale.edu to request such a directory.","title":"Shared Group Directories"},{"location":"clusters-at-yale/data/permissions/#share-with-specific-users-or-other-groups","text":"It can be very useful to create shared directories that can be read and written by multiple users, or all members of a group. The linux command setfacl is useful for this, but can be complicated to use. Here are some simple scenarios. We recommend that you create a shared directory somewhere in your project or scratch60 directories, rather than home . Warning Your ~/project and ~/scratch60 directories are actually symlinks (shortcuts) to elsewhere on the filesystem. Either run mydirectories or readlink - f dirname (replace dirname with the one you're interested in) to get their true paths. Give these true to people you are sharing with.","title":"Share With Specific Users or Other Groups"},{"location":"clusters-at-yale/data/permissions/#share-a-directory-with-all-members-of-a-group","text":"To share a new directory called shared with group othergroup : mkdir shared setfacl -d -m g:othergroup:rwx shared","title":"Share a Directory with All Members of a Group"},{"location":"clusters-at-yale/data/permissions/#share-a-directory-with-a-particular-person","text":"To share a new directory called shared with a person with netid aa111 : mkdir shared setfacl -d -m u:aa111:rwx shared","title":"Share a Directory with a Particular Person"},{"location":"clusters-at-yale/data/transfer/","text":"Transfer Data Transferring files can be done using several methods. The most common are described below. scp/rsync (macOS/Linux only) Linux and Apple macOS users can use scp or rsync to transfer files to/from a cluster. You will need the hostname of the cluster login nodes to transfer files. Note that you must have your ssh keys properly configured to use the commands outlined below. See the Cluster Access documentation for more info. scp and sftp are both used from a Terminal window. The basic syntax of scp is scp [from] [to] The \u201cfrom\u201d portion can be a filename or a directory/folder. The \u201cto\u201d portion will contain your netid, the hostname of the cluster login node, and the destination directory/folder. Transfer a File from Your Computer to a Cluster Assuming the user\u2019s netid is ra359 , the following is run on your computer's local terminal. scp myfile.txt ra359@grace.hpc.yale.edu:/home/fas/admins/ra359/test In this example, myfile.txt is copied to the directory /home/fas/admins/ra359/test: on Grace. This example assumes that myfile.txt is in your current directory. You may also specify the full path of myfile.txt . scp /home/xyz/myfile.txt ra359@grace.hpc.yale.edu:/home/fas/admins/ra359/test Transfer a Directory to a Cluster scp -r mydirectory ra359@grace.hpc.yale.edu:/home/fas/admins/ra359/test In this example, the contents of mydirectory are transferred. The -r indicates that the copy is recursive. Transfer Files from the Cluster to Your Computer Assuming you would like the files copied to your current directory: scp ra359@grace.hpc.yale.edu:/home/fas/admins/ra359/myfile.txt . Note that . represents your current working directory. To specify the destination, simply replace the . with the full path: scp ra359@grace.hpc.yale.edu:/home/fas/admins/ra359/myfile.txt /path/myfolder FTP Client You can also transfer files between your local computer and a cluster using an FTP client, such as Cyberduck (OSX/Windows) or FileZilla (Linux) . You will need to configure the client with your netid as the username, the cluster login node as the hostname and your private key as the authentication method. An example configuration of Cyberduck is shown below. FTP on Ruddle If you are connecting to Ruddle, which requires Multi-Factor Authentication , there are a couple additional steps. You will need to change the Cyberduck preferences to \"Use browser connection\" instead of \"open multiple connections\". This can be found under Cyberduck > Preferences > Transfers > General . Then once you establish your connection, you will prompted with a \"Partial authentication success\" window. In the password field, type in \"push\" to receive a push approval notification to the Duo app on your phone. Alternate multi-factor authentications can be used by enter the following words in the password field: \"push\" to receive a push notification to your smart phone (requires the Duo mobile app) \"sms\" to receive a verification passcode via text message \"phone\" to receive a phone call Large Transfers (Globus) For larger transfers both within Yale and to external collaborators, we recommend using Globus. Globus is a file transfer service that is efficient and easy to use. It has several advantages: Globus can robustly and quickly transfer large files, and large collections of files Files can be transferred between your computer and the clusters Files can be transferred between Yale and other sites There is a simple web interface for starting and monitoring transfers, as well as a command line interface. You can provide access to specific files or directories to non-Yale people in a secure way. We have set up Globus endpoints on most of the Yale clusters. Globus uses gridftp to perform transfers in parallel. Globus works a bit differently than other transfer services such as ftp or rsync. With Globus, files are always transferred between two \"endpoints\". One endpoint is always a Globus server, such as the ones we've set up on the clusters. The other endpoint can be a second server, or a Globus connect personal endpoint, which is a desktop application. \u200bGet Started with Globus In a browser, go to www.globus.org. Click on \"Login\". Use the pulldown to select Yale in the list of organizations and click \"Continue\". If you are not already logged into CAS, you will be asked for netid and password. You'll see a transfer panel with dual panes. Enter an endpoint name in the left endpoint box, e.g. yale#grace. The file browser will show you the directories in the root directory that Globus is exporting, normally / Browse to any directory you can normally access, such as your home directory. Enter another endpoint name in the right endpoint box, and browse to your chosen directory. Select one or more files in either the left or right box, and click the < or > button to transfer the files in that direction. For more information, see the official Globus Documentation . Cluster Endpoints We currently support endpoints for the following clusters Grace: yale#grace Farnam: yale#farnam Omega: yale#omega Ruddle: yale#ruddle All of these endpoints provide access to all files you normally have access to, except sequencing data on Ruddle. Google Drive Endpoints See our Google Drive Documentation for instructions for using Globus to transfer data to Eliapps Google Drive. Setup an Endpoint on Your Own Computer You can set up your own endpoint for transferring data to and from your own computer. This is called Globus Connect, and you can find instructions here . Share Data with non-Yale Collaborators Among Globus' greatest features is the ability to allow collaborators at other institutions access to specific files or directories. This is done by creating a \"shared endpoint\" and setting the permissions on that endpoint. Please the official Globus documentation on sharing for detailed instructions.","title":"Transfer Data"},{"location":"clusters-at-yale/data/transfer/#transfer-data","text":"Transferring files can be done using several methods. The most common are described below.","title":"Transfer Data"},{"location":"clusters-at-yale/data/transfer/#scprsync-macoslinux-only","text":"Linux and Apple macOS users can use scp or rsync to transfer files to/from a cluster. You will need the hostname of the cluster login nodes to transfer files. Note that you must have your ssh keys properly configured to use the commands outlined below. See the Cluster Access documentation for more info. scp and sftp are both used from a Terminal window. The basic syntax of scp is scp [from] [to] The \u201cfrom\u201d portion can be a filename or a directory/folder. The \u201cto\u201d portion will contain your netid, the hostname of the cluster login node, and the destination directory/folder.","title":"scp/rsync (macOS/Linux only)"},{"location":"clusters-at-yale/data/transfer/#transfer-a-file-from-your-computer-to-a-cluster","text":"Assuming the user\u2019s netid is ra359 , the following is run on your computer's local terminal. scp myfile.txt ra359@grace.hpc.yale.edu:/home/fas/admins/ra359/test In this example, myfile.txt is copied to the directory /home/fas/admins/ra359/test: on Grace. This example assumes that myfile.txt is in your current directory. You may also specify the full path of myfile.txt . scp /home/xyz/myfile.txt ra359@grace.hpc.yale.edu:/home/fas/admins/ra359/test","title":"Transfer a File from Your Computer to a Cluster"},{"location":"clusters-at-yale/data/transfer/#transfer-a-directory-to-a-cluster","text":"scp -r mydirectory ra359@grace.hpc.yale.edu:/home/fas/admins/ra359/test In this example, the contents of mydirectory are transferred. The -r indicates that the copy is recursive.","title":"Transfer a Directory to a Cluster"},{"location":"clusters-at-yale/data/transfer/#transfer-files-from-the-cluster-to-your-computer","text":"Assuming you would like the files copied to your current directory: scp ra359@grace.hpc.yale.edu:/home/fas/admins/ra359/myfile.txt . Note that . represents your current working directory. To specify the destination, simply replace the . with the full path: scp ra359@grace.hpc.yale.edu:/home/fas/admins/ra359/myfile.txt /path/myfolder","title":"Transfer Files from the Cluster to Your Computer"},{"location":"clusters-at-yale/data/transfer/#ftp-client","text":"You can also transfer files between your local computer and a cluster using an FTP client, such as Cyberduck (OSX/Windows) or FileZilla (Linux) . You will need to configure the client with your netid as the username, the cluster login node as the hostname and your private key as the authentication method. An example configuration of Cyberduck is shown below.","title":"FTP Client"},{"location":"clusters-at-yale/data/transfer/#ftp-on-ruddle","text":"If you are connecting to Ruddle, which requires Multi-Factor Authentication , there are a couple additional steps. You will need to change the Cyberduck preferences to \"Use browser connection\" instead of \"open multiple connections\". This can be found under Cyberduck > Preferences > Transfers > General . Then once you establish your connection, you will prompted with a \"Partial authentication success\" window. In the password field, type in \"push\" to receive a push approval notification to the Duo app on your phone. Alternate multi-factor authentications can be used by enter the following words in the password field: \"push\" to receive a push notification to your smart phone (requires the Duo mobile app) \"sms\" to receive a verification passcode via text message \"phone\" to receive a phone call","title":"FTP on Ruddle"},{"location":"clusters-at-yale/data/transfer/#large-transfers-globus","text":"For larger transfers both within Yale and to external collaborators, we recommend using Globus. Globus is a file transfer service that is efficient and easy to use. It has several advantages: Globus can robustly and quickly transfer large files, and large collections of files Files can be transferred between your computer and the clusters Files can be transferred between Yale and other sites There is a simple web interface for starting and monitoring transfers, as well as a command line interface. You can provide access to specific files or directories to non-Yale people in a secure way. We have set up Globus endpoints on most of the Yale clusters. Globus uses gridftp to perform transfers in parallel. Globus works a bit differently than other transfer services such as ftp or rsync. With Globus, files are always transferred between two \"endpoints\". One endpoint is always a Globus server, such as the ones we've set up on the clusters. The other endpoint can be a second server, or a Globus connect personal endpoint, which is a desktop application.","title":"Large Transfers (Globus)"},{"location":"clusters-at-yale/data/transfer/#get-started-with-globus","text":"In a browser, go to www.globus.org. Click on \"Login\". Use the pulldown to select Yale in the list of organizations and click \"Continue\". If you are not already logged into CAS, you will be asked for netid and password. You'll see a transfer panel with dual panes. Enter an endpoint name in the left endpoint box, e.g. yale#grace. The file browser will show you the directories in the root directory that Globus is exporting, normally / Browse to any directory you can normally access, such as your home directory. Enter another endpoint name in the right endpoint box, and browse to your chosen directory. Select one or more files in either the left or right box, and click the < or > button to transfer the files in that direction. For more information, see the official Globus Documentation .","title":"\u200bGet Started with Globus"},{"location":"clusters-at-yale/data/transfer/#cluster-endpoints","text":"We currently support endpoints for the following clusters Grace: yale#grace Farnam: yale#farnam Omega: yale#omega Ruddle: yale#ruddle All of these endpoints provide access to all files you normally have access to, except sequencing data on Ruddle.","title":"Cluster Endpoints"},{"location":"clusters-at-yale/data/transfer/#google-drive-endpoints","text":"See our Google Drive Documentation for instructions for using Globus to transfer data to Eliapps Google Drive.","title":"Google Drive Endpoints"},{"location":"clusters-at-yale/data/transfer/#setup-an-endpoint-on-your-own-computer","text":"You can set up your own endpoint for transferring data to and from your own computer. This is called Globus Connect, and you can find instructions here .","title":"Setup an Endpoint on Your Own Computer"},{"location":"clusters-at-yale/data/transfer/#share-data-with-non-yale-collaborators","text":"Among Globus' greatest features is the ability to allow collaborators at other institutions access to specific files or directories. This is done by creating a \"shared endpoint\" and setting the permissions on that endpoint. Please the official Globus documentation on sharing for detailed instructions.","title":"Share Data with non-Yale Collaborators"},{"location":"clusters-at-yale/guides/","text":"Overview We provide guides for running certain software packages on our clusters. If you have tips for running a commonly used software and would like to contribute them to our Software Guides, email us at hpc@yale.edu . Also checkout our list of recommended online tutorials on Python, R, unix commands and more .","title":"Overview"},{"location":"clusters-at-yale/guides/#overview","text":"We provide guides for running certain software packages on our clusters. If you have tips for running a commonly used software and would like to contribute them to our Software Guides, email us at hpc@yale.edu . Also checkout our list of recommended online tutorials on Python, R, unix commands and more .","title":"Overview"},{"location":"clusters-at-yale/guides/cesm/","text":"CESM/CAM This is a quick start guide for CESM at Yale. You will still need to read the CESM User Guide and work with your fellow research group members to design and run your simulations, but this guide covers the basics that are specific to running CESM at Yale. CESM User Guides CESM1.0.4 User\u2019s Guide CESM1.1.z User\u2019s Guide CESM User\u2019s Guide (CESM1.2 Release Series User\u2019s Guide) (PDF) Modules CESM 1.0.4, 1.2.2, 2.x are available on Omega and Grace. Other versions may be available on one of the clusters. To find which version are available on your cluster, run module avail cesm Once you have located your module, run module load <module-name> with the module name from above. The module will configure your environment with the Intel compiler, OpenMPI and NetCDF libraries as well as set the location of the Yale\u2019s repository of CESM input data. If you will be primarily using CESM, you can avoid rerunning the module load command every time you login by saving it to your default environment: module load <module-name> module save Input Data To reduce the amount of data duplication on the cluster, we keep one centralized repository of CESM input data. The YCRC staff are only people who can add to that directory, so if your build fails due to missing inputdata, send your create_newcase line to Kaylea ( kaylea.nelson@yale.edu ) and she will download that data for you. Running CESM CESM needs to be rebuilt separately for each run. As a result, running CESM is more complicated than a standard piece of software where you would just run the executable. Create Your Case Each simulation is called a \u201ccase\u201d. Loading a CESM module will put the create_newcase script in your path, so you can call it as follows. This will create a directory with your case name, that we will refer to as $CASE s through out the rest of the guide. create_newcase -case <case_name> -compset = <compset> -res = <resolution> -mach = <machine> # substitute your case name for \"$CASE\" below cd <case_name> The mach parameters for Grace and Omega are yalegrace and omega , respectively. For example # on Grace create_newcase -case Test_Sim -compset = B2000 -res = f19_f19 -mach = yalegrace # on Omega create_newcase -case Test_Sim -compset = B2000 -res = f19_f19 -mach = omega cd Test_Sim Setup Your Case If you are making any changes to the namelist files (such as increasing the duration of the simulation), do those before running the setup scripts below. CESM 1.0.X If you are using CESM 1.0.X (e.g. 1.0.4), set up your case with ./configure -case CESM 1.1.X and CESM 1.2.X If you are using CESM 1.1.X (e.g. 1.1.1) or CESM 1.2.X (e.g. 1.2.2), set up your case with ./cesm_setup Building your case After you run the setup script, there will be a set of the scripts in your case directory that start with your case name. To compile your simulation executable, first move to an interactive job and then run the build script corresponding to your case. srun --pty -c 4 -p interactive bash ./ $CASE . $mach .build For more details on interactive jobs, see our Slurm documentation . During the build, CESM will create a corresponding directory in your scratch or project directory at # On Omega ls ~/scratch/CESM/$CASE # On Grace ls ~/project/CESM/$CASE This directory will contain all the outputs from your simulation as well as logs and the cesm.exe executable. Common Build Issues Make sure you compile on an interactive node as described above. If you build fails, it will direct you to look in a bldlog file. If that log complains that it can\u2019t find mpirun, NetCDF or another library or executable, make sure you have the correct CESM module loaded. Submit Your Case Once the build is complete, which can take 5-15 minutes, you can submit your case with the submit script. ./ $CASE . $mach .submit For more details on monitoring your submitted jobs, see our Slurm documentation . Troubleshoot Your Run If your run doesn\u2019t complete, there are a few places to look to identify the error. CESM writes to multiple log files for the different components and you will likely have to look in a few to find the root cause of your error. Slurm Log In your case directory, there will be a file that looks like slurm-<job_id>.log . Check that file first to make sure the job started up properly. If the last few lines in the file redirect you to look at cpl.log.<some_number> file in your scratch directory, see below. If there is another error, try to address it and resubmit. CESM Run Logs If the last few lines of the slurm log direct you to look at cpl.log.<some_number> file, change directory to your case \u201crun\u201d directory in your scratch directory: cd ~/scratch/CESM/ $CASE /run The pointer to the cpl file is often misleading as I have found the error is usually located in one of the other logs. Instead look in the cesm.log.xxxxxx file. Towards the end there may be an error or it may signify which component was running. Then look in the log corresponding to that component to track down the issue. One shortcut to finding the relevant logs is to sort the log files by the time to see which ones were last updated: ls -ltr *log* Look at the end of the last couple logs listed and look for an indication of the error. Resolve Errors Once you have identified the lines in the logs corresponding to your error: If your log says something like \u201cDisk quota exceeded\u201d, your group is out of space in the fileset you are writing to. You can run the getquota script to get details on your disk usage. Your group will need to reduce their usage before you will be able to run successfully. If it looks like a model error and you don\u2019t know how to fix it, I strongly recommend Googling your error and/or looking in the CESM forums . If you still can\u2019t resolve your error, you can come by Kaylea\u2019s office hours, posted outside KGL 227. Alternative Submission Parameters By default, the submission script will submit to the week partition for 7 days or the geo partition on Omega depending on your group. Sometimes the wait times can be very long in the week partition, so you can either submit day or scavenge partition. To change this, edit your case\u2019s run script and change the partition and time lines. The maximum walltime in the day partition is 24 hours. The maximum walltime in scavenge is 24 hours on Grace and 7 days on Omega. For example: ## day partition #SBATCH --partition=day #SBATCH --time=1- ## scavenge partition #SBATCH --partition=scavenge #SBATCH --time=1- Note that is your submission script was configured to submit to the geo partition on Omega, you will need to modify the #SBATCH -A pi_<group_name> to be just #SBATCH -A <group_name> to submit to any other partition. Then you can submit by running the submit script ./ $CASE . $mach .submit Further Reading I recommend referencing the User Guides listed at the top of this page. CESM User Forum Our Slurm Documentation CESM is a very widely used package, you can often find answers by simply using Google. Just make sure that the solutions you find correspond to the approximate version of CESM you are using. CESM changes in subtle but significant ways between versions.","title":"CESM/CAM"},{"location":"clusters-at-yale/guides/cesm/#cesmcam","text":"This is a quick start guide for CESM at Yale. You will still need to read the CESM User Guide and work with your fellow research group members to design and run your simulations, but this guide covers the basics that are specific to running CESM at Yale.","title":"CESM/CAM"},{"location":"clusters-at-yale/guides/cesm/#cesm-user-guides","text":"CESM1.0.4 User\u2019s Guide CESM1.1.z User\u2019s Guide CESM User\u2019s Guide (CESM1.2 Release Series User\u2019s Guide) (PDF)","title":"CESM User Guides"},{"location":"clusters-at-yale/guides/cesm/#modules","text":"CESM 1.0.4, 1.2.2, 2.x are available on Omega and Grace. Other versions may be available on one of the clusters. To find which version are available on your cluster, run module avail cesm Once you have located your module, run module load <module-name> with the module name from above. The module will configure your environment with the Intel compiler, OpenMPI and NetCDF libraries as well as set the location of the Yale\u2019s repository of CESM input data. If you will be primarily using CESM, you can avoid rerunning the module load command every time you login by saving it to your default environment: module load <module-name> module save","title":"Modules"},{"location":"clusters-at-yale/guides/cesm/#input-data","text":"To reduce the amount of data duplication on the cluster, we keep one centralized repository of CESM input data. The YCRC staff are only people who can add to that directory, so if your build fails due to missing inputdata, send your create_newcase line to Kaylea ( kaylea.nelson@yale.edu ) and she will download that data for you.","title":"Input Data"},{"location":"clusters-at-yale/guides/cesm/#running-cesm","text":"CESM needs to be rebuilt separately for each run. As a result, running CESM is more complicated than a standard piece of software where you would just run the executable.","title":"Running CESM"},{"location":"clusters-at-yale/guides/cesm/#create-your-case","text":"Each simulation is called a \u201ccase\u201d. Loading a CESM module will put the create_newcase script in your path, so you can call it as follows. This will create a directory with your case name, that we will refer to as $CASE s through out the rest of the guide. create_newcase -case <case_name> -compset = <compset> -res = <resolution> -mach = <machine> # substitute your case name for \"$CASE\" below cd <case_name> The mach parameters for Grace and Omega are yalegrace and omega , respectively. For example # on Grace create_newcase -case Test_Sim -compset = B2000 -res = f19_f19 -mach = yalegrace # on Omega create_newcase -case Test_Sim -compset = B2000 -res = f19_f19 -mach = omega cd Test_Sim","title":"Create Your Case"},{"location":"clusters-at-yale/guides/cesm/#setup-your-case","text":"If you are making any changes to the namelist files (such as increasing the duration of the simulation), do those before running the setup scripts below.","title":"Setup Your Case"},{"location":"clusters-at-yale/guides/cesm/#cesm-10x","text":"If you are using CESM 1.0.X (e.g. 1.0.4), set up your case with ./configure -case","title":"CESM 1.0.X"},{"location":"clusters-at-yale/guides/cesm/#cesm-11x-and-cesm-12x","text":"If you are using CESM 1.1.X (e.g. 1.1.1) or CESM 1.2.X (e.g. 1.2.2), set up your case with ./cesm_setup","title":"CESM 1.1.X and CESM 1.2.X"},{"location":"clusters-at-yale/guides/cesm/#building-your-case","text":"After you run the setup script, there will be a set of the scripts in your case directory that start with your case name. To compile your simulation executable, first move to an interactive job and then run the build script corresponding to your case. srun --pty -c 4 -p interactive bash ./ $CASE . $mach .build For more details on interactive jobs, see our Slurm documentation . During the build, CESM will create a corresponding directory in your scratch or project directory at # On Omega ls ~/scratch/CESM/$CASE # On Grace ls ~/project/CESM/$CASE This directory will contain all the outputs from your simulation as well as logs and the cesm.exe executable.","title":"Building your case"},{"location":"clusters-at-yale/guides/cesm/#common-build-issues","text":"Make sure you compile on an interactive node as described above. If you build fails, it will direct you to look in a bldlog file. If that log complains that it can\u2019t find mpirun, NetCDF or another library or executable, make sure you have the correct CESM module loaded.","title":"Common Build Issues"},{"location":"clusters-at-yale/guides/cesm/#submit-your-case","text":"Once the build is complete, which can take 5-15 minutes, you can submit your case with the submit script. ./ $CASE . $mach .submit For more details on monitoring your submitted jobs, see our Slurm documentation .","title":"Submit Your Case"},{"location":"clusters-at-yale/guides/cesm/#troubleshoot-your-run","text":"If your run doesn\u2019t complete, there are a few places to look to identify the error. CESM writes to multiple log files for the different components and you will likely have to look in a few to find the root cause of your error.","title":"Troubleshoot Your Run"},{"location":"clusters-at-yale/guides/cesm/#slurm-log","text":"In your case directory, there will be a file that looks like slurm-<job_id>.log . Check that file first to make sure the job started up properly. If the last few lines in the file redirect you to look at cpl.log.<some_number> file in your scratch directory, see below. If there is another error, try to address it and resubmit.","title":"Slurm Log"},{"location":"clusters-at-yale/guides/cesm/#cesm-run-logs","text":"If the last few lines of the slurm log direct you to look at cpl.log.<some_number> file, change directory to your case \u201crun\u201d directory in your scratch directory: cd ~/scratch/CESM/ $CASE /run The pointer to the cpl file is often misleading as I have found the error is usually located in one of the other logs. Instead look in the cesm.log.xxxxxx file. Towards the end there may be an error or it may signify which component was running. Then look in the log corresponding to that component to track down the issue. One shortcut to finding the relevant logs is to sort the log files by the time to see which ones were last updated: ls -ltr *log* Look at the end of the last couple logs listed and look for an indication of the error.","title":"CESM Run Logs"},{"location":"clusters-at-yale/guides/cesm/#resolve-errors","text":"Once you have identified the lines in the logs corresponding to your error: If your log says something like \u201cDisk quota exceeded\u201d, your group is out of space in the fileset you are writing to. You can run the getquota script to get details on your disk usage. Your group will need to reduce their usage before you will be able to run successfully. If it looks like a model error and you don\u2019t know how to fix it, I strongly recommend Googling your error and/or looking in the CESM forums . If you still can\u2019t resolve your error, you can come by Kaylea\u2019s office hours, posted outside KGL 227.","title":"Resolve Errors"},{"location":"clusters-at-yale/guides/cesm/#alternative-submission-parameters","text":"By default, the submission script will submit to the week partition for 7 days or the geo partition on Omega depending on your group. Sometimes the wait times can be very long in the week partition, so you can either submit day or scavenge partition. To change this, edit your case\u2019s run script and change the partition and time lines. The maximum walltime in the day partition is 24 hours. The maximum walltime in scavenge is 24 hours on Grace and 7 days on Omega. For example: ## day partition #SBATCH --partition=day #SBATCH --time=1- ## scavenge partition #SBATCH --partition=scavenge #SBATCH --time=1- Note that is your submission script was configured to submit to the geo partition on Omega, you will need to modify the #SBATCH -A pi_<group_name> to be just #SBATCH -A <group_name> to submit to any other partition. Then you can submit by running the submit script ./ $CASE . $mach .submit","title":"Alternative Submission Parameters"},{"location":"clusters-at-yale/guides/cesm/#further-reading","text":"I recommend referencing the User Guides listed at the top of this page. CESM User Forum Our Slurm Documentation CESM is a very widely used package, you can often find answers by simply using Google. Just make sure that the solutions you find correspond to the approximate version of CESM you are using. CESM changes in subtle but significant ways between versions.","title":"Further Reading"},{"location":"clusters-at-yale/guides/conda/","text":"Conda For researchers who have Python or R package requirements beyond the most common packages (e.g. Numpy, Scipy, Pandas), we recommend using Anaconda . Using Anaconda's Conda package manager, you can create and manage packages and environments. These allow you to easily switch between versions of Python or R libraries and applications for different projects. Many other software applications have also started to use Conda as a package manager. It has become a popular choice for managing pipelines that involve several tools, especially with multiple languages. The Miniconda Module For your convenience, we provide a relatively recent version of Miniconda (a minimal set of Anaconda libraries) as a module. It serves to bootstrap your personal environments. By using this module, you do not need to download your own copy of Conda, which will prevent unnecessary file and storage usage in your directories. Note: If you are on Milgram and run out of space in your home directory for Conda, you can either reinstall your environment in your project space (see below) or contact us at hpc@yale.edu for help with your home quota. Setup Your Environment Load the Miniconda Module # Grace, Omega module load Tools/miniconda # all Others module load miniconda You can save this to your default module collection by using module save . See our module documentation for more details. Install to Your Project Directory Conda will look in the directory/directories specified in the environment variable CONDA_ENVS_PATH for places to find and install environments. If you want your environments stored in a directory where your quotas are higher, for example, ~/project/conda_envs , you would need to set this variable to something like. We set this by default for you on Grace, Farnam and Ruddle. To match this behavior on Milgram: echo \"export CONDA_ENVS_PATH=~/project/conda_envs: $CONDA_ENVS_PATH \" >> ~/.bashrc source ~/.bashrc Create a conda Environment To create an environment (saved to the first location in $CONDA_ENVS_PATH or to ~/.conda/envs ) use the conda create command. You should give your environments names that are meaningful to you, so you can more easily keep track of which serves which project or purpose. You can also use environments manage groups of packages that have conflicting prerequisites. Because dependency resolution is hard and messy, we find specifying as many packages as possible at environment creation time can help minimize broken dependencies. Although often unavoidable for Python, we also recommend against heavily mixing the use of conda and pip to install applications. If needed, try to get as much installed with conda , then use pip to get the rest of the way to your desired environment. Tip For added reproducibility and control, specify versions of packages to be installed using conda with packagename=version syntax. E.g. numpy=1.14 For example, if you have a legacy application that needs Python 2 and OpenBLAS: conda create -n legacy_application python = 2 .7 openblas If you want a good starting point for interactive development of scientific Python scripts: conda create -n py37_dev python = 3 .7 numpy scipy pandas matplotlib ipython jupyter Conda Channels There are also community-lead collections of unofficial packages that you can use with conda called channels. A few popular examples are Conda Forge and Bioconda . You could use the Conda Forge channel to install Brian2 conda create -n brian2 --channel conda-forge brian2 Bioconda provides recent versions of various bioinformatics tools, for example: conda create -n bioinfo --channel bioconda biopython bedtools bowtie2 repeatmasker Using Your Environment To use the applications in your environment, make sure you have the miniconda module loaded then run the following: source activate env_name Interactive Your conda environments will not follow you into job allocations, so make sure to activate them after your interactive job begins. In a Job Script To make sure that you are running in your project environment in a submission script, make sure to include the following lines in your submission script before running any other commands or scripts (but after your Slurm directives ): #!/bin/bash #SBATCH --partition=general #SBATCH --job-name=my_conda_job #SBATCH --cpus-per-task 4 #SBATCH --mem-per-cpu=6000 # Grace, Omega module load Tools/miniconda # All other clusters module load miniconda source activate env_name python analyses.py Troubleshoot \"Permission Denied\" If you get a permission denied error while trying to conda or pip install a package, make sure you have created an environment or activated an existing one first. \"-bash: activate: No such file or directory\" If you get the above error, it is likely that you don't have the necessary module file loaded. Try loading the appropriate module and rerunning your source activate env_name command. \"could not find environment:\" This error means that the version of Anaconda/Miniconda you have loaded doesn't recognize the environment name you have supplied. Make sure you have the Miniconda module loaded (and not a different Python module) and have previously created this environment. You can see a list of previously created environments by running: conda info --envs Additional Conda Commands List Installed Packages source activate env_name conda list Delete a Conda Environment conda env remove --name env_name Share your Conda Environment If you want to share or back up a conda environment, you can export it to a file. To do so you need to run the following, replacing env_name with the desired environment. source activate env_name conda env export > env_name_environment.yml # on another machine or account, run conda env create -f env_name_environment.yml","title":"Conda"},{"location":"clusters-at-yale/guides/conda/#conda","text":"For researchers who have Python or R package requirements beyond the most common packages (e.g. Numpy, Scipy, Pandas), we recommend using Anaconda . Using Anaconda's Conda package manager, you can create and manage packages and environments. These allow you to easily switch between versions of Python or R libraries and applications for different projects. Many other software applications have also started to use Conda as a package manager. It has become a popular choice for managing pipelines that involve several tools, especially with multiple languages.","title":"Conda"},{"location":"clusters-at-yale/guides/conda/#the-miniconda-module","text":"For your convenience, we provide a relatively recent version of Miniconda (a minimal set of Anaconda libraries) as a module. It serves to bootstrap your personal environments. By using this module, you do not need to download your own copy of Conda, which will prevent unnecessary file and storage usage in your directories. Note: If you are on Milgram and run out of space in your home directory for Conda, you can either reinstall your environment in your project space (see below) or contact us at hpc@yale.edu for help with your home quota.","title":"The Miniconda Module"},{"location":"clusters-at-yale/guides/conda/#setup-your-environment","text":"","title":"Setup Your Environment"},{"location":"clusters-at-yale/guides/conda/#load-the-miniconda-module","text":"# Grace, Omega module load Tools/miniconda # all Others module load miniconda You can save this to your default module collection by using module save . See our module documentation for more details.","title":"Load the Miniconda Module"},{"location":"clusters-at-yale/guides/conda/#install-to-your-project-directory","text":"Conda will look in the directory/directories specified in the environment variable CONDA_ENVS_PATH for places to find and install environments. If you want your environments stored in a directory where your quotas are higher, for example, ~/project/conda_envs , you would need to set this variable to something like. We set this by default for you on Grace, Farnam and Ruddle. To match this behavior on Milgram: echo \"export CONDA_ENVS_PATH=~/project/conda_envs: $CONDA_ENVS_PATH \" >> ~/.bashrc source ~/.bashrc","title":"Install to Your Project Directory"},{"location":"clusters-at-yale/guides/conda/#create-a-conda-environment","text":"To create an environment (saved to the first location in $CONDA_ENVS_PATH or to ~/.conda/envs ) use the conda create command. You should give your environments names that are meaningful to you, so you can more easily keep track of which serves which project or purpose. You can also use environments manage groups of packages that have conflicting prerequisites. Because dependency resolution is hard and messy, we find specifying as many packages as possible at environment creation time can help minimize broken dependencies. Although often unavoidable for Python, we also recommend against heavily mixing the use of conda and pip to install applications. If needed, try to get as much installed with conda , then use pip to get the rest of the way to your desired environment. Tip For added reproducibility and control, specify versions of packages to be installed using conda with packagename=version syntax. E.g. numpy=1.14 For example, if you have a legacy application that needs Python 2 and OpenBLAS: conda create -n legacy_application python = 2 .7 openblas If you want a good starting point for interactive development of scientific Python scripts: conda create -n py37_dev python = 3 .7 numpy scipy pandas matplotlib ipython jupyter","title":"Create a conda Environment"},{"location":"clusters-at-yale/guides/conda/#conda-channels","text":"There are also community-lead collections of unofficial packages that you can use with conda called channels. A few popular examples are Conda Forge and Bioconda . You could use the Conda Forge channel to install Brian2 conda create -n brian2 --channel conda-forge brian2 Bioconda provides recent versions of various bioinformatics tools, for example: conda create -n bioinfo --channel bioconda biopython bedtools bowtie2 repeatmasker","title":"Conda Channels"},{"location":"clusters-at-yale/guides/conda/#using-your-environment","text":"To use the applications in your environment, make sure you have the miniconda module loaded then run the following: source activate env_name","title":"Using Your Environment"},{"location":"clusters-at-yale/guides/conda/#interactive","text":"Your conda environments will not follow you into job allocations, so make sure to activate them after your interactive job begins.","title":"Interactive"},{"location":"clusters-at-yale/guides/conda/#in-a-job-script","text":"To make sure that you are running in your project environment in a submission script, make sure to include the following lines in your submission script before running any other commands or scripts (but after your Slurm directives ): #!/bin/bash #SBATCH --partition=general #SBATCH --job-name=my_conda_job #SBATCH --cpus-per-task 4 #SBATCH --mem-per-cpu=6000 # Grace, Omega module load Tools/miniconda # All other clusters module load miniconda source activate env_name python analyses.py","title":"In a Job Script"},{"location":"clusters-at-yale/guides/conda/#troubleshoot","text":"","title":"Troubleshoot"},{"location":"clusters-at-yale/guides/conda/#permission-denied","text":"If you get a permission denied error while trying to conda or pip install a package, make sure you have created an environment or activated an existing one first.","title":"\"Permission Denied\""},{"location":"clusters-at-yale/guides/conda/#-bash-activate-no-such-file-or-directory","text":"If you get the above error, it is likely that you don't have the necessary module file loaded. Try loading the appropriate module and rerunning your source activate env_name command.","title":"\"-bash: activate: No such file or directory\""},{"location":"clusters-at-yale/guides/conda/#could-not-find-environment","text":"This error means that the version of Anaconda/Miniconda you have loaded doesn't recognize the environment name you have supplied. Make sure you have the Miniconda module loaded (and not a different Python module) and have previously created this environment. You can see a list of previously created environments by running: conda info --envs","title":"\"could not find environment:\""},{"location":"clusters-at-yale/guides/conda/#additional-conda-commands","text":"","title":"Additional Conda Commands"},{"location":"clusters-at-yale/guides/conda/#list-installed-packages","text":"source activate env_name conda list","title":"List Installed Packages"},{"location":"clusters-at-yale/guides/conda/#delete-a-conda-environment","text":"conda env remove --name env_name","title":"Delete a Conda Environment"},{"location":"clusters-at-yale/guides/conda/#share-your-conda-environment","text":"If you want to share or back up a conda environment, you can export it to a file. To do so you need to run the following, replacing env_name with the desired environment. source activate env_name conda env export > env_name_environment.yml # on another machine or account, run conda env create -f env_name_environment.yml","title":"Share your Conda Environment"},{"location":"clusters-at-yale/guides/cryosparc/","text":"cryoSPARCv2 on Farnam Getting cryoSPARC set up and running on the YCRC clusters is something of a task. This guide is meant for intermediate/advanced users. If enought people can convince Structura bio ( see ticket here ) to make cryoSPARC more cluster-friendly we could have a single instance running that you'd just log in to with your Yale credentials. Until then, venture below at your own peril. Install Before you get started, you will need to request a licence from Structura from their website . These instructions are gently modified from the official cryoSPARC documentation . 1. Set up Environment First allocate an interactive job on a compute node to run the install on. srun --cpus-per-task 2 --pty -p interactive bash Then, set the following environment variables to suit your install. We filled in some defaults for you. # where to install cryosparc2 and its sample database install_path = $( readlink -f ${ HOME } /project ) /software/cryosparc2 # the license ID you got from Structura license_id = # your email my_email = $( head -n1 ~/.forward ) # slurm partition to submit your cryosparc jobs to # not sure you can change at runtime? partition = gpu 2. Set up Directories, Download installers # your username my_name = ${ USER } # a temp password cryosparc_passwd = Password123 # load the right CUDA module load CUDA/9.0.176 # set up some more paths db_path = ${ install_path } /database worker_path = ${ install_path } /cryosparc2_worker ssd_path = /tmp/ ${ USER } /cryosparc2_cache # go get the installers mkdir -p $install_path cd $install_path curl -sL https://get.cryosparc.com/download/master-latest/ $license_id > cryosparc2_master.tar.gz curl -sL https://get.cryosparc.com/download/worker-latest/ $license_id > cryosparc2_worker.tar.gz tar -xf cryosparc2_master.tar.gz tar -xf cryosparc2_worker.tar.gz 3. Install the Server and Worker cd ${ install_path } /cryosparc2_master ./install.sh --license $license_id --hostname $( hostname ) --dbpath $db_path --yes source ~/.bashrc cd ${ install_path } /cryosparc2_worker ./install.sh --license $license_id --cudapath $CUDA_HOME --yes source ~/.bashrc 4. Configure for Farnam # Farnam cluster setup mkdir -p ${ install_path } /site_configs && cd ${ install_path } /site_configs cat << EOF > cluster_info.json { \"name\" : \"farnam\", \"worker_bin_path\" : \"${install_path}/cryosparc2_worker/bin/cryosparcw\", \"cache_path\" : \"/tmp/{{ cryosparc_username }}/cryosparc_cache\", \"send_cmd_tpl\" : \"{{ command }}\", \"qsub_cmd_tpl\" : \"sbatch {{ script_path_abs }}\", \"qstat_cmd_tpl\" : \"squeue -j {{ cluster_job_id }}\", \"qdel_cmd_tpl\" : \"scancel {{ cluster_job_id }}\", \"qinfo_cmd_tpl\" : \"sinfo\" } EOF cat << EOF > cluster_script.sh #!/usr/bin/env bash #SBATCH --job-name cryosparc_{{ project_uid }}_{{ job_uid }} #SBATCH -c {{ num_cpu }} #SBATCH --gres=gpu:{{ num_gpu }} #SBATCH -p ${partition} #SBATCH --mem={{ (ram_gb*1024)|int }} #SBATCH -o {{ job_dir_abs }} #SBATCH -e {{ job_dir_abs }} module load CUDA/9.0.176 mkdir -p /tmp/${USER}/cryosparc2_cache {{ run_cmd }} EOF Run srun --cpus-per-task 2 --pty -p interactive bash master_host = $( hostname ) base_dir = $( dirname \" $( dirname \" $( which cryosparcm ) \" ) \" ) sed -i.bak 's/export CRYOSPARC_MASTER_HOSTNAME.*$/export CRYOSPARC_MASTER_HOSTNAME=\\\"' \" $master_host \" '\\\"/g' $base_dir /config.sh source $base_dir /config.sh cryosparcm start cryosparcm status # run the output from the following command on your local linux/mac machine echo \"ssh -N -L $CRYOSPARC_BASE_PORT : $master_host : $CRYOSPARC_BASE_PORT $USER @farnam.hpc.yale.edu\" Database errors If your database won't start and you're sure there isn't another server running, you can remove lock files and try again. # rm -f $CRYOSPARC_DB_PATH/WiredTiger.lock $CRYOSPARC_DB_PATH/mongod.lock","title":"cryoSPARCv2 on Farnam"},{"location":"clusters-at-yale/guides/cryosparc/#cryosparcv2-on-farnam","text":"Getting cryoSPARC set up and running on the YCRC clusters is something of a task. This guide is meant for intermediate/advanced users. If enought people can convince Structura bio ( see ticket here ) to make cryoSPARC more cluster-friendly we could have a single instance running that you'd just log in to with your Yale credentials. Until then, venture below at your own peril.","title":"cryoSPARCv2 on Farnam"},{"location":"clusters-at-yale/guides/cryosparc/#install","text":"Before you get started, you will need to request a licence from Structura from their website . These instructions are gently modified from the official cryoSPARC documentation .","title":"Install"},{"location":"clusters-at-yale/guides/cryosparc/#1-set-up-environment","text":"First allocate an interactive job on a compute node to run the install on. srun --cpus-per-task 2 --pty -p interactive bash Then, set the following environment variables to suit your install. We filled in some defaults for you. # where to install cryosparc2 and its sample database install_path = $( readlink -f ${ HOME } /project ) /software/cryosparc2 # the license ID you got from Structura license_id = # your email my_email = $( head -n1 ~/.forward ) # slurm partition to submit your cryosparc jobs to # not sure you can change at runtime? partition = gpu","title":"1. Set up Environment"},{"location":"clusters-at-yale/guides/cryosparc/#2-set-up-directories-download-installers","text":"# your username my_name = ${ USER } # a temp password cryosparc_passwd = Password123 # load the right CUDA module load CUDA/9.0.176 # set up some more paths db_path = ${ install_path } /database worker_path = ${ install_path } /cryosparc2_worker ssd_path = /tmp/ ${ USER } /cryosparc2_cache # go get the installers mkdir -p $install_path cd $install_path curl -sL https://get.cryosparc.com/download/master-latest/ $license_id > cryosparc2_master.tar.gz curl -sL https://get.cryosparc.com/download/worker-latest/ $license_id > cryosparc2_worker.tar.gz tar -xf cryosparc2_master.tar.gz tar -xf cryosparc2_worker.tar.gz","title":"2. Set up Directories, Download installers"},{"location":"clusters-at-yale/guides/cryosparc/#3-install-the-server-and-worker","text":"cd ${ install_path } /cryosparc2_master ./install.sh --license $license_id --hostname $( hostname ) --dbpath $db_path --yes source ~/.bashrc cd ${ install_path } /cryosparc2_worker ./install.sh --license $license_id --cudapath $CUDA_HOME --yes source ~/.bashrc","title":"3. Install the Server and Worker"},{"location":"clusters-at-yale/guides/cryosparc/#4-configure-for-farnam","text":"# Farnam cluster setup mkdir -p ${ install_path } /site_configs && cd ${ install_path } /site_configs cat << EOF > cluster_info.json { \"name\" : \"farnam\", \"worker_bin_path\" : \"${install_path}/cryosparc2_worker/bin/cryosparcw\", \"cache_path\" : \"/tmp/{{ cryosparc_username }}/cryosparc_cache\", \"send_cmd_tpl\" : \"{{ command }}\", \"qsub_cmd_tpl\" : \"sbatch {{ script_path_abs }}\", \"qstat_cmd_tpl\" : \"squeue -j {{ cluster_job_id }}\", \"qdel_cmd_tpl\" : \"scancel {{ cluster_job_id }}\", \"qinfo_cmd_tpl\" : \"sinfo\" } EOF cat << EOF > cluster_script.sh #!/usr/bin/env bash #SBATCH --job-name cryosparc_{{ project_uid }}_{{ job_uid }} #SBATCH -c {{ num_cpu }} #SBATCH --gres=gpu:{{ num_gpu }} #SBATCH -p ${partition} #SBATCH --mem={{ (ram_gb*1024)|int }} #SBATCH -o {{ job_dir_abs }} #SBATCH -e {{ job_dir_abs }} module load CUDA/9.0.176 mkdir -p /tmp/${USER}/cryosparc2_cache {{ run_cmd }} EOF","title":"4. Configure for Farnam"},{"location":"clusters-at-yale/guides/cryosparc/#run","text":"srun --cpus-per-task 2 --pty -p interactive bash master_host = $( hostname ) base_dir = $( dirname \" $( dirname \" $( which cryosparcm ) \" ) \" ) sed -i.bak 's/export CRYOSPARC_MASTER_HOSTNAME.*$/export CRYOSPARC_MASTER_HOSTNAME=\\\"' \" $master_host \" '\\\"/g' $base_dir /config.sh source $base_dir /config.sh cryosparcm start cryosparcm status # run the output from the following command on your local linux/mac machine echo \"ssh -N -L $CRYOSPARC_BASE_PORT : $master_host : $CRYOSPARC_BASE_PORT $USER @farnam.hpc.yale.edu\"","title":"Run"},{"location":"clusters-at-yale/guides/cryosparc/#database-errors","text":"If your database won't start and you're sure there isn't another server running, you can remove lock files and try again. # rm -f $CRYOSPARC_DB_PATH/WiredTiger.lock $CRYOSPARC_DB_PATH/mongod.lock","title":"Database errors"},{"location":"clusters-at-yale/guides/deep-learning-gpus/","text":"Deep Learning with GPUs In order to get your job set up properly and test your environment, you will want to allocate a compute node that has a GPU. Here are a couple examples: srun --pty -p gpu -c 2 -t 12 :00:00 --gres = gpu:1 bash or, if the gpu queue is busy, try to scavenge some private GPUs: srun --pty -p scavenge -c 2 -t 12 :00:00 --gres = gpu:1 bash Next, we'll walk though the setup and activation of your environment. One time Setup Modules Load the appropriate modules for either Farnam or Grace. # load modules for Farnam module purge module load GCC/7.3.0-2.30 module load cuDNN/7.1.4-CUDA-9.0.176 module load Python/miniconda # or load modules for Grace module purge module load Langs/GCC/5.2.0 module load GPU/cuDNN/9.0-v7 module load Langs/Python/miniconda Then save your modules as a collection. # save module environment module save cuda module purge Create Your Python Environment You will want to create a virtual environment for your GPU enabled code. For more details on Conda environments, see our Conda documentation . # create conda environment for deep learning/neural networks conda create -y -n dlnn python = 3 .6 anaconda source activate dlnn #install libraries pip install --force --upgrade setuptools pip install keras pip install Theano pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.8.0-cp36-cp36m-linux_x86_64.whl pip install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-linux_x86_64.whl pip install torchvision Use Your Environment Now, to re-enter your Deep Neural Network environment, you just need the following: module restore cuda source activate dlnn","title":"Deep Learning with GPUs"},{"location":"clusters-at-yale/guides/deep-learning-gpus/#deep-learning-with-gpus","text":"In order to get your job set up properly and test your environment, you will want to allocate a compute node that has a GPU. Here are a couple examples: srun --pty -p gpu -c 2 -t 12 :00:00 --gres = gpu:1 bash or, if the gpu queue is busy, try to scavenge some private GPUs: srun --pty -p scavenge -c 2 -t 12 :00:00 --gres = gpu:1 bash Next, we'll walk though the setup and activation of your environment.","title":"Deep Learning with GPUs"},{"location":"clusters-at-yale/guides/deep-learning-gpus/#one-time-setup","text":"","title":"One time Setup"},{"location":"clusters-at-yale/guides/deep-learning-gpus/#modules","text":"Load the appropriate modules for either Farnam or Grace. # load modules for Farnam module purge module load GCC/7.3.0-2.30 module load cuDNN/7.1.4-CUDA-9.0.176 module load Python/miniconda # or load modules for Grace module purge module load Langs/GCC/5.2.0 module load GPU/cuDNN/9.0-v7 module load Langs/Python/miniconda Then save your modules as a collection. # save module environment module save cuda module purge","title":"Modules"},{"location":"clusters-at-yale/guides/deep-learning-gpus/#create-your-python-environment","text":"You will want to create a virtual environment for your GPU enabled code. For more details on Conda environments, see our Conda documentation . # create conda environment for deep learning/neural networks conda create -y -n dlnn python = 3 .6 anaconda source activate dlnn #install libraries pip install --force --upgrade setuptools pip install keras pip install Theano pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.8.0-cp36-cp36m-linux_x86_64.whl pip install http://download.pytorch.org/whl/cu90/torch-0.4.0-cp36-cp36m-linux_x86_64.whl pip install torchvision","title":"Create Your Python Environment"},{"location":"clusters-at-yale/guides/deep-learning-gpus/#use-your-environment","text":"Now, to re-enter your Deep Neural Network environment, you just need the following: module restore cuda source activate dlnn","title":"Use Your Environment"},{"location":"clusters-at-yale/guides/gaussian/","text":"Gaussian Gaussian is an electronic structure modeling program that Yale has licensed for its HPC clusters. To see available versions of Guassian on the cluster, run: module avail gaussian Running Gaussian on the Cluster When running Gaussian, it is recommended that users request exclusive access to allocated nodes (e.g., by requesting 8 cpus per node) and that they specify the largest possible memory allocation for the number of nodes requested. In addition, in most cases, the scratch storage location (set by the environment variable GAUSS_SCRDIR ) should be on the local parallel file system of the cluster, rather than in the user\u2019s home directory. Before running Gaussian, you must set up a number of environment variables. This is accomplished most easily by loading the Gaussian module file using: module load Apps/Gaussian To run Gaussian interactively, you need to create an interactive session on a compute node. You could start an interactive session using 4 cores for 4 hours using srun --pty -c 4 -p interactive -t 4 :00:00 bash See our Slurm documentation for more detailed information on requesting resources for interactive jobs. Gaussian in parallel To run Gaussian in parallel, you may find it simplest to use the script par_g09 instead of executing Gaussian directly. par_g09 takes 1 optional argument and is used as shown here: par_g09 [ num_proc_shared ] < g09_input > g09_output When included in a submission script, par_g09 will set the following Gaussian input variables: %LindaWorkers : List of nodes that will act as parallel workers during parallel portions of the Gaussian computations. The list is created automatically based on the nodes allocated to the particular job. Workers communicate using the Network Linda system provided with the Gaussian software. %NProcShared : Number of shared-memory processes per Linda worker. It is highly recommended that users run 1 Linda worker per node with %NProcShared=8 on the Omega cluster. (This is the default behavior of par_g09 .) However, users may easily override the value of %NProcShared by supplying the optional num_proc_shared argument to par_g09 . (Of course, both variables may be overridden by including them in the Gaussian input file.) On the Grace cluster, there are up to 20 processors and 128 GB of memory per node, so it may often work well to use just a single node with many shared-memory processors, in which case you can simply modify the Gaussian input file and skip the use of the par_g09 script. If you do wish to use multiple nodes on the Grace cluster, please pay careful attention to the Slurm parameters you use, particular to the setting of --mem-per-cpu to ensure that each of the nodes you request has sufficient resources available. GaussView In connection with Gaussian, we have also installed GaussView 5, Gaussian Inc.'s advanced and powerful graphical interface for Gaussian. With GaussView, you can import or build the molecular structures that interest you; set up, launch, monitor and control Gaussian calculations; and retrieve and view the results, all without ever leaving the application. GaussView 5 includes many new features designed to make working with large systems of chemical interest convenient and straightforward. It also provides full support for all of the new modeling methods and features in Gaussian 09. In order to use GaussView, you must run an X Server on your desktop or laptop, and you must enable X forwarding when logging into the cluster. See our X11 forwarding documentation for instructions. Loading the module file for Gaussian sets up your environment for GaussView as well. Then you can type the command gv . GaussView 5 may not be compatible with certain versions of the X servers you may run on your desktop or laptop. If you encounter problems, these can often be overcome by starting GaussView with the command gv -mesagl or gv -soft .","title":"Gaussian"},{"location":"clusters-at-yale/guides/gaussian/#gaussian","text":"Gaussian is an electronic structure modeling program that Yale has licensed for its HPC clusters. To see available versions of Guassian on the cluster, run: module avail gaussian","title":"Gaussian"},{"location":"clusters-at-yale/guides/gaussian/#running-gaussian-on-the-cluster","text":"When running Gaussian, it is recommended that users request exclusive access to allocated nodes (e.g., by requesting 8 cpus per node) and that they specify the largest possible memory allocation for the number of nodes requested. In addition, in most cases, the scratch storage location (set by the environment variable GAUSS_SCRDIR ) should be on the local parallel file system of the cluster, rather than in the user\u2019s home directory. Before running Gaussian, you must set up a number of environment variables. This is accomplished most easily by loading the Gaussian module file using: module load Apps/Gaussian To run Gaussian interactively, you need to create an interactive session on a compute node. You could start an interactive session using 4 cores for 4 hours using srun --pty -c 4 -p interactive -t 4 :00:00 bash See our Slurm documentation for more detailed information on requesting resources for interactive jobs.","title":"Running Gaussian on the Cluster"},{"location":"clusters-at-yale/guides/gaussian/#gaussian-in-parallel","text":"To run Gaussian in parallel, you may find it simplest to use the script par_g09 instead of executing Gaussian directly. par_g09 takes 1 optional argument and is used as shown here: par_g09 [ num_proc_shared ] < g09_input > g09_output When included in a submission script, par_g09 will set the following Gaussian input variables: %LindaWorkers : List of nodes that will act as parallel workers during parallel portions of the Gaussian computations. The list is created automatically based on the nodes allocated to the particular job. Workers communicate using the Network Linda system provided with the Gaussian software. %NProcShared : Number of shared-memory processes per Linda worker. It is highly recommended that users run 1 Linda worker per node with %NProcShared=8 on the Omega cluster. (This is the default behavior of par_g09 .) However, users may easily override the value of %NProcShared by supplying the optional num_proc_shared argument to par_g09 . (Of course, both variables may be overridden by including them in the Gaussian input file.) On the Grace cluster, there are up to 20 processors and 128 GB of memory per node, so it may often work well to use just a single node with many shared-memory processors, in which case you can simply modify the Gaussian input file and skip the use of the par_g09 script. If you do wish to use multiple nodes on the Grace cluster, please pay careful attention to the Slurm parameters you use, particular to the setting of --mem-per-cpu to ensure that each of the nodes you request has sufficient resources available.","title":"Gaussian in parallel"},{"location":"clusters-at-yale/guides/gaussian/#gaussview","text":"In connection with Gaussian, we have also installed GaussView 5, Gaussian Inc.'s advanced and powerful graphical interface for Gaussian. With GaussView, you can import or build the molecular structures that interest you; set up, launch, monitor and control Gaussian calculations; and retrieve and view the results, all without ever leaving the application. GaussView 5 includes many new features designed to make working with large systems of chemical interest convenient and straightforward. It also provides full support for all of the new modeling methods and features in Gaussian 09. In order to use GaussView, you must run an X Server on your desktop or laptop, and you must enable X forwarding when logging into the cluster. See our X11 forwarding documentation for instructions. Loading the module file for Gaussian sets up your environment for GaussView as well. Then you can type the command gv . GaussView 5 may not be compatible with certain versions of the X servers you may run on your desktop or laptop. If you encounter problems, these can often be overcome by starting GaussView with the command gv -mesagl or gv -soft .","title":"GaussView"},{"location":"clusters-at-yale/guides/gpus-cuda/","text":"GPUs and CUDA We currently have GPUs available for general use on Grace and Farnam . See cluster pages for hardware and queue/partition specifics. Access the GPU Nodes To access the GPU nodes you must request them with the scheduler. An example Slurm command to request an interactive job on the gpu partition with X forwarding and 1/2 of a GPU node (10 cores and 1 K80): srun --pty --x11 -p gpu -c 10 -t 24 :00:00 --gres = gpu:2 --gres-flags = enforce-binding bash The --gres=gpu:2 option asks for two gpus, and the --gres-flags=enforce-binding option ensures you get two GPUs on the same card, and that the CPUs you are allocated are on the same bus as your GPU. To submit a batch job, include the following directives (in addition to your core, time, etc requests): #SBATCH -p gpu #SBATCH --gres=gpu:1 #SBATCH --gres-flags=enforce-binding Warning Please do not use nodes with GPUs unless your application or job can make use of them. Any jobs found running in a GPU partition without a GPU will be terminated without warning. You can check the available GPUs and their current usage with the command nvidia-smi . Software CUDA and cuDNN are available as modules where applicable. On your cluster of choice use toolkit is installed on the GPU nodes. To see what is available, type: module avail cuda Drivers The CUDA libraries you load will allow you to compile code against them. To run CUDA-enabled code you must also be running on a node with a gpu allocated and a compatible driver installed. The minimum driver versions are as follows ( borrowed from this nvidia developer site ): CUDA 10.0: 410.48 CUDA 9.2: 396.xx CUDA 9.1: 390.xx CUDA 9.0: 384.xx CUDA 8.0: 375.xx CUDA 7.5: 352.xx CUDA 7.0: 346.xx CUDA 6.5: 340.xx CUDA 6.0: 331.xx To check the insalled version of the nvidia drivers, you can also use nvidia-smi : [ user@gpu01 ~ ] $ nvidia-smi Tue Jan 16 10 :31:45 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 375 .66 Driver Version: 375 .66 | ... Here we see that the node gpu01 is running driver version 375.66. It can run CUDA 8 but not CUDA 9 yet.","title":"GPUs and CUDA"},{"location":"clusters-at-yale/guides/gpus-cuda/#gpus-and-cuda","text":"We currently have GPUs available for general use on Grace and Farnam . See cluster pages for hardware and queue/partition specifics.","title":"GPUs and CUDA"},{"location":"clusters-at-yale/guides/gpus-cuda/#access-the-gpu-nodes","text":"To access the GPU nodes you must request them with the scheduler. An example Slurm command to request an interactive job on the gpu partition with X forwarding and 1/2 of a GPU node (10 cores and 1 K80): srun --pty --x11 -p gpu -c 10 -t 24 :00:00 --gres = gpu:2 --gres-flags = enforce-binding bash The --gres=gpu:2 option asks for two gpus, and the --gres-flags=enforce-binding option ensures you get two GPUs on the same card, and that the CPUs you are allocated are on the same bus as your GPU. To submit a batch job, include the following directives (in addition to your core, time, etc requests): #SBATCH -p gpu #SBATCH --gres=gpu:1 #SBATCH --gres-flags=enforce-binding Warning Please do not use nodes with GPUs unless your application or job can make use of them. Any jobs found running in a GPU partition without a GPU will be terminated without warning. You can check the available GPUs and their current usage with the command nvidia-smi .","title":"Access the GPU Nodes"},{"location":"clusters-at-yale/guides/gpus-cuda/#software","text":"CUDA and cuDNN are available as modules where applicable. On your cluster of choice use toolkit is installed on the GPU nodes. To see what is available, type: module avail cuda","title":"Software"},{"location":"clusters-at-yale/guides/gpus-cuda/#drivers","text":"The CUDA libraries you load will allow you to compile code against them. To run CUDA-enabled code you must also be running on a node with a gpu allocated and a compatible driver installed. The minimum driver versions are as follows ( borrowed from this nvidia developer site ): CUDA 10.0: 410.48 CUDA 9.2: 396.xx CUDA 9.1: 390.xx CUDA 9.0: 384.xx CUDA 8.0: 375.xx CUDA 7.5: 352.xx CUDA 7.0: 346.xx CUDA 6.5: 340.xx CUDA 6.0: 331.xx To check the insalled version of the nvidia drivers, you can also use nvidia-smi : [ user@gpu01 ~ ] $ nvidia-smi Tue Jan 16 10 :31:45 2018 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 375 .66 Driver Version: 375 .66 | ... Here we see that the node gpu01 is running driver version 375.66. It can run CUDA 8 but not CUDA 9 yet.","title":"Drivers"},{"location":"clusters-at-yale/guides/jupyter/","text":"Jupyter Notebooks With a small amount of configuration, you can use a compute node to run a jupyter notebook and access it from your local machine. You will need to be on campus or logged in to the VPN for your connections to work. The main steps are: Start a jupyter notebook job. Start an ssh tunnel. Use your local browser to connect. Start the Server Here is a template for submitting a jupyter-notebook server as a batch job. You may need to edit some of the slurm options, including the time limit or the partition. You will also need to either load a module that contains jupyter-notebook or source activate an environment if you're using Anaconda Python . Save your edited version of this script on the cluster, and submit it with sbatch . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #!/bin/bash #SBATCH --partition general #SBATCH --nodes 1 #SBATCH --ntasks-per-node 1 #SBATCH --mem-per-cpu 8G #SBATCH --time 1-0:00:00 #SBATCH --job-name jupyter-notebook #SBATCH --output jupyter-notebook-%J.log # get tunneling info XDG_RUNTIME_DIR = \"\" port = $( shuf -i8000-9999 -n1 ) node = $( hostname -s ) user = $( whoami ) cluster = $( hostname -f | awk -F \".\" '{print $2}' ) # print tunneling instructions jupyter-log echo -e \" MacOS or linux terminal command to create your ssh tunnel: ssh -N -L ${ port } : ${ node } : ${ port } ${ user } @ ${ cluster } .hpc.yale.edu For more info and how to connect from windows, see research.computing.yale.edu/jupyter-nb Here is the MobaXterm info: Forwarded port:same as remote port Remote server: ${ node } Remote port: ${ port } SSH server: ${ cluster } .hpc.yale.edu SSH login: $user SSH port: 22 Use a Browser on your local machine to go to: localhost: ${ port } (prefix w/ https:// if using password) \" # load modules or conda environments here # e.g. farnam: # module load Python/2.7.13-foss-2016b # DON'T USE ADDRESS BELOW. # DO USE TOKEN BELOW jupyter-notebook --no-browser --port = ${ port } --ip = ${ node } Start the Tunnel Once you have submitted your job and it starts, your notebook server will be ready for you to connect. You can run squeue -u$(whoami) to check. You will see an \"R\" in the ST or status column for your notebook job if it is running. If you see a \"PD\" in the status column, you will have to wait for your job to start running to connect. The log file with information about how to connect will be in the directory you submitted the script from, and be named jupyter-notebook-[jobid].log where jobid is the slurm id for your job. MacOS and Linux On a Mac or Linux machine, you can start the tunnel with an SSH command. You can check the output from the job you started to get the specifc info you need. Windows On a windows machine, we recommend you use MobaXterm. See our guide on connecting with MobaXterm for instructions on how to get set up. You will need to take a look at your job's log file to get the details you need. Then start MobaXterm: Under Tools choose \"MobaSSHTunnel (port forwarding)\". Click the \"New SSH Tunnel\" button. Click the radio button for \"Local port forwarding\". Use the information in your jupyter notebook log file to fill out the boxes. Click Save. On your new tunnel, click the key symbol under the settings column and choose your ssh private key. Click the play button under the Start/Stop column. Browse the Notebook Finally, open a web browser on your local machine and enter the address http://localhost:port where port is the one specified in your log file. The address Jupyter creates by default (the one with the name of a compute node) will not work outside the cluster's network. Since version 5 of jupyter, the notebook will automatically generate a token that allows you to authenticate when you connect. It is long, and will be at the end of the url jupyter generates. It will look something like http://c14n06:9230/?token=**ad0775eaff315e6f1d98b13ef10b919bc6b9ef7d0605cc20** If you run into trouble or need help, as always just shoot us an email at hpc@yale.edu .","title":"Jupyter Notebooks"},{"location":"clusters-at-yale/guides/jupyter/#jupyter-notebooks","text":"With a small amount of configuration, you can use a compute node to run a jupyter notebook and access it from your local machine. You will need to be on campus or logged in to the VPN for your connections to work. The main steps are: Start a jupyter notebook job. Start an ssh tunnel. Use your local browser to connect.","title":"Jupyter Notebooks"},{"location":"clusters-at-yale/guides/jupyter/#start-the-server","text":"Here is a template for submitting a jupyter-notebook server as a batch job. You may need to edit some of the slurm options, including the time limit or the partition. You will also need to either load a module that contains jupyter-notebook or source activate an environment if you're using Anaconda Python . Save your edited version of this script on the cluster, and submit it with sbatch . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #!/bin/bash #SBATCH --partition general #SBATCH --nodes 1 #SBATCH --ntasks-per-node 1 #SBATCH --mem-per-cpu 8G #SBATCH --time 1-0:00:00 #SBATCH --job-name jupyter-notebook #SBATCH --output jupyter-notebook-%J.log # get tunneling info XDG_RUNTIME_DIR = \"\" port = $( shuf -i8000-9999 -n1 ) node = $( hostname -s ) user = $( whoami ) cluster = $( hostname -f | awk -F \".\" '{print $2}' ) # print tunneling instructions jupyter-log echo -e \" MacOS or linux terminal command to create your ssh tunnel: ssh -N -L ${ port } : ${ node } : ${ port } ${ user } @ ${ cluster } .hpc.yale.edu For more info and how to connect from windows, see research.computing.yale.edu/jupyter-nb Here is the MobaXterm info: Forwarded port:same as remote port Remote server: ${ node } Remote port: ${ port } SSH server: ${ cluster } .hpc.yale.edu SSH login: $user SSH port: 22 Use a Browser on your local machine to go to: localhost: ${ port } (prefix w/ https:// if using password) \" # load modules or conda environments here # e.g. farnam: # module load Python/2.7.13-foss-2016b # DON'T USE ADDRESS BELOW. # DO USE TOKEN BELOW jupyter-notebook --no-browser --port = ${ port } --ip = ${ node }","title":"Start the Server"},{"location":"clusters-at-yale/guides/jupyter/#start-the-tunnel","text":"Once you have submitted your job and it starts, your notebook server will be ready for you to connect. You can run squeue -u$(whoami) to check. You will see an \"R\" in the ST or status column for your notebook job if it is running. If you see a \"PD\" in the status column, you will have to wait for your job to start running to connect. The log file with information about how to connect will be in the directory you submitted the script from, and be named jupyter-notebook-[jobid].log where jobid is the slurm id for your job.","title":"Start the Tunnel"},{"location":"clusters-at-yale/guides/jupyter/#macos-and-linux","text":"On a Mac or Linux machine, you can start the tunnel with an SSH command. You can check the output from the job you started to get the specifc info you need.","title":"MacOS and Linux"},{"location":"clusters-at-yale/guides/jupyter/#windows","text":"On a windows machine, we recommend you use MobaXterm. See our guide on connecting with MobaXterm for instructions on how to get set up. You will need to take a look at your job's log file to get the details you need. Then start MobaXterm: Under Tools choose \"MobaSSHTunnel (port forwarding)\". Click the \"New SSH Tunnel\" button. Click the radio button for \"Local port forwarding\". Use the information in your jupyter notebook log file to fill out the boxes. Click Save. On your new tunnel, click the key symbol under the settings column and choose your ssh private key. Click the play button under the Start/Stop column.","title":"Windows"},{"location":"clusters-at-yale/guides/jupyter/#browse-the-notebook","text":"Finally, open a web browser on your local machine and enter the address http://localhost:port where port is the one specified in your log file. The address Jupyter creates by default (the one with the name of a compute node) will not work outside the cluster's network. Since version 5 of jupyter, the notebook will automatically generate a token that allows you to authenticate when you connect. It is long, and will be at the end of the url jupyter generates. It will look something like http://c14n06:9230/?token=**ad0775eaff315e6f1d98b13ef10b919bc6b9ef7d0605cc20** If you run into trouble or need help, as always just shoot us an email at hpc@yale.edu .","title":"Browse the Notebook"},{"location":"clusters-at-yale/guides/matlab/","text":"MATLAB Find MATLAB Run one of the commands below, which will list available versions and the corresponding module files: module spider matlab Load the appropriate module file. For example, to run version R2014a: module load MATLAB / 2014 a The module load command sets up your environment, including the PATH to find the proper version of the MATLAB program. Run MATLAB Warning The MATLAB program is too large to fit on a login node. If you try to run it there, it will crash. Instead, launch it in an interactive or batch job (see below). To launch MATLAB, using matlab command. # launch the MATLAB GUI matlab # or launch the MATLAB command line prompt maltab -nodisplay # or to launch a script matlab -nodisplay < runscript.m Interactive Job To run Matlab interactively, you need to create an interactive session on a compute node. You could start an interactive session using 4 cores on 1 node using something like srun --pty --x11 -c 4 -p interactive -t 4:00:00 bash Once your interactive session starts, you can load the appropriate module file and start Matlab as described above. See our Slurm documentation for more detailed information on requesting resources for interactive jobs. Batch Mode (without a GUI) Create a batch script containing both instructions to the scheduler and shell instructions to set up directories and start Matlab. At the point you wish to start Matlab, use a command like: matlab -nodisplay -nosplash -r YourFunction < /dev/null This command will run the contents of YourFunction.m. Your batch submission script must either be in or cd to the directory containing YourFunction.m for this to work. Below is a sample batch script to run Matlab in batch mode on Grace. If the name of the script is runit.sh, you would submit it using sbatch runit.sh Here's a script for Grace: 1 2 3 4 5 6 7 8 #!/bin/bash #SBATCH -J myjob #SBATCH -c 4 #SBATCH -t 24:00:00 #SBATCH -p day module load Apps/Matlab/R2016b matlab -nodisplay -nosplash -r YourFunction < /dev/null Unless you specify otherwise (using > redirects), both output and error logs will show up in the slurm-jobid.out log file in the same directory as your submission script. Using More than 12 Cores with Matlab In Matlab, 12 workers is a poorly documented default limit (seemingly for historical reasons) when setting up the parallel environment. You can override it by explicitly setting up your parpool before calling parfor or other parallel functions. parpool(feature('NumCores')); Matlab Distributed Computing Engine (MDCE) The Matlab Distributed Computing Engine (MDCE) allows users to run parallel Matlab computations over multiple cluster compute nodes. To run parallel Matlab computations on any number of cores of a single compute node, please use ordinary Matlab (not MDCE) as described above to avoid tying up our limited number of licenses for MDCE. MDCE is installed on all the HPC clusters, and we provide scripts to make it easy to use. Currently, our license for MDCE is restricted to a total of 32 concurrent labs per cluster (aggregated over all jobs using MDCE on the cluster), plus an additional 128 licenses that float and are available on any of the clusters when the cluster-specific licenses are already in use. Use (MDCE) The first step required for use of MDCE is to develop a parallel Matlab program using the Parallel Computing Toolbox (PCT). The PCT allows you to run parallel computations on a single node. When used with MDCE, the PCT can enable you to run your computation across multiple nodes. In most cases, before running on multiple nodes, you should develop and test your algorithm on a single node using multiple cores. For the single-node case, you simply run ordinary Matlab as described above (either interactively or in batch mode) and make use of the PCT commands using the \"local\" cluster configuration. This capability is enabled for any Matlab invocation on any of the clusters, and there are no limitations on the number of concurrent PCT users in this case. If you intend to run on a single node, therefore, please do not use MDCE, since that would consume some of our limited quantity of multi-node MDCE licenses. For the multi-node case, you must run an MDCE server that is private to your job. We provide a script (yale_mdce_start.sh) that starts the server and the Matlab workers (known as \"labs\") for you. The yale_mdce_start.sh script has parameters that allow you to control the number of labs on each node, subject to license availability. (For details, see the comments in the file runit.sh shown later on this page.) The MDCE server will be terminated automatically when your cluster job ends, though we also provide a script (yale_mdce_stop.sh) to terminate it earlier if you wish. The yale_mdce scripts will be in your PATH once you have loaded a Matlab module file (e.g., Apps/Matlab/R2015a). To use the yale_mdce_start.sh script, you need to load module files for both Matlab and OpenMPI (see the runit.sh script below for an example). We have also developed a template batch script (runit.sh) that you can submit to the job scheduler to run your parallel Matlab program. The script loads module files, invokes yale_mdce_start.sh, and then runs Matlab in batch mode. You can copy the template and and customize it to meet your needs. If you prefer to run interactively, you can start an ordinary multi-node interactive session (similar to what's shown above) and run the setup commands in runit.sh by hand. Example Submission Script (for Slurm) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/bin/bash #SBATCH -J MDCE_JOB #SBATCH --ntasks=25 #SBATCH --time=24:00:00 #SBATCH --partition=day # Load Matlab and MPI module files module load Apps/Matlab/R2015a MPI/OpenMPI # Invoke yale_mdce_start.sh to set up a job manager and MDCE server # Note: yale_mdce_start.sh and runscript.m are in the MDCE_SCRIPTS subdirectory of the root Matlab directory (e.g., /home/apps/fas/Apps/Matlab on Omega). # The MDCE_SCRIPTS directory is added to your PATH by the Matlab module file loaded above. # Options for yale_mdce_start.sh: # -jmworkers: number of labs to run on same node as the job manager. # \"-jmworkers NN\" runs NN labs on job manager node # \"-jmworkers -1\" run 1 fewer than labs than the number of cores allocated on the node # Default: -1 # -nodeworkers: number of labs to run on nodes other than the job manager node. # \"-nodeworkers NN\" runs NN labs on each node # \"-nodeworkers -1\" run same number of labs as the number of cores allocated on each node # Default: -1 yale_mdce_start.sh -jmworkers -1 -nodeworkers -1 export MDCE_JM_NAME = ` cat MDCE_JM_NAME ` # invoke either runscript.m or your own M-file # (You need to modify runscript.m first to run your computations!!) # runscript.m uses the parallel cluster created by yale_mdce_start.sh. matlab -nodisplay < runscript.m Example runscript.m clear % CD TO PROPER DIRECTORY HERE, IF NECESSARY % FOLLOWING ASSUMES USE OF STANDARD YALE MDCE STARTUP SCRIPT p=parallel.cluster.MJS('Name',getenv('MDCE_JM_NAME')) nw = p.NumIdleWorkers ppool=p.parpool(nw) ppool.NumWorkers % INVOKE YOUR OWN SCRIPT HERE ppool.delete exit","title":"MATLAB"},{"location":"clusters-at-yale/guides/matlab/#matlab","text":"","title":"MATLAB"},{"location":"clusters-at-yale/guides/matlab/#find-matlab","text":"Run one of the commands below, which will list available versions and the corresponding module files: module spider matlab Load the appropriate module file. For example, to run version R2014a: module load MATLAB / 2014 a The module load command sets up your environment, including the PATH to find the proper version of the MATLAB program.","title":"Find MATLAB"},{"location":"clusters-at-yale/guides/matlab/#run-matlab","text":"Warning The MATLAB program is too large to fit on a login node. If you try to run it there, it will crash. Instead, launch it in an interactive or batch job (see below). To launch MATLAB, using matlab command. # launch the MATLAB GUI matlab # or launch the MATLAB command line prompt maltab -nodisplay # or to launch a script matlab -nodisplay < runscript.m","title":"Run MATLAB"},{"location":"clusters-at-yale/guides/matlab/#interactive-job","text":"To run Matlab interactively, you need to create an interactive session on a compute node. You could start an interactive session using 4 cores on 1 node using something like srun --pty --x11 -c 4 -p interactive -t 4:00:00 bash Once your interactive session starts, you can load the appropriate module file and start Matlab as described above. See our Slurm documentation for more detailed information on requesting resources for interactive jobs.","title":"Interactive Job"},{"location":"clusters-at-yale/guides/matlab/#batch-mode-without-a-gui","text":"Create a batch script containing both instructions to the scheduler and shell instructions to set up directories and start Matlab. At the point you wish to start Matlab, use a command like: matlab -nodisplay -nosplash -r YourFunction < /dev/null This command will run the contents of YourFunction.m. Your batch submission script must either be in or cd to the directory containing YourFunction.m for this to work. Below is a sample batch script to run Matlab in batch mode on Grace. If the name of the script is runit.sh, you would submit it using sbatch runit.sh Here's a script for Grace: 1 2 3 4 5 6 7 8 #!/bin/bash #SBATCH -J myjob #SBATCH -c 4 #SBATCH -t 24:00:00 #SBATCH -p day module load Apps/Matlab/R2016b matlab -nodisplay -nosplash -r YourFunction < /dev/null Unless you specify otherwise (using > redirects), both output and error logs will show up in the slurm-jobid.out log file in the same directory as your submission script.","title":"Batch Mode (without a GUI)"},{"location":"clusters-at-yale/guides/matlab/#using-more-than-12-cores-with-matlab","text":"In Matlab, 12 workers is a poorly documented default limit (seemingly for historical reasons) when setting up the parallel environment. You can override it by explicitly setting up your parpool before calling parfor or other parallel functions. parpool(feature('NumCores'));","title":"Using More than 12 Cores with Matlab"},{"location":"clusters-at-yale/guides/matlab/#matlab-distributed-computing-engine-mdce","text":"The Matlab Distributed Computing Engine (MDCE) allows users to run parallel Matlab computations over multiple cluster compute nodes. To run parallel Matlab computations on any number of cores of a single compute node, please use ordinary Matlab (not MDCE) as described above to avoid tying up our limited number of licenses for MDCE. MDCE is installed on all the HPC clusters, and we provide scripts to make it easy to use. Currently, our license for MDCE is restricted to a total of 32 concurrent labs per cluster (aggregated over all jobs using MDCE on the cluster), plus an additional 128 licenses that float and are available on any of the clusters when the cluster-specific licenses are already in use.","title":"Matlab Distributed Computing Engine (MDCE)"},{"location":"clusters-at-yale/guides/matlab/#use-mdce","text":"The first step required for use of MDCE is to develop a parallel Matlab program using the Parallel Computing Toolbox (PCT). The PCT allows you to run parallel computations on a single node. When used with MDCE, the PCT can enable you to run your computation across multiple nodes. In most cases, before running on multiple nodes, you should develop and test your algorithm on a single node using multiple cores. For the single-node case, you simply run ordinary Matlab as described above (either interactively or in batch mode) and make use of the PCT commands using the \"local\" cluster configuration. This capability is enabled for any Matlab invocation on any of the clusters, and there are no limitations on the number of concurrent PCT users in this case. If you intend to run on a single node, therefore, please do not use MDCE, since that would consume some of our limited quantity of multi-node MDCE licenses. For the multi-node case, you must run an MDCE server that is private to your job. We provide a script (yale_mdce_start.sh) that starts the server and the Matlab workers (known as \"labs\") for you. The yale_mdce_start.sh script has parameters that allow you to control the number of labs on each node, subject to license availability. (For details, see the comments in the file runit.sh shown later on this page.) The MDCE server will be terminated automatically when your cluster job ends, though we also provide a script (yale_mdce_stop.sh) to terminate it earlier if you wish. The yale_mdce scripts will be in your PATH once you have loaded a Matlab module file (e.g., Apps/Matlab/R2015a). To use the yale_mdce_start.sh script, you need to load module files for both Matlab and OpenMPI (see the runit.sh script below for an example). We have also developed a template batch script (runit.sh) that you can submit to the job scheduler to run your parallel Matlab program. The script loads module files, invokes yale_mdce_start.sh, and then runs Matlab in batch mode. You can copy the template and and customize it to meet your needs. If you prefer to run interactively, you can start an ordinary multi-node interactive session (similar to what's shown above) and run the setup commands in runit.sh by hand.","title":"Use (MDCE)"},{"location":"clusters-at-yale/guides/matlab/#example-submission-script-for-slurm","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/bin/bash #SBATCH -J MDCE_JOB #SBATCH --ntasks=25 #SBATCH --time=24:00:00 #SBATCH --partition=day # Load Matlab and MPI module files module load Apps/Matlab/R2015a MPI/OpenMPI # Invoke yale_mdce_start.sh to set up a job manager and MDCE server # Note: yale_mdce_start.sh and runscript.m are in the MDCE_SCRIPTS subdirectory of the root Matlab directory (e.g., /home/apps/fas/Apps/Matlab on Omega). # The MDCE_SCRIPTS directory is added to your PATH by the Matlab module file loaded above. # Options for yale_mdce_start.sh: # -jmworkers: number of labs to run on same node as the job manager. # \"-jmworkers NN\" runs NN labs on job manager node # \"-jmworkers -1\" run 1 fewer than labs than the number of cores allocated on the node # Default: -1 # -nodeworkers: number of labs to run on nodes other than the job manager node. # \"-nodeworkers NN\" runs NN labs on each node # \"-nodeworkers -1\" run same number of labs as the number of cores allocated on each node # Default: -1 yale_mdce_start.sh -jmworkers -1 -nodeworkers -1 export MDCE_JM_NAME = ` cat MDCE_JM_NAME ` # invoke either runscript.m or your own M-file # (You need to modify runscript.m first to run your computations!!) # runscript.m uses the parallel cluster created by yale_mdce_start.sh. matlab -nodisplay < runscript.m","title":"Example Submission Script (for Slurm)"},{"location":"clusters-at-yale/guides/matlab/#example-runscriptm","text":"clear % CD TO PROPER DIRECTORY HERE, IF NECESSARY % FOLLOWING ASSUMES USE OF STANDARD YALE MDCE STARTUP SCRIPT p=parallel.cluster.MJS('Name',getenv('MDCE_JM_NAME')) nw = p.NumIdleWorkers ppool=p.parpool(nw) ppool.NumWorkers % INVOKE YOUR OWN SCRIPT HERE ppool.delete exit","title":"Example runscript.m"},{"location":"clusters-at-yale/guides/parallel/","text":"GNU Parallel GNU Parallel a simple but powerful way to run independent tasks in parallel. Although it is possible to run on multiple nodes, it is simplest to run on multiple cpus of a single node, and that is what we will consider here. Note that what is presented here just scratches the surface of what parallel can do. Basic Examples Loop Let's parallelize the following bash loop that prints the letters a through f using bash's brace expansion : for letter in { a..f } ; do echo $letter done ... which produces the following output: a b c d e f To achieve the same result, parallel starts some number of workers and then runs tasks on them. The number of workers and tasks need not be the same. You specify the number of workers with -j . The tasks can be generated with a list of arguments specified after the separator ::: . For parallel to perform well, you should allocate at least the same number of CPUs as workers with the slurm option --cpus-per-task or more simply -c . srun --pty -p interactive -c 4 bash module load parallel parallel -j 4 \"echo {}\" ::: { a..f } This runs four workers that each run echo , filling in the argument {} with the next item in the list. This produces the output: Nested Loop Let's parallelize the following nested bash loop. for letter in { a..c } do for number in { 1 ..7..2 } do echo $letter $number done done ... which produces the following output: a 1 a 2 a 3 b 1 b 2 b 3 c 1 c 2 c 3 You can use the ::: separator with parallel to specify multiple lists of parameters you would like to iterate over. Then you can refer to them by one-based index, e.g. list one is {1} . Using these, you can ask parallel to execute combinations of parameters. Here is a way to recreate the result of the serial bash loop above: parallel -j 4 \"echo {1} {2}\" ::: { a..c } ::: { 1 ..3 } Advanced Examples md5sum You have a number of files scattered throughout a directory tree. Their names end with fastq.gz, e.g. d1/d3/sample3.fastq.gz. You'd like to run md5sum on each, and put the output in a file in the same directory, with a filename ending with .md5sum, e.g. d1/d3/sample3.md5sum. Here is a script that will do that in parallel, using 16 cpus on one node of the cluster: #!/bin/bash #SBATCH -c 16 module load parallel parallel -j ${ SLURM_CPUS_PER_TASK } --plus \"echo {}; md5sum {} > {/fastq.gz/md5sum.new}\" ::: $( find . -name \"*.fastq.gz\" -print ) The $(find . -name \"*.fastq.gz\" -print) portion of the command returns all of the files of interest. They will be plugged into the {} in the md5sum command. {/fastq.gz/md5sum.new} does a string replacement on the filename, producing the desired output filename. String replacement requires the --plus flag to parallel, which enables a number of powerful string manipulation features. Finally, we pass -j ${SLURM_CPUS_PER_TASK} so that parallel will use all of the allocated cpus, however many there are. Parameter Sweep You want to run a simulation program that takes a number of input parameters, and you want to sample a variety of values for each parameter. #!/bin/bash #SBATCH -c 16 module load parallel parallel -j ${ SLURM_CPUS_PER_TASK } simulate { 1 } { 2 } { 3 } ::: { 1 ..5 } ::: 2 16 ::: { 5 ..50..5 } This will run 100 jobs, each with parameters that vary as : simulate 1 2 5 simulate 1 2 10 simulate 1 2 15 ... simulate 5 16 45 simulate 5 16 50 If simulate doesn't create unique output based on parameters, you can use redirection so you can review results from each task. You'll need to use quotes so that the > is seen as part of the command: parallel -j ${ SLURM_CPUS_PER_TASK } \"simulate {1} {2} {3} > results_{1}_{2}_{3}.out\" ::: $( seq 1 5 ) ::: 2 16 ::: $( seq 5 5 50 )","title":"GNU Parallel"},{"location":"clusters-at-yale/guides/parallel/#gnu-parallel","text":"GNU Parallel a simple but powerful way to run independent tasks in parallel. Although it is possible to run on multiple nodes, it is simplest to run on multiple cpus of a single node, and that is what we will consider here. Note that what is presented here just scratches the surface of what parallel can do.","title":"GNU Parallel"},{"location":"clusters-at-yale/guides/parallel/#basic-examples","text":"","title":"Basic Examples"},{"location":"clusters-at-yale/guides/parallel/#loop","text":"Let's parallelize the following bash loop that prints the letters a through f using bash's brace expansion : for letter in { a..f } ; do echo $letter done ... which produces the following output: a b c d e f To achieve the same result, parallel starts some number of workers and then runs tasks on them. The number of workers and tasks need not be the same. You specify the number of workers with -j . The tasks can be generated with a list of arguments specified after the separator ::: . For parallel to perform well, you should allocate at least the same number of CPUs as workers with the slurm option --cpus-per-task or more simply -c . srun --pty -p interactive -c 4 bash module load parallel parallel -j 4 \"echo {}\" ::: { a..f } This runs four workers that each run echo , filling in the argument {} with the next item in the list. This produces the output:","title":"Loop"},{"location":"clusters-at-yale/guides/parallel/#nested-loop","text":"Let's parallelize the following nested bash loop. for letter in { a..c } do for number in { 1 ..7..2 } do echo $letter $number done done ... which produces the following output: a 1 a 2 a 3 b 1 b 2 b 3 c 1 c 2 c 3 You can use the ::: separator with parallel to specify multiple lists of parameters you would like to iterate over. Then you can refer to them by one-based index, e.g. list one is {1} . Using these, you can ask parallel to execute combinations of parameters. Here is a way to recreate the result of the serial bash loop above: parallel -j 4 \"echo {1} {2}\" ::: { a..c } ::: { 1 ..3 }","title":"Nested Loop"},{"location":"clusters-at-yale/guides/parallel/#advanced-examples","text":"","title":"Advanced Examples"},{"location":"clusters-at-yale/guides/parallel/#md5sum","text":"You have a number of files scattered throughout a directory tree. Their names end with fastq.gz, e.g. d1/d3/sample3.fastq.gz. You'd like to run md5sum on each, and put the output in a file in the same directory, with a filename ending with .md5sum, e.g. d1/d3/sample3.md5sum. Here is a script that will do that in parallel, using 16 cpus on one node of the cluster: #!/bin/bash #SBATCH -c 16 module load parallel parallel -j ${ SLURM_CPUS_PER_TASK } --plus \"echo {}; md5sum {} > {/fastq.gz/md5sum.new}\" ::: $( find . -name \"*.fastq.gz\" -print ) The $(find . -name \"*.fastq.gz\" -print) portion of the command returns all of the files of interest. They will be plugged into the {} in the md5sum command. {/fastq.gz/md5sum.new} does a string replacement on the filename, producing the desired output filename. String replacement requires the --plus flag to parallel, which enables a number of powerful string manipulation features. Finally, we pass -j ${SLURM_CPUS_PER_TASK} so that parallel will use all of the allocated cpus, however many there are.","title":"md5sum"},{"location":"clusters-at-yale/guides/parallel/#parameter-sweep","text":"You want to run a simulation program that takes a number of input parameters, and you want to sample a variety of values for each parameter. #!/bin/bash #SBATCH -c 16 module load parallel parallel -j ${ SLURM_CPUS_PER_TASK } simulate { 1 } { 2 } { 3 } ::: { 1 ..5 } ::: 2 16 ::: { 5 ..50..5 } This will run 100 jobs, each with parameters that vary as : simulate 1 2 5 simulate 1 2 10 simulate 1 2 15 ... simulate 5 16 45 simulate 5 16 50 If simulate doesn't create unique output based on parameters, you can use redirection so you can review results from each task. You'll need to use quotes so that the > is seen as part of the command: parallel -j ${ SLURM_CPUS_PER_TASK } \"simulate {1} {2} {3} > results_{1}_{2}_{3}.out\" ::: $( seq 1 5 ) ::: 2 16 ::: $( seq 5 5 50 )","title":"Parameter Sweep"},{"location":"clusters-at-yale/guides/r/","text":"R Load R Run one of the commands below, which will list available versions and the corresponding module files: module avail R Load the appropriate module file. For example, to run version 3.4.1: module load R / 3.4.1 - foss-2016b The module load command sets up your environment, including the PATH to find the proper version of R. Run R To run R, launch it using the R command. # launch an R session R # or to launch a script R --slave -f myscript.R Warning The R program is too large to fit on a login node. If you try to run it there, it will crash. Instead, launch it in an interactive or batch job (see below). Interactive Job To run R interactively, you need to launch an interactive session on a compute node. For example srun --pty -p interactive -t 4:00:00 bash Once your interactive session starts, you can load the appropriate module file and start R as described above. See our Slurm documentation for more detailed information on requesting resources for interactive jobs. Batch Mode To run R in batch mode, you create a batch script . In that script, you would invoke your R script in batch mode. 1 2 3 4 #!/bin/bash #SBATCH -J my_r_program R --slave -f myscript.R Install Additional R Packages Our R module has some of the most commonly used packages pre-installed, such as Rmpi. If you need additional R packages not already included, we recommend installing them into your own directories using the install.package() R function. Contact us at hpc@yale.edu if you run into issues.","title":"R"},{"location":"clusters-at-yale/guides/r/#r","text":"","title":"R"},{"location":"clusters-at-yale/guides/r/#load-r","text":"Run one of the commands below, which will list available versions and the corresponding module files: module avail R Load the appropriate module file. For example, to run version 3.4.1: module load R / 3.4.1 - foss-2016b The module load command sets up your environment, including the PATH to find the proper version of R.","title":"Load R"},{"location":"clusters-at-yale/guides/r/#run-r","text":"To run R, launch it using the R command. # launch an R session R # or to launch a script R --slave -f myscript.R Warning The R program is too large to fit on a login node. If you try to run it there, it will crash. Instead, launch it in an interactive or batch job (see below).","title":"Run R"},{"location":"clusters-at-yale/guides/r/#interactive-job","text":"To run R interactively, you need to launch an interactive session on a compute node. For example srun --pty -p interactive -t 4:00:00 bash Once your interactive session starts, you can load the appropriate module file and start R as described above. See our Slurm documentation for more detailed information on requesting resources for interactive jobs.","title":"Interactive Job"},{"location":"clusters-at-yale/guides/r/#batch-mode","text":"To run R in batch mode, you create a batch script . In that script, you would invoke your R script in batch mode. 1 2 3 4 #!/bin/bash #SBATCH -J my_r_program R --slave -f myscript.R","title":"Batch Mode"},{"location":"clusters-at-yale/guides/r/#install-additional-r-packages","text":"Our R module has some of the most commonly used packages pre-installed, such as Rmpi. If you need additional R packages not already included, we recommend installing them into your own directories using the install.package() R function. Contact us at hpc@yale.edu if you run into issues.","title":"Install Additional R Packages"},{"location":"clusters-at-yale/guides/singularity/","text":"Singularity Singularity is a linux container technology that is well suited to use in shared-user environments such as the clusters we maintain at Yale. It is similar to Docker ; You can bring with you a stack of software, libraries, and a Linux operating system that is independent of the host computer you run the container on. This can be very useful if you want to share your software environment with other researchers or yourself across several computers. Because Singularity containers run as the user that started them and mount home directories by default, you can usually see the data you're interested in working on that is stored on a host computer without any extra work. Below we will outline some common use cases covering the creation and use of containers. There is also excellent documentation available on the full and official user guide for Singularity . We are happy to help, just email us with your questions. Warning On the Yale clusters, Singularity is not installed on login nodes. You will need to run it from compute nodes. Singularity Containers Images are the file(s) you use to run your container. Singularity images are single files that usually end in .simg or .img and are read-only by default, meaning changes you make to the environment inside the container are not persistent. Use a Pre-existing Container If someone has already built a container that suits your needs, you can use it directly. You can either copy the singularity image file directly with scp , rsync , or globus . You can also use Docker Hub or Singularity Hub images hosted on their respective registries. Container images can take up a lot of disk space (dozens of gigabytes), so you may want to change the default location singularity uses to cache these files. To do this before getting started, you should add something like the example below to to your ~/.bashrc file: # set SINGULARITY_CACHEDIR if you want to pull files (which can get big) somewhere other than $HOME/.singularity # e.g. export SINGULARITY_CACHEDIR=~/project/.singularity Here are some examples of getting containers already built by someone else with singularity: # from Docker Hub (https://hub.docker.com/) singularity build ubuntu-18.10.simg docker://ubuntu:18.10 singularity build tensorflow-10.0-py3.simg docker://tensorflow/tensorflow:1.10.0-py3 # from Singularity Hub (https://singularity-hub.org/) singularity build bioconvert-latest.simg shub://biokit/bioconvert:latest Build Your Own Container You can define a container image to be exactly how you want/need it to be, including applications, libraries, and files of your choosing with a recipe file. Singularity recipe files are similar to Docker's Dockerfile , but use different syntax. To build a container from a recipe file, you need administrative privileges on a Linux machine where Singularity is installed . If you don't have such an environment available, get in touch with us for help getting set up. For full recipes and more documentation please see the singularity site . Header Every container recipe must begin with a header that defines what image to start with, or bootstrap from. This can be an official Linux distribution or someone else's container that gets you nearly what you want. To start from Ubuntu Bionic Beaver (18.04 LTS): Bootstrap : docker From : ubuntu : 18.04 Or an Nvidia developer image Bootstrap : docker From : nvidia / cuda : 9.2 - cudnn7 - devel - ubuntu18 . 04 The rest of the sections all begin with % and the section name. You will see section contents indented by convention, but this is not required. %labels The labels section allows you to define metadata for your container: %labels Name Maintainer \"YCRC Support Team\" < hpc @ yale . edu > Version v99 .9 Architecture x86_64 URL https : //research.computing.yale.edu/</hpc@yale.edu> You can examine container metadata with the singularity inspect command. %files If you'd like to copy any files from the system you are building on, you do so in the %files section. Each line in the files section is a pair of source and destination paths, where the source is on your host system, and destination is a path in the container. %files sample_data . tar / opt / sample_data / example_script . sh / opt / sample_data / %post The post section is where you can run updates, installs, etc in your container to customize it. %post echo \"Customizing Ubuntu\" apt - get update apt - get - y install software - properties - common build - essential cmake add - apt - repository universe apt - get update apt - get - y libboost - all - dev libgl1 - mesa - dev libglu1 - mesa - dev cd / tmp git clone https : //github.com/gitdudette/myapp && cd myapp # ... etc etc %environment The environment section allows you to define environment variables for your container. These variables are available when you run the built container, not during its build. %environment export PATH =/ opt / my_app / bin : $PATH export LD_LIBRARY_PATH =/ opt / my_app / lib : $LD_LIBRARY_PATH Building To finally build your container after saving your recipe file as my_app.def , for example, you would run singularity build my_app.simg my_app.def Use a Container Image Once you have a container image, you can run it as a part of a batch job, or interactively. Interactively To get a shell in a container so you can interactively work in its environment: singularity shell --shell /bin/bash containername.simg In a Job Script You can also run applications from your container non-interactively as you would in a batch job. If I wanted to run a script called my_script.py using my container's python: singularity exec containername.simg python my_script.py Environment Variables If you are unsure if you are running inside or outside your container, you can run: echo $SINGULARITY_NAME If you get back text, you are in your container. If you'd like to pass environment variables into your container, you can do so by defining them prefixed with SINGULARITYENV_ . For Example: export SINGULARITYENV_BLASTDB=/home/me/db/blast singularity exec my_blast_image.simg env | grep BLAST Should return BLASTDB=/home/me/db/blast , which means you set the BLASTDB environment variable in the container properly. Additional Notes MPI MPI support for Singularity is relatively straight-forward. The only thing to watch is to make sure that you are using the same version of MPI inside your container as you are on the cluster. GPUs You can use gpu-accelerated code inside your container, which will need most everything also installed in your container (e.g. CUDA, cuDNN). In order for your applications to have access to the right drivers on the host machine, use the --nv flag. For example: singularity exec --nv tensorflow-10.0-py3.simg python ./my-tf-model.py Home Directories Sometimes the maintainer of a docker container you are trying to use installed software into a special user's home directory. If you need access to someone's home directory that exists in the container and not on the host, you should add the --contain option. Unfortunately, you will also then have to explicitly tell Singularity about the paths that you want to use from inside the container with the --bind option. singularity shell --shell /bin/bash --contain --bind /gpfs/ysm/project/be59:/home/be59/project bioconvert-latest.simg","title":"Singularity"},{"location":"clusters-at-yale/guides/singularity/#singularity","text":"Singularity is a linux container technology that is well suited to use in shared-user environments such as the clusters we maintain at Yale. It is similar to Docker ; You can bring with you a stack of software, libraries, and a Linux operating system that is independent of the host computer you run the container on. This can be very useful if you want to share your software environment with other researchers or yourself across several computers. Because Singularity containers run as the user that started them and mount home directories by default, you can usually see the data you're interested in working on that is stored on a host computer without any extra work. Below we will outline some common use cases covering the creation and use of containers. There is also excellent documentation available on the full and official user guide for Singularity . We are happy to help, just email us with your questions. Warning On the Yale clusters, Singularity is not installed on login nodes. You will need to run it from compute nodes.","title":"Singularity"},{"location":"clusters-at-yale/guides/singularity/#singularity-containers","text":"Images are the file(s) you use to run your container. Singularity images are single files that usually end in .simg or .img and are read-only by default, meaning changes you make to the environment inside the container are not persistent.","title":"Singularity Containers"},{"location":"clusters-at-yale/guides/singularity/#use-a-pre-existing-container","text":"If someone has already built a container that suits your needs, you can use it directly. You can either copy the singularity image file directly with scp , rsync , or globus . You can also use Docker Hub or Singularity Hub images hosted on their respective registries. Container images can take up a lot of disk space (dozens of gigabytes), so you may want to change the default location singularity uses to cache these files. To do this before getting started, you should add something like the example below to to your ~/.bashrc file: # set SINGULARITY_CACHEDIR if you want to pull files (which can get big) somewhere other than $HOME/.singularity # e.g. export SINGULARITY_CACHEDIR=~/project/.singularity Here are some examples of getting containers already built by someone else with singularity: # from Docker Hub (https://hub.docker.com/) singularity build ubuntu-18.10.simg docker://ubuntu:18.10 singularity build tensorflow-10.0-py3.simg docker://tensorflow/tensorflow:1.10.0-py3 # from Singularity Hub (https://singularity-hub.org/) singularity build bioconvert-latest.simg shub://biokit/bioconvert:latest","title":"Use a Pre-existing Container"},{"location":"clusters-at-yale/guides/singularity/#build-your-own-container","text":"You can define a container image to be exactly how you want/need it to be, including applications, libraries, and files of your choosing with a recipe file. Singularity recipe files are similar to Docker's Dockerfile , but use different syntax. To build a container from a recipe file, you need administrative privileges on a Linux machine where Singularity is installed . If you don't have such an environment available, get in touch with us for help getting set up. For full recipes and more documentation please see the singularity site .","title":"Build Your Own Container"},{"location":"clusters-at-yale/guides/singularity/#header","text":"Every container recipe must begin with a header that defines what image to start with, or bootstrap from. This can be an official Linux distribution or someone else's container that gets you nearly what you want. To start from Ubuntu Bionic Beaver (18.04 LTS): Bootstrap : docker From : ubuntu : 18.04 Or an Nvidia developer image Bootstrap : docker From : nvidia / cuda : 9.2 - cudnn7 - devel - ubuntu18 . 04 The rest of the sections all begin with % and the section name. You will see section contents indented by convention, but this is not required.","title":"Header"},{"location":"clusters-at-yale/guides/singularity/#labels","text":"The labels section allows you to define metadata for your container: %labels Name Maintainer \"YCRC Support Team\" < hpc @ yale . edu > Version v99 .9 Architecture x86_64 URL https : //research.computing.yale.edu/</hpc@yale.edu> You can examine container metadata with the singularity inspect command.","title":"%labels"},{"location":"clusters-at-yale/guides/singularity/#files","text":"If you'd like to copy any files from the system you are building on, you do so in the %files section. Each line in the files section is a pair of source and destination paths, where the source is on your host system, and destination is a path in the container. %files sample_data . tar / opt / sample_data / example_script . sh / opt / sample_data /","title":"%files"},{"location":"clusters-at-yale/guides/singularity/#post","text":"The post section is where you can run updates, installs, etc in your container to customize it. %post echo \"Customizing Ubuntu\" apt - get update apt - get - y install software - properties - common build - essential cmake add - apt - repository universe apt - get update apt - get - y libboost - all - dev libgl1 - mesa - dev libglu1 - mesa - dev cd / tmp git clone https : //github.com/gitdudette/myapp && cd myapp # ... etc etc","title":"%post"},{"location":"clusters-at-yale/guides/singularity/#environment","text":"The environment section allows you to define environment variables for your container. These variables are available when you run the built container, not during its build. %environment export PATH =/ opt / my_app / bin : $PATH export LD_LIBRARY_PATH =/ opt / my_app / lib : $LD_LIBRARY_PATH","title":"%environment"},{"location":"clusters-at-yale/guides/singularity/#building","text":"To finally build your container after saving your recipe file as my_app.def , for example, you would run singularity build my_app.simg my_app.def","title":"Building"},{"location":"clusters-at-yale/guides/singularity/#use-a-container-image","text":"Once you have a container image, you can run it as a part of a batch job, or interactively.","title":"Use a Container Image"},{"location":"clusters-at-yale/guides/singularity/#interactively","text":"To get a shell in a container so you can interactively work in its environment: singularity shell --shell /bin/bash containername.simg","title":"Interactively"},{"location":"clusters-at-yale/guides/singularity/#in-a-job-script","text":"You can also run applications from your container non-interactively as you would in a batch job. If I wanted to run a script called my_script.py using my container's python: singularity exec containername.simg python my_script.py","title":"In a Job Script"},{"location":"clusters-at-yale/guides/singularity/#environment-variables","text":"If you are unsure if you are running inside or outside your container, you can run: echo $SINGULARITY_NAME If you get back text, you are in your container. If you'd like to pass environment variables into your container, you can do so by defining them prefixed with SINGULARITYENV_ . For Example: export SINGULARITYENV_BLASTDB=/home/me/db/blast singularity exec my_blast_image.simg env | grep BLAST Should return BLASTDB=/home/me/db/blast , which means you set the BLASTDB environment variable in the container properly.","title":"Environment Variables"},{"location":"clusters-at-yale/guides/singularity/#additional-notes","text":"","title":"Additional Notes"},{"location":"clusters-at-yale/guides/singularity/#mpi","text":"MPI support for Singularity is relatively straight-forward. The only thing to watch is to make sure that you are using the same version of MPI inside your container as you are on the cluster.","title":"MPI"},{"location":"clusters-at-yale/guides/singularity/#gpus","text":"You can use gpu-accelerated code inside your container, which will need most everything also installed in your container (e.g. CUDA, cuDNN). In order for your applications to have access to the right drivers on the host machine, use the --nv flag. For example: singularity exec --nv tensorflow-10.0-py3.simg python ./my-tf-model.py","title":"GPUs"},{"location":"clusters-at-yale/guides/singularity/#home-directories","text":"Sometimes the maintainer of a docker container you are trying to use installed software into a special user's home directory. If you need access to someone's home directory that exists in the container and not on the host, you should add the --contain option. Unfortunately, you will also then have to explicitly tell Singularity about the paths that you want to use from inside the container with the --bind option. singularity shell --shell /bin/bash --contain --bind /gpfs/ysm/project/be59:/home/be59/project bioconvert-latest.simg","title":"Home Directories"},{"location":"clusters-at-yale/guides/tmux/","text":"tmux tmux is a \"terminal multiplexer\", it enables a number of terminals (or windows) to be accessed and controlled from a single terminal. tmux is a great way to save an interactive session between connections you make to the clusters. You can reconnect to the session from a workstation in your lab or from your laptop from home! Get Started To begin a tmux session named myproject, type tmux new -s myproject You should see a bar across the bottom of your terminal window now that gives you some information about your session. If you are disconnected or detached from this session, anything you were doing will still be there waiting when you reattach The most important shortcut to remember is Ctrl + b (hold the ctrl or control key, then type \"b\"). This is how you signal to tmux that the following keystroke is meant for it and not the session you are working in. For example: if you want to gracefully detach from your session, you can type Ctrl + b , then d for detach. To reattach to our sample tmux session after detatching, type: tmux attach -t myproject #If you are lazy and have only one session running, #This works too: tmux a Lines starting with a \"#\" denote a commented line, which aren't read as code Finally, to exit, you can type exit or Ctrl + d tmux on the Clusters Using tmux on the cluster allows you to create interactive allocations that you can detach from. Normally, if you get an interactive allocation (e.g. srun --pty) then disconnect from the cluster, for example by putting your laptop to sleep, your allocation will be terminated and your job killed. Using tmux, you can detach gracefully and tmux will maintain your allocation. Here is how to do this correctly: ssh to your cluster of choice Start tmux Inside your tmux session, submit an interactive job with srun. See the Slurm documentation for more details Inside your job allocation (on a compute node), start your application (e.g. matlab) Detach from tmux by typing Ctrl + b then d Later, on the same login node, reattach by running tmux attach Make sure to: run tmux on the login node, NOT on compute nodes run srun inside tmux, not the reverse. Windows and Panes tmux allows you to create, toggle between and manipulate panes and windows in your session. A window is the whole screen that tmux displays to you. Panes are subdivisions in the curent window, where each runs an independent terminal. Especially at first, you probably won't need more than one pane at a time. Multiple windows can be created and run off-screen. Here is an example where this may be useful. Say you just submitted an interactive job that is running on a compute node inside your tmux session. [ be59@farnam2 ~ ] $ tmux new -s analysis # I am in my tmux session now [ be59@farnam2 ~ ] $ srun --pty -p interactive bash [ be59@c23n08 ~ ] $ ./my_fancy_analysis.sh Now you can easily monitor its CPU and memory utilization without ever taking your eyes off of it by creating a new pane and running top there. Split your window by typing: Ctrl + b then % ssh into the compute node you are working on, then run top to watch your work as it runs all from the same window. # I'm in a new pane now. [ be59@farnam2 ~ ] $ ssh c23n08 [ be59@c23n08 ~ ] $ top Your view will look something like this: To switch back and forth between panes, type Ctrl + b then o","title":"tmux"},{"location":"clusters-at-yale/guides/tmux/#tmux","text":"tmux is a \"terminal multiplexer\", it enables a number of terminals (or windows) to be accessed and controlled from a single terminal. tmux is a great way to save an interactive session between connections you make to the clusters. You can reconnect to the session from a workstation in your lab or from your laptop from home!","title":"tmux"},{"location":"clusters-at-yale/guides/tmux/#get-started","text":"To begin a tmux session named myproject, type tmux new -s myproject You should see a bar across the bottom of your terminal window now that gives you some information about your session. If you are disconnected or detached from this session, anything you were doing will still be there waiting when you reattach The most important shortcut to remember is Ctrl + b (hold the ctrl or control key, then type \"b\"). This is how you signal to tmux that the following keystroke is meant for it and not the session you are working in. For example: if you want to gracefully detach from your session, you can type Ctrl + b , then d for detach. To reattach to our sample tmux session after detatching, type: tmux attach -t myproject #If you are lazy and have only one session running, #This works too: tmux a Lines starting with a \"#\" denote a commented line, which aren't read as code Finally, to exit, you can type exit or Ctrl + d","title":"Get Started"},{"location":"clusters-at-yale/guides/tmux/#tmux-on-the-clusters","text":"Using tmux on the cluster allows you to create interactive allocations that you can detach from. Normally, if you get an interactive allocation (e.g. srun --pty) then disconnect from the cluster, for example by putting your laptop to sleep, your allocation will be terminated and your job killed. Using tmux, you can detach gracefully and tmux will maintain your allocation. Here is how to do this correctly: ssh to your cluster of choice Start tmux Inside your tmux session, submit an interactive job with srun. See the Slurm documentation for more details Inside your job allocation (on a compute node), start your application (e.g. matlab) Detach from tmux by typing Ctrl + b then d Later, on the same login node, reattach by running tmux attach Make sure to: run tmux on the login node, NOT on compute nodes run srun inside tmux, not the reverse.","title":"tmux on the Clusters"},{"location":"clusters-at-yale/guides/tmux/#windows-and-panes","text":"tmux allows you to create, toggle between and manipulate panes and windows in your session. A window is the whole screen that tmux displays to you. Panes are subdivisions in the curent window, where each runs an independent terminal. Especially at first, you probably won't need more than one pane at a time. Multiple windows can be created and run off-screen. Here is an example where this may be useful. Say you just submitted an interactive job that is running on a compute node inside your tmux session. [ be59@farnam2 ~ ] $ tmux new -s analysis # I am in my tmux session now [ be59@farnam2 ~ ] $ srun --pty -p interactive bash [ be59@c23n08 ~ ] $ ./my_fancy_analysis.sh Now you can easily monitor its CPU and memory utilization without ever taking your eyes off of it by creating a new pane and running top there. Split your window by typing: Ctrl + b then % ssh into the compute node you are working on, then run top to watch your work as it runs all from the same window. # I'm in a new pane now. [ be59@farnam2 ~ ] $ ssh c23n08 [ be59@c23n08 ~ ] $ top Your view will look something like this: To switch back and forth between panes, type Ctrl + b then o","title":"Windows and Panes"},{"location":"clusters-at-yale/guides/vasp/","text":"VASP NOTE: VASP requires a paid license. If you looking to use VASP, but your research group has not purchased a license, please do not use the cluster installations without first contacting hpc@yale.edu . Thank you for your cooperation. VASP and Slurm In Slurm, there is big difference between --ntasks and --cpus-per-task which is explained in our Requesting Resources documentation . For the purposes of VASP, --cpu-per-tasks should always equal NCORE (in your INCAR file). Then --ntasks should be equal to the total number of cores you want, divided by --cpu-per-tasks . VASP has two parameters for controlling processor layouts, NCORE and NPAR , but you only need to set one of them. If you set NCORE , you don\u2019t need to set NPAR . Instead VASP will automatically set NPAR to --ntasks . So the formula should be: In your mpirun line, you should specify the number of MPI tasks as: mpirun -n $SLURM_NTASKS vasp_std You don\u2019t need to specify \u2014nodes unless you are trying to force the tasks on a certain number of nodes (which will likely increase you wait time with minimal speed up). But regardless, --nodes shouldn\u2019t be part of the total number of cpu calculation. Cores Layout Examples If you want 40 cores (2 nodes and 20 cpus per node): in your submission script: #SBATCH --ntasks=2 #SBATCH --cpus-per-task=20 mpirun -n 2 vasp_std in INCAR : NCORE=20 You may however find that the wait time to get 20 cores on two nodes can be very long since cores request via --cpus-per-task can\u2019t span multiple nodes. Instead you might want to try breaking it up into smaller chunks. Therefore, try: in your submission script: #SBATCH --ntasks=4 #SBATCH --cpus-per-task=10 mpirun -n 4 vasp_std in INCAR : NCORE=10 which would likely spread over 4 nodes using 10 cores each and spend less time in the queue. Omega On Omega, since cores are assigned as whole 8-core nodes, NCORE should always be equal to 8 and then you can just request \u2014ntasks in multiples of 8. in your submission script: #SBATCH --ntasks=16 # some multiple of 8 mpirun -n $SLURM_NTASKS vasp_std in INCAR : NCORE=8 Additional Reading Here is some documentation on how to optimally configure NCORE and NPAR: https://cms.mpi.univie.ac.at/wiki/index.php/NCORE https://cms.mpi.univie.ac.at/wiki/index.php/NPAR https://www.nsc.liu.se/~pla/blog/2015/01/12/vasp-how-many-cores/","title":"VASP"},{"location":"clusters-at-yale/guides/vasp/#vasp","text":"NOTE: VASP requires a paid license. If you looking to use VASP, but your research group has not purchased a license, please do not use the cluster installations without first contacting hpc@yale.edu . Thank you for your cooperation.","title":"VASP"},{"location":"clusters-at-yale/guides/vasp/#vasp-and-slurm","text":"In Slurm, there is big difference between --ntasks and --cpus-per-task which is explained in our Requesting Resources documentation . For the purposes of VASP, --cpu-per-tasks should always equal NCORE (in your INCAR file). Then --ntasks should be equal to the total number of cores you want, divided by --cpu-per-tasks . VASP has two parameters for controlling processor layouts, NCORE and NPAR , but you only need to set one of them. If you set NCORE , you don\u2019t need to set NPAR . Instead VASP will automatically set NPAR to --ntasks . So the formula should be: In your mpirun line, you should specify the number of MPI tasks as: mpirun -n $SLURM_NTASKS vasp_std You don\u2019t need to specify \u2014nodes unless you are trying to force the tasks on a certain number of nodes (which will likely increase you wait time with minimal speed up). But regardless, --nodes shouldn\u2019t be part of the total number of cpu calculation.","title":"VASP and Slurm"},{"location":"clusters-at-yale/guides/vasp/#cores-layout-examples","text":"If you want 40 cores (2 nodes and 20 cpus per node): in your submission script: #SBATCH --ntasks=2 #SBATCH --cpus-per-task=20 mpirun -n 2 vasp_std in INCAR : NCORE=20 You may however find that the wait time to get 20 cores on two nodes can be very long since cores request via --cpus-per-task can\u2019t span multiple nodes. Instead you might want to try breaking it up into smaller chunks. Therefore, try: in your submission script: #SBATCH --ntasks=4 #SBATCH --cpus-per-task=10 mpirun -n 4 vasp_std in INCAR : NCORE=10 which would likely spread over 4 nodes using 10 cores each and spend less time in the queue.","title":"Cores Layout Examples"},{"location":"clusters-at-yale/guides/vasp/#omega","text":"On Omega, since cores are assigned as whole 8-core nodes, NCORE should always be equal to 8 and then you can just request \u2014ntasks in multiples of 8. in your submission script: #SBATCH --ntasks=16 # some multiple of 8 mpirun -n $SLURM_NTASKS vasp_std in INCAR : NCORE=8","title":"Omega"},{"location":"clusters-at-yale/guides/vasp/#additional-reading","text":"Here is some documentation on how to optimally configure NCORE and NPAR: https://cms.mpi.univie.ac.at/wiki/index.php/NCORE https://cms.mpi.univie.ac.at/wiki/index.php/NPAR https://www.nsc.liu.se/~pla/blog/2015/01/12/vasp-how-many-cores/","title":"Additional Reading"},{"location":"clusters-at-yale/job-scheduling/","text":"Run Jobs with Slurm Slurm is used to submit jobs to a specified set of compute resources, which are variously called queues or partitions. Slurm uses the term partition. Partitions, their defaults, limits and purposes are listed on each cluster page . To see more details about how jobs are scheduled see our information on factors affecting scheduling priority . Please be a good cluster citizen. Do not run jobs on login nodes (e.g. grace1, farnam2), as these can impact the sessions and connectivity of everyone else on the cluster. We also sometimes find jobs on the clusters that allocate resources incorrectly for the job that is running. Please see our documentation on Monitoring CPU and Memory Usage for examples of how to measure the resources your jobs use. If you find yourself wondering how best to schedule a job feel free to email us or come to office hours . Efficient jobs help you get your work done faster and free resources for others as well. Common Slurm Commands Submit a submission script (see below for details) sbatch <script> List queued and running jobs squeue -u $USER Cancel a queued job or kill a running job scancel <job_id> Check status of individual job (including failed or completed) sacct -j <job_id> Interactive Jobs Interactive jobs can be used for testing and troubleshooting code. By requesting an interactive job, you will be allocated resources and logged onto the node in a shell. srun --pty -p interactive bash This will assign a free node to you, allocating the requested number of CPUs, walltime, and memory, and put you within a shell on that node. You can run any number of commands within that shell. To free the allocated node, exit from the shell. Tip When using an interactive shell under slurm, your job is vulnerable to being killed if you lose your network connection. We recommend using tmux alleviate this. When using tmux , please be sure to keep track of your allocations and free those no longer needed! To use a GUI application (such as Matlab), when in an interactive job, use the --x11 flag: srun --pty --x11 -p interactive [ additional slurm options ] bash Warning For X11 forwarding to work, you need to have your local machine setup properly. Please see our X11 setup guide for more info. Batch Jobs To submit a job via Slurm, you first write a simple shell script called a \"submission script\" that wraps your job. A submission script is comprised of three parts: The program that should run the script. This is normally #!/bin/bash . The \"directives\" that tell the scheduler how to setup the computational resources for your job. These lines must appear before any other commands or definitions, otherwise they will be ignored. The actual \"script\" portion, which are the commands you want executed during your job. Here is an example script.sh that runs a job on one CPU on single node: #!/bin/bash #SBATCH --partition=general #SBATCH --job-name=my_job #SBATCH --ntasks=1 --nodes=1 #SBATCH --mem-per-cpu=6000 #SBATCH --time=12:00:00 #SBATCH --mail-type=ALL #SBATCH --mail-user=<email> ./myprog -p 20 arg1 arg2 arg3 ... Directives As shown in the above example, \"directives\" are comprised of #SBATCH followed by Slurm options. Most commonly used options include: Full Option Abbreviated Description --job-name -J Custom job name --partition -p Partition to run on --nodes -N Total number of nodes --ntasks -n Number of \"tasks\". For use with distributed parallelism. See below. --cpus-per-task -c # of CPUs allocated to each task. For use with shared memory parallelism. --ntasks-per-node Number of \"tasks\" per node. For use with distributed parallelism. See below. --time -t Maximum walltime of the job in the format D-HH:MM:SS (e.g. --time=1- for one day or --time=4:00:00 for 4 hours) --constraint -C specific node architecture (if applicable) --mem-per-cpu Memory requested per CPU in MB --mem Memory requested per node in MB --mail-user Mail address (alternatively, put your email address in ~/.forward) --mail-type Control emails to user on job events. Use ALL to receive email notications at the beginning and end of the job. Additional options can be found on in the official Slurm documentation . Resource Limit Enforcement Slurm uses the linux cgroup feature to enforce limits on CPUs, GPUs, and memory. Jobs are only permitted to run on a node if they have a valid allocation, and only within the limits specified by that limitation. Thus, if you request a single core from slurm (the default) and start a job that runs 20 parallel threads, those threads will be packed into a single CPU, and run very slowly. Similarly, if you do not explicitly request memory, your job will be granted a fairly modest default per CPU, and if your job attempts to exceed that amount, it will be killed. Using Private Partitions If you have special permission to submit to a partition that belongs to another group, you may be asked to assign a special \"account\" to your jobs in that partition. You will be given the name of this account when you get access the partition and then simple add the -A <account> flag to your submission command or as an additional directive in your submission script.","title":"Run Jobs with Slurm"},{"location":"clusters-at-yale/job-scheduling/#run-jobs-with-slurm","text":"Slurm is used to submit jobs to a specified set of compute resources, which are variously called queues or partitions. Slurm uses the term partition. Partitions, their defaults, limits and purposes are listed on each cluster page . To see more details about how jobs are scheduled see our information on factors affecting scheduling priority . Please be a good cluster citizen. Do not run jobs on login nodes (e.g. grace1, farnam2), as these can impact the sessions and connectivity of everyone else on the cluster. We also sometimes find jobs on the clusters that allocate resources incorrectly for the job that is running. Please see our documentation on Monitoring CPU and Memory Usage for examples of how to measure the resources your jobs use. If you find yourself wondering how best to schedule a job feel free to email us or come to office hours . Efficient jobs help you get your work done faster and free resources for others as well.","title":"Run Jobs with Slurm"},{"location":"clusters-at-yale/job-scheduling/#common-slurm-commands","text":"Submit a submission script (see below for details) sbatch <script> List queued and running jobs squeue -u $USER Cancel a queued job or kill a running job scancel <job_id> Check status of individual job (including failed or completed) sacct -j <job_id>","title":"Common Slurm Commands"},{"location":"clusters-at-yale/job-scheduling/#interactive-jobs","text":"Interactive jobs can be used for testing and troubleshooting code. By requesting an interactive job, you will be allocated resources and logged onto the node in a shell. srun --pty -p interactive bash This will assign a free node to you, allocating the requested number of CPUs, walltime, and memory, and put you within a shell on that node. You can run any number of commands within that shell. To free the allocated node, exit from the shell. Tip When using an interactive shell under slurm, your job is vulnerable to being killed if you lose your network connection. We recommend using tmux alleviate this. When using tmux , please be sure to keep track of your allocations and free those no longer needed! To use a GUI application (such as Matlab), when in an interactive job, use the --x11 flag: srun --pty --x11 -p interactive [ additional slurm options ] bash Warning For X11 forwarding to work, you need to have your local machine setup properly. Please see our X11 setup guide for more info.","title":"Interactive Jobs"},{"location":"clusters-at-yale/job-scheduling/#batch-jobs","text":"To submit a job via Slurm, you first write a simple shell script called a \"submission script\" that wraps your job. A submission script is comprised of three parts: The program that should run the script. This is normally #!/bin/bash . The \"directives\" that tell the scheduler how to setup the computational resources for your job. These lines must appear before any other commands or definitions, otherwise they will be ignored. The actual \"script\" portion, which are the commands you want executed during your job. Here is an example script.sh that runs a job on one CPU on single node: #!/bin/bash #SBATCH --partition=general #SBATCH --job-name=my_job #SBATCH --ntasks=1 --nodes=1 #SBATCH --mem-per-cpu=6000 #SBATCH --time=12:00:00 #SBATCH --mail-type=ALL #SBATCH --mail-user=<email> ./myprog -p 20 arg1 arg2 arg3 ...","title":"Batch Jobs"},{"location":"clusters-at-yale/job-scheduling/#directives","text":"As shown in the above example, \"directives\" are comprised of #SBATCH followed by Slurm options. Most commonly used options include: Full Option Abbreviated Description --job-name -J Custom job name --partition -p Partition to run on --nodes -N Total number of nodes --ntasks -n Number of \"tasks\". For use with distributed parallelism. See below. --cpus-per-task -c # of CPUs allocated to each task. For use with shared memory parallelism. --ntasks-per-node Number of \"tasks\" per node. For use with distributed parallelism. See below. --time -t Maximum walltime of the job in the format D-HH:MM:SS (e.g. --time=1- for one day or --time=4:00:00 for 4 hours) --constraint -C specific node architecture (if applicable) --mem-per-cpu Memory requested per CPU in MB --mem Memory requested per node in MB --mail-user Mail address (alternatively, put your email address in ~/.forward) --mail-type Control emails to user on job events. Use ALL to receive email notications at the beginning and end of the job. Additional options can be found on in the official Slurm documentation .","title":"Directives"},{"location":"clusters-at-yale/job-scheduling/#resource-limit-enforcement","text":"Slurm uses the linux cgroup feature to enforce limits on CPUs, GPUs, and memory. Jobs are only permitted to run on a node if they have a valid allocation, and only within the limits specified by that limitation. Thus, if you request a single core from slurm (the default) and start a job that runs 20 parallel threads, those threads will be packed into a single CPU, and run very slowly. Similarly, if you do not explicitly request memory, your job will be granted a fairly modest default per CPU, and if your job attempts to exceed that amount, it will be killed.","title":"Resource Limit Enforcement"},{"location":"clusters-at-yale/job-scheduling/#using-private-partitions","text":"If you have special permission to submit to a partition that belongs to another group, you may be asked to assign a special \"account\" to your jobs in that partition. You will be given the name of this account when you get access the partition and then simple add the -A <account> flag to your submission command or as an additional directive in your submission script.","title":"Using Private Partitions"},{"location":"clusters-at-yale/job-scheduling/dsq/","text":"Submit Arrays with dSQ Dead Simple Queue is a light-weight successor to SimpleQueue. It wraps around slurm's sbatch to help you submit independent jobs as job arrays. It has several advantages over SimpleQueue: Your job will adapt during the run to the available resources. As more cpus and/or memory are available, it will grow up (up to the per-user limit) Your job will only use the resources needed to complete remaining jobs. It will shrink as your jobs finish, giving you and your peers better access to compute resources. When run on the scavenge partition, only the subjobs are preempted, and the job as a whole will continue. You can then use dSQAutopsy to create a new job file that has only the jobs that didn't complete. All you need is Python 2.7 or higher (Python 3 works too!) dSQ is not recommended for situations where the initialiazation of the job takes most of its execution time and it is re-usable. These situations are much better handled by a worker-based job handler. Step 1: Job File First, you'll need to generate a job file. Each line of this job file needs to specify exactly what you want run for each job, including any modules that need to be loaded or modifications to your environment variables. Empty lines or lines that begin with # will be ignored when submitting your job array. Note: slurm jobs begin in the directory from which your job was submitted. For example, imagine that you have 1000 fastq files that correspond to individual samples you want to map to a genome with bowtie2 and convert to bam files with samtools . Given some initial testing, you think that each job needs 4 GB of RAM, and will run in less than 10 minutes. You'll create a file with the \"tasks\" you want to run, one per line. Each task corresponds to what you might otherwise have run as a single job. A task can be a simple command invocation, or a sequence of commands. You can call the task file anything, but for this example assume it's called \"joblist.txt\" and contains: module load bowtie2 samtools ; bowtie2 - p 8 -- local -- rg-id sample1 -- rg SM:sample1 -- rg LB:sci_seq -- rg PL:ILLUMINA - x my_genome - U sample1 . fastq - | samtools view - Shu - | samtools sort - sample1 module load bowtie2 samtools ; bowtie2 - p 8 -- local -- rg-id sample2 -- rg SM:sample2 -- rg LB:sci_seq -- rg PL:ILLUMINA - x my_genome - U sample2 . fastq - | samtools view - Shu - | samtools sort - sample2 ... module load bowtie2 samtools ; bowtie2 - p 8 -- local -- rg-id sample1000 -- rg SM:sample1000 -- rg LB:sci_seq -- rg PL:ILLUMINA - x my_genome - U sample1000 . fastq - | samtools view - Shu - | samtools sort - sample1000 Step 2: Create Batch Script Load Dead Simple Queue onto your path with: On Farnam, Milgram Ruddle: module load dSQ On Grace or Omega: module load Tools / dSQ dSQ.py takes a few arguments, then passes the rest directly to sbatch, either by writing a script to stdout that you should capture to a file. Unlike SimpleQueue, the resources you request will be given to each job in the array (each line in your job file) , e.g. requesting 2 GB of RAM with dSQ will run each individual job with a separate 2 GB of RAM available. Without specifying any additional sbatch arguments, some defaults will be set. run sbatch --help or see the official Slurm documentation for more info on sbatch options. dSQ . py -- jobfile jobfile [ dSQ args ] [ slurm args ] > run . sh Required dSQ arguments : -- jobfile JOBFILE Job file , one job per line Optional dSQ arguments : - h , -- help show this help message and exit -- version show program ' s version number and exit -- submit Submit the job array on the fly instead of printing to stdout . -- max - jobs MAX_JOBS Maximum number of simultaneously running jobs from the job array In the example above, we want walltime of 10 minutes and memory=4GB per task. Our invocation would be: dSQ --jobfile joblist.txt --mem-per-cpu=4g -t 10:00 > run.sh After creating the batch script, take a look at its contents. It should look quite familiar. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash #SBATCH -p general #SBATCH -t 10:00 #SBATCH --mem-per-cpu=4g #SBATCH --array=0-999 #SBATCH --cpus-per-task=1 #SBATCH --mail-type=ALL #SBATCH --mail-user=<youremail>@yale.edu #SBATCH --ntasks=1 /ysm-gpfs/apps/software/dSQ/0.92/dSQBatch.py joblist.txt</youremail> Step 3: Submit Batch Script sbatch run.sh Manage Your dSQ Job You can refer to any portion of your job with jobid_index syntax, or the entire array with its jobid. The index Dead Simple Queue uses starts at zero , so the 3rd line in your job file will have an index of 2. You can also specify ranges. #to cancel job 4 for array job 14567 scancel 14567_4 #to cancel jobs 3,5 and 10-20 for job 14567: scancel 14567_[3,5,10-20] dSQ Output You can monitor the status of your jobs in Slurm by using squeue -u <netid> . dSQ creates a file named job_<jobid>_status.tsv , which will report the success or failure of each job as it finishes. Note this file will not contain information for any jobs that were canceled (e.g. by the user with scancel) before they began. This file contains details about the completed jobs in the following tab-separated columns: Job_ID: the zero-based line number from your job file Exit_Code: exit code returned from your job (non-zero number generally indicates a failed job) Time_Started: time started, formatted as year-month-day hour:minute:second Time_Ended: time started, formatted as year-month-day hour:minute:second Time_Elapsed: in seconds Job: the line from your job file Additionally, Slurm will honor the -e,--error and -i,--input arguments you provide to capture stdout and stderr. By default both standard output and standard error are directed to a file of the name \"slurm-%j.out\", where the \"%j\" is replaced with the job allocation number and array index, which is conveniently also the 0-based line number from your job file. We recommend inspecting these outputs for troubleshooting individual failed jobss. dSQAutopsy Once the dSQ job is finished, you can use dSQAutopsy to create both a report of the run, as well as a new jobsfile that contains just the jobss that failed. $ dSQAutopsy --help usage: dSQAutopsy jobsfile status.tsv Dead Simple Queue Autopsy v0.9 https://github.com/ycrc/dSQ A helper script for analyzing the success state of your jobss after a dSQ run has completed. Specify the jobsfile and the status.tsv file generated by the dSQ job and dSQAutopsy will print the jobss that didn 't run or completed with non-zero exit codes. It will also report count of each to stderr. positional arguments: jobsfile jobs file, one jobs per line statusfile The status.tsv file generated from your dSQ run optional arguments: -h, --help show this help message and exit -v, --version show program' s version number and exit You can conveniently redirect the report and the failed jobss to separate files: dSQAutopsy jobsfile.txt job_2629186_status.tsv > failedjobs.txt 2> report.txt","title":"Submit Arrays with dSQ"},{"location":"clusters-at-yale/job-scheduling/dsq/#submit-arrays-with-dsq","text":"Dead Simple Queue is a light-weight successor to SimpleQueue. It wraps around slurm's sbatch to help you submit independent jobs as job arrays. It has several advantages over SimpleQueue: Your job will adapt during the run to the available resources. As more cpus and/or memory are available, it will grow up (up to the per-user limit) Your job will only use the resources needed to complete remaining jobs. It will shrink as your jobs finish, giving you and your peers better access to compute resources. When run on the scavenge partition, only the subjobs are preempted, and the job as a whole will continue. You can then use dSQAutopsy to create a new job file that has only the jobs that didn't complete. All you need is Python 2.7 or higher (Python 3 works too!) dSQ is not recommended for situations where the initialiazation of the job takes most of its execution time and it is re-usable. These situations are much better handled by a worker-based job handler.","title":"Submit Arrays with dSQ"},{"location":"clusters-at-yale/job-scheduling/dsq/#step-1-job-file","text":"First, you'll need to generate a job file. Each line of this job file needs to specify exactly what you want run for each job, including any modules that need to be loaded or modifications to your environment variables. Empty lines or lines that begin with # will be ignored when submitting your job array. Note: slurm jobs begin in the directory from which your job was submitted. For example, imagine that you have 1000 fastq files that correspond to individual samples you want to map to a genome with bowtie2 and convert to bam files with samtools . Given some initial testing, you think that each job needs 4 GB of RAM, and will run in less than 10 minutes. You'll create a file with the \"tasks\" you want to run, one per line. Each task corresponds to what you might otherwise have run as a single job. A task can be a simple command invocation, or a sequence of commands. You can call the task file anything, but for this example assume it's called \"joblist.txt\" and contains: module load bowtie2 samtools ; bowtie2 - p 8 -- local -- rg-id sample1 -- rg SM:sample1 -- rg LB:sci_seq -- rg PL:ILLUMINA - x my_genome - U sample1 . fastq - | samtools view - Shu - | samtools sort - sample1 module load bowtie2 samtools ; bowtie2 - p 8 -- local -- rg-id sample2 -- rg SM:sample2 -- rg LB:sci_seq -- rg PL:ILLUMINA - x my_genome - U sample2 . fastq - | samtools view - Shu - | samtools sort - sample2 ... module load bowtie2 samtools ; bowtie2 - p 8 -- local -- rg-id sample1000 -- rg SM:sample1000 -- rg LB:sci_seq -- rg PL:ILLUMINA - x my_genome - U sample1000 . fastq - | samtools view - Shu - | samtools sort - sample1000","title":"Step 1: Job File"},{"location":"clusters-at-yale/job-scheduling/dsq/#step-2-create-batch-script","text":"Load Dead Simple Queue onto your path with: On Farnam, Milgram Ruddle: module load dSQ On Grace or Omega: module load Tools / dSQ dSQ.py takes a few arguments, then passes the rest directly to sbatch, either by writing a script to stdout that you should capture to a file. Unlike SimpleQueue, the resources you request will be given to each job in the array (each line in your job file) , e.g. requesting 2 GB of RAM with dSQ will run each individual job with a separate 2 GB of RAM available. Without specifying any additional sbatch arguments, some defaults will be set. run sbatch --help or see the official Slurm documentation for more info on sbatch options. dSQ . py -- jobfile jobfile [ dSQ args ] [ slurm args ] > run . sh Required dSQ arguments : -- jobfile JOBFILE Job file , one job per line Optional dSQ arguments : - h , -- help show this help message and exit -- version show program ' s version number and exit -- submit Submit the job array on the fly instead of printing to stdout . -- max - jobs MAX_JOBS Maximum number of simultaneously running jobs from the job array In the example above, we want walltime of 10 minutes and memory=4GB per task. Our invocation would be: dSQ --jobfile joblist.txt --mem-per-cpu=4g -t 10:00 > run.sh After creating the batch script, take a look at its contents. It should look quite familiar. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash #SBATCH -p general #SBATCH -t 10:00 #SBATCH --mem-per-cpu=4g #SBATCH --array=0-999 #SBATCH --cpus-per-task=1 #SBATCH --mail-type=ALL #SBATCH --mail-user=<youremail>@yale.edu #SBATCH --ntasks=1 /ysm-gpfs/apps/software/dSQ/0.92/dSQBatch.py joblist.txt</youremail>","title":"Step 2: Create Batch Script"},{"location":"clusters-at-yale/job-scheduling/dsq/#step-3-submit-batch-script","text":"sbatch run.sh","title":"Step 3: Submit Batch Script"},{"location":"clusters-at-yale/job-scheduling/dsq/#manage-your-dsq-job","text":"You can refer to any portion of your job with jobid_index syntax, or the entire array with its jobid. The index Dead Simple Queue uses starts at zero , so the 3rd line in your job file will have an index of 2. You can also specify ranges. #to cancel job 4 for array job 14567 scancel 14567_4 #to cancel jobs 3,5 and 10-20 for job 14567: scancel 14567_[3,5,10-20]","title":"Manage Your dSQ Job"},{"location":"clusters-at-yale/job-scheduling/dsq/#dsq-output","text":"You can monitor the status of your jobs in Slurm by using squeue -u <netid> . dSQ creates a file named job_<jobid>_status.tsv , which will report the success or failure of each job as it finishes. Note this file will not contain information for any jobs that were canceled (e.g. by the user with scancel) before they began. This file contains details about the completed jobs in the following tab-separated columns: Job_ID: the zero-based line number from your job file Exit_Code: exit code returned from your job (non-zero number generally indicates a failed job) Time_Started: time started, formatted as year-month-day hour:minute:second Time_Ended: time started, formatted as year-month-day hour:minute:second Time_Elapsed: in seconds Job: the line from your job file Additionally, Slurm will honor the -e,--error and -i,--input arguments you provide to capture stdout and stderr. By default both standard output and standard error are directed to a file of the name \"slurm-%j.out\", where the \"%j\" is replaced with the job allocation number and array index, which is conveniently also the 0-based line number from your job file. We recommend inspecting these outputs for troubleshooting individual failed jobss.","title":"dSQ Output"},{"location":"clusters-at-yale/job-scheduling/dsq/#dsqautopsy","text":"Once the dSQ job is finished, you can use dSQAutopsy to create both a report of the run, as well as a new jobsfile that contains just the jobss that failed. $ dSQAutopsy --help usage: dSQAutopsy jobsfile status.tsv Dead Simple Queue Autopsy v0.9 https://github.com/ycrc/dSQ A helper script for analyzing the success state of your jobss after a dSQ run has completed. Specify the jobsfile and the status.tsv file generated by the dSQ job and dSQAutopsy will print the jobss that didn 't run or completed with non-zero exit codes. It will also report count of each to stderr. positional arguments: jobsfile jobs file, one jobs per line statusfile The status.tsv file generated from your dSQ run optional arguments: -h, --help show this help message and exit -v, --version show program' s version number and exit You can conveniently redirect the report and the failed jobss to separate files: dSQAutopsy jobsfile.txt job_2629186_status.tsv > failedjobs.txt 2> report.txt","title":"dSQAutopsy"},{"location":"clusters-at-yale/job-scheduling/fairshare/","text":"Factors Affecting Scheduling Job Priority Score Fairshare We have \"fairshare\" systems on our clusters, which means that jobs have a \"priority\" score that is affected by the amount of CPU hours used in the past few weeks. If a group has used a large amount of CPU hours, their jobs are given a lower priority score and therefore will take longer to start if the cluster is busy. Regardless of a job's prority, the scheduler still considers all jobs for backfill (see below). To see all pending jobs sorted by priority (jobs with higher priority at the top), use the following squeue command: squeue --sort=-p -t PD -p <partition_name> To monitor usage of members of your group, run the sshare command: sshare -a -A <group> Note: Resources used on private partitions do not count affect fairshare. Similarly, resources used in the scavenge partition cost 10% of comparable resources in the other partitions. Length of Time in Queue In addition to fairshare, any pending job will accrue priority over time, which can help overcome small fairshare penalties. To see the factors affecting your job's priority, run the following sprio command: sprio -j <job_id> Backfill In addition to the main scheduling cycle, where jobs are run in the order of priority and availability of resources, all jobs are also considered for \"backfill\". Backfill is a mechanism which will let jobs with lower priority score start before high priority jobs if they can fit in around them. For example, if a higher priority job needs 4 nodes with 20 cores on each node and it will have to wait 30 hours for those resources to be available, if a lower priority job only needs a couple cores for an hour, Slurm will run that job in the meantime. For this reason, it is important to request accurate walltime limits for your jobs. If your job only requires 2 hours to run, but you request 24 hours, the likelihood that your job will be backfilled is greatly lowered. Moreover, for performance reasons, the backfill scheduler on Grace only looks at the top 10 jobs by each user. Therefore, if you bundle similar jobs into job arrays (see dSQ ), the backfill cycle will consider more of your jobs since entire job arrays only count as one job for the limit accounting.","title":"Factors Affecting Scheduling"},{"location":"clusters-at-yale/job-scheduling/fairshare/#factors-affecting-scheduling","text":"","title":"Factors Affecting Scheduling"},{"location":"clusters-at-yale/job-scheduling/fairshare/#job-priority-score","text":"","title":"Job Priority Score"},{"location":"clusters-at-yale/job-scheduling/fairshare/#fairshare","text":"We have \"fairshare\" systems on our clusters, which means that jobs have a \"priority\" score that is affected by the amount of CPU hours used in the past few weeks. If a group has used a large amount of CPU hours, their jobs are given a lower priority score and therefore will take longer to start if the cluster is busy. Regardless of a job's prority, the scheduler still considers all jobs for backfill (see below). To see all pending jobs sorted by priority (jobs with higher priority at the top), use the following squeue command: squeue --sort=-p -t PD -p <partition_name> To monitor usage of members of your group, run the sshare command: sshare -a -A <group> Note: Resources used on private partitions do not count affect fairshare. Similarly, resources used in the scavenge partition cost 10% of comparable resources in the other partitions.","title":"Fairshare"},{"location":"clusters-at-yale/job-scheduling/fairshare/#length-of-time-in-queue","text":"In addition to fairshare, any pending job will accrue priority over time, which can help overcome small fairshare penalties. To see the factors affecting your job's priority, run the following sprio command: sprio -j <job_id>","title":"Length of Time in Queue"},{"location":"clusters-at-yale/job-scheduling/fairshare/#backfill","text":"In addition to the main scheduling cycle, where jobs are run in the order of priority and availability of resources, all jobs are also considered for \"backfill\". Backfill is a mechanism which will let jobs with lower priority score start before high priority jobs if they can fit in around them. For example, if a higher priority job needs 4 nodes with 20 cores on each node and it will have to wait 30 hours for those resources to be available, if a lower priority job only needs a couple cores for an hour, Slurm will run that job in the meantime. For this reason, it is important to request accurate walltime limits for your jobs. If your job only requires 2 hours to run, but you request 24 hours, the likelihood that your job will be backfilled is greatly lowered. Moreover, for performance reasons, the backfill scheduler on Grace only looks at the top 10 jobs by each user. Therefore, if you bundle similar jobs into job arrays (see dSQ ), the backfill cycle will consider more of your jobs since entire job arrays only count as one job for the limit accounting.","title":"Backfill"},{"location":"clusters-at-yale/job-scheduling/resource-requests/","text":"Request Compute Resources Request Cores and Nodes Slurm is very explicit in how one requests cores and nodes. While extremely powerful, the three flags, --nodes , --ntasks , and --cpus-per-task can be a bit confusing at first. We attempt to disambiguate them below. --ntasks vs --cpus-per-task The term \"task\" in this context can be thought of as a \"process\". Therefore, a multi-process program (e.g. MPI) is comprised of multiple tasks. And a multi-threaded program is comprised of a single task, which can in turn use multiple CPUs. In Slurm, tasks are requested with the --ntasks flag. CPUs, for the multithreaded programs, are requested with the --cpus-per-task flag. 1. Multi-threaded & multi-process programs To request CPUs for your multi-threaded program, use the --cpus-per-task flag. Individual tasks cannot be split across multiple compute nodes, so requesting a number of CPUs with --cpus-per-task flag will always result in all your CPUs allocated on the same compute node. 2. MPI programs In Slurm, the --ntasks flag specifies the number of MPI tasks created for your job. Note that, even within the same job, multiple tasks do not necessarily run on a single node. Therefore, requesting the same number of CPUs as above, but with the --ntasks flag, could result in those CPUs being allocated on several, distinct compute nodes. For many users, differentiating between --ntasks and --cpus-per-task is sufficient. However, for more control over how Slurm lays out your job, you can add the --nodes and --ntasks-per-node flags. --nodes specifies how many nodes to allocate to your job. Slurm will allocate your requested number of cores to a minimal number of nodes on the cluster, so it is extremely likely if you request a small number of tasks that they will all be allocated on the same node. However, to ensure they are on the same node, set --nodes=1 (obviously this is contingent on the number of CPUs on your cluster's nodes and requesting too many may result in a job that will never run). Conversely, if you would like to ensure a specific layout, such as one task per node for memory, I/O or other reasons, you can also set --ntasks-per-node=1 . Note that the following must be true: ntasks-per-node * nodes >= ntasks 3. Hybrid (MPI+OpenMP) programs For the most predictable performance for hybrid codes, you will need to use all three of the --ntasks , --cpus-per-task , and --nodes flags, where --ntasks equals the number of MPI tasks, --cpus-per-task equals the number of OMP_NUM_THREADS and --nodes is the number of nodes required to fit --ntasks * --cpus-per-task . Request GPUs Some of our clusters have nodes that contain GPU co-processors. Please refer to the cluster-specifc documentation regarding the node configurations that include gpus. In order for your job to be able to access gpus, you must request them as a slurm \"Generic Resource\" or gres. You specify the gres configuration per-node for a job with the --gres flag and a number of gpus. If you are agnostic about the kind of GPU your job gets, --gres=gpu:1 will allocate one of any kind of GPU per node. To specifically request, for example, a p100 for each node in your job you would use the flag --gres=gpu:p100:1 . Tip As with requesting multiple cores or multiple nodes, we strongly recommend that you test your jobs using the gpu_devel partition to make sure they can well utilize multiple GPUs before requesting them; allocating more GPUs does not magically speed up code that can only use one at a time. For more documentation on using GPUs on our clusters, please see Python Deep Learning with GPUs and GPUs and CUDA . Features and Constraints You may want to run programs that require more specific hardware than slurm may be willing to allocate to your job. To ensure your job runs on specific types of nodes, use the --constraint flag. You can use the processor type (e.g. E5-2660_v3 ) or processor codename (e.g. haswell ) to limit your job to specific node types. You can also specify an instruction set (e.g. avx ) to require that no matter what CPU your job runs on, it must understand at least these instructions. See the individual cluster pages for the exact tags for the different node types. # run on a node with a haswell codenamed CPU (e.g. a E5-2660 v3) sbatch --constraint=haswell submit.sh # only run on nodes with E5-2660 v4 CPUs sbatch --constraint=E5-2660_v4 submit.sh # run on any node that understands avx instructions # Your job could also run on an avx2 node sbatch --constraint=avx submit.sh Tip Use the command scontrol show node <hostname> , replacing <hostname> with the node's name you're interested in, to see more information about the node including its features.","title":"Request Compute Resources"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#request-compute-resources","text":"","title":"Request Compute Resources"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#request-cores-and-nodes","text":"Slurm is very explicit in how one requests cores and nodes. While extremely powerful, the three flags, --nodes , --ntasks , and --cpus-per-task can be a bit confusing at first. We attempt to disambiguate them below.","title":"Request Cores and Nodes"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#-ntasks-vs-cpus-per-task","text":"The term \"task\" in this context can be thought of as a \"process\". Therefore, a multi-process program (e.g. MPI) is comprised of multiple tasks. And a multi-threaded program is comprised of a single task, which can in turn use multiple CPUs. In Slurm, tasks are requested with the --ntasks flag. CPUs, for the multithreaded programs, are requested with the --cpus-per-task flag.","title":"--ntasks vs --cpus-per-task"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#146-multi-threaded-multi-process-programs","text":"To request CPUs for your multi-threaded program, use the --cpus-per-task flag. Individual tasks cannot be split across multiple compute nodes, so requesting a number of CPUs with --cpus-per-task flag will always result in all your CPUs allocated on the same compute node.","title":"1. Multi-threaded &amp; multi-process programs"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#246-mpi-programs","text":"In Slurm, the --ntasks flag specifies the number of MPI tasks created for your job. Note that, even within the same job, multiple tasks do not necessarily run on a single node. Therefore, requesting the same number of CPUs as above, but with the --ntasks flag, could result in those CPUs being allocated on several, distinct compute nodes. For many users, differentiating between --ntasks and --cpus-per-task is sufficient. However, for more control over how Slurm lays out your job, you can add the --nodes and --ntasks-per-node flags. --nodes specifies how many nodes to allocate to your job. Slurm will allocate your requested number of cores to a minimal number of nodes on the cluster, so it is extremely likely if you request a small number of tasks that they will all be allocated on the same node. However, to ensure they are on the same node, set --nodes=1 (obviously this is contingent on the number of CPUs on your cluster's nodes and requesting too many may result in a job that will never run). Conversely, if you would like to ensure a specific layout, such as one task per node for memory, I/O or other reasons, you can also set --ntasks-per-node=1 . Note that the following must be true: ntasks-per-node * nodes >= ntasks","title":"2. MPI programs"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#346-hybrid-mpiopenmp-programs","text":"For the most predictable performance for hybrid codes, you will need to use all three of the --ntasks , --cpus-per-task , and --nodes flags, where --ntasks equals the number of MPI tasks, --cpus-per-task equals the number of OMP_NUM_THREADS and --nodes is the number of nodes required to fit --ntasks * --cpus-per-task .","title":"3. Hybrid (MPI+OpenMP) programs"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#request-gpus","text":"Some of our clusters have nodes that contain GPU co-processors. Please refer to the cluster-specifc documentation regarding the node configurations that include gpus. In order for your job to be able to access gpus, you must request them as a slurm \"Generic Resource\" or gres. You specify the gres configuration per-node for a job with the --gres flag and a number of gpus. If you are agnostic about the kind of GPU your job gets, --gres=gpu:1 will allocate one of any kind of GPU per node. To specifically request, for example, a p100 for each node in your job you would use the flag --gres=gpu:p100:1 . Tip As with requesting multiple cores or multiple nodes, we strongly recommend that you test your jobs using the gpu_devel partition to make sure they can well utilize multiple GPUs before requesting them; allocating more GPUs does not magically speed up code that can only use one at a time. For more documentation on using GPUs on our clusters, please see Python Deep Learning with GPUs and GPUs and CUDA .","title":"Request GPUs"},{"location":"clusters-at-yale/job-scheduling/resource-requests/#features-and-constraints","text":"You may want to run programs that require more specific hardware than slurm may be willing to allocate to your job. To ensure your job runs on specific types of nodes, use the --constraint flag. You can use the processor type (e.g. E5-2660_v3 ) or processor codename (e.g. haswell ) to limit your job to specific node types. You can also specify an instruction set (e.g. avx ) to require that no matter what CPU your job runs on, it must understand at least these instructions. See the individual cluster pages for the exact tags for the different node types. # run on a node with a haswell codenamed CPU (e.g. a E5-2660 v3) sbatch --constraint=haswell submit.sh # only run on nodes with E5-2660 v4 CPUs sbatch --constraint=E5-2660_v4 submit.sh # run on any node that understands avx instructions # Your job could also run on an avx2 node sbatch --constraint=avx submit.sh Tip Use the command scontrol show node <hostname> , replacing <hostname> with the node's name you're interested in, to see more information about the node including its features.","title":"Features and Constraints"},{"location":"clusters-at-yale/job-scheduling/resource-usage/","text":"Monitor CPU and Memory General Note Making sure your jobs use the right amount of RAM and the right number of CPUs helps you and others using the clusters use these resources more effeciently, and in turn get work done more quickly. Below are some examples of how to measure your CPU and RAM (aka memory) usage so you can make this happen. Be sure to check the Slurm documentation and the clusters page (especially the partitions and hardware sections) to make sure you are submitting the right jobs to the right hardware. Future Jobs If you launch a program by putting /usr/bin/time in front of it, time will watch your program and provide statistics about the resources it used. For example: [be59@c01n01 ~]$ /usr/bin/time -v stress-ng --cpu 8 --timeout 10s stress-ng: info: [32574] dispatching hogs: 8 cpu stress-ng: info: [32574] successful run completed in 10.08s Command being timed: \"stress-ng --cpu 8 --timeout 10s\" User time (seconds): 80.22 System time (seconds): 0.04 Percent of CPU this job got: 795% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:10.09 Average shared text size (kbytes): 0 Average unshared data size (kbytes): 0 Average stack size (kbytes): 0 Average total size (kbytes): 0 Maximum resident set size (kbytes): 6328 Average resident set size (kbytes): 0 Major (requiring I/O) page faults: 0 Minor (reclaiming a frame) page faults: 30799 Voluntary context switches: 1380 Involuntary context switches: 68 Swaps: 0 File system inputs: 0 File system outputs: 0 Socket messages sent: 0 Socket messages received: 0 Signals delivered: 0 To know how much RAM your job used (and what jobs like it will need in the future), look at the \"Maximum resident set size\" Running Jobs If your job is already running, you can check on its usage, but will have to wait until it has finished to find the maximum memory and CPU used. The easiest way to check the instantaneous memory and CPU usage of a job is to ssh to a compute node your job is running on. To find the node you should ssh to, run: [be59@farnam1 ~]$ squeue -u$USER JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 21252409 general 12345 be59 R 32:17 17 c13n[02-04],c14n[05-10],c16n[03-10] Then use ssh to connect to a node your job is running on from the NODELIST column: [be59@farnam1 ~]$ ssh c13n03 [be59@c13n03 ~]$ Once you are on the compute node, run either ps or top . ps ps will give you instantaneous usage every time you run it. Here is some sample ps output: [be59@bigmem01 ~]$ ps -u$USER -o %cpu,rss,args %CPU RSS COMMAND 92.6 79446140 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask 94.5 80758040 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask 92.6 79676460 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask 92.5 81243364 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask 93.8 80799668 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask ps reports memory used in kilobytes, so each of the 5 matlab processes is using ~77GB of RAM. They are also using most of 5 cores, so future jobs like this should request 5 CPUs. top top runs interactively and shows you live usage statistics. You can press u , enter your netid, then enter to filter just your processes. For Memory usage, the number you are interested in is RES. In the case below, the YEPNEE.exe programs are each consuming ~600MB of memory and each fully utilizing one CPU. You can press ? for help and q to quit. Completed Jobs Slurm records statistics for every job, including how much memory and CPU was used. seff After the job completes, you can run seff <jobid> to get some useful information about your job, including the memory used and what percent of your allocated memory that amounts to. [rdb9@farnam1 ~]$ seff 21294645 Job ID: 21294645 Cluster: farnam User/Group: rdb9/lsprog State: COMPLETED (exit code 0) Cores: 1 CPU Utilized: 00:15:55 CPU Efficiency: 17.04% of 01:33:23 core-walltime Job Wall-clock time: 01:33:23 Memory Utilized: 446.20 MB Memory Efficiency: 8.71% of 5.00 GB sacct You can also use the more flexible sacct to get that info, along with other more advanced job queries. Unfortunately, the default output from sacct is not as useful. We recommend setting an environment variable to customize the output. [rdb9@farnam1 ~]$ export SACCT_FORMAT=\"JobID%20,JobName,User,Partition,NodeList,Elapsed,State,ExitCode,MaxRSS,AllocTRES%32\" [rdb9@farnam1 ~]$ sacct -j 21294645 JobID JobName User Partition NodeList Elapsed State ExitCode MaxRSS AllocTRES -------------------- ---------- --------- ---------- --------------- ---------- ---------- -------- ---------- -------------------------------- 21294645 bash rdb9 interacti+ c06n09 01:33:23 COMPLETED 0:0 cpu=1,mem=5G,node=1,billing=1 21294645.extern extern c06n09 01:33:23 COMPLETED 0:0 716K cpu=1,mem=5G,node=1,billing=1 21294645.0 bash c06n09 01:33:23 COMPLETED 0:0 456908K cpu=1,mem=5G,node=1 You should look at the MaxRSS value to see your memory usage.","title":"Monitor CPU and Memory"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#monitor-cpu-and-memory","text":"","title":"Monitor CPU and Memory"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#general-note","text":"Making sure your jobs use the right amount of RAM and the right number of CPUs helps you and others using the clusters use these resources more effeciently, and in turn get work done more quickly. Below are some examples of how to measure your CPU and RAM (aka memory) usage so you can make this happen. Be sure to check the Slurm documentation and the clusters page (especially the partitions and hardware sections) to make sure you are submitting the right jobs to the right hardware.","title":"General Note"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#future-jobs","text":"If you launch a program by putting /usr/bin/time in front of it, time will watch your program and provide statistics about the resources it used. For example: [be59@c01n01 ~]$ /usr/bin/time -v stress-ng --cpu 8 --timeout 10s stress-ng: info: [32574] dispatching hogs: 8 cpu stress-ng: info: [32574] successful run completed in 10.08s Command being timed: \"stress-ng --cpu 8 --timeout 10s\" User time (seconds): 80.22 System time (seconds): 0.04 Percent of CPU this job got: 795% Elapsed (wall clock) time (h:mm:ss or m:ss): 0:10.09 Average shared text size (kbytes): 0 Average unshared data size (kbytes): 0 Average stack size (kbytes): 0 Average total size (kbytes): 0 Maximum resident set size (kbytes): 6328 Average resident set size (kbytes): 0 Major (requiring I/O) page faults: 0 Minor (reclaiming a frame) page faults: 30799 Voluntary context switches: 1380 Involuntary context switches: 68 Swaps: 0 File system inputs: 0 File system outputs: 0 Socket messages sent: 0 Socket messages received: 0 Signals delivered: 0 To know how much RAM your job used (and what jobs like it will need in the future), look at the \"Maximum resident set size\"","title":"Future Jobs"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#running-jobs","text":"If your job is already running, you can check on its usage, but will have to wait until it has finished to find the maximum memory and CPU used. The easiest way to check the instantaneous memory and CPU usage of a job is to ssh to a compute node your job is running on. To find the node you should ssh to, run: [be59@farnam1 ~]$ squeue -u$USER JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 21252409 general 12345 be59 R 32:17 17 c13n[02-04],c14n[05-10],c16n[03-10] Then use ssh to connect to a node your job is running on from the NODELIST column: [be59@farnam1 ~]$ ssh c13n03 [be59@c13n03 ~]$ Once you are on the compute node, run either ps or top .","title":"Running Jobs"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#ps","text":"ps will give you instantaneous usage every time you run it. Here is some sample ps output: [be59@bigmem01 ~]$ ps -u$USER -o %cpu,rss,args %CPU RSS COMMAND 92.6 79446140 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask 94.5 80758040 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask 92.6 79676460 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask 92.5 81243364 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask 93.8 80799668 /gpfs/ysm/apps/hpc/Apps/Matlab/R2016b/bin/glnxa64/MATLAB -dmlworker -nodisplay -r distcomp_evaluate_filetask ps reports memory used in kilobytes, so each of the 5 matlab processes is using ~77GB of RAM. They are also using most of 5 cores, so future jobs like this should request 5 CPUs.","title":"ps"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#top","text":"top runs interactively and shows you live usage statistics. You can press u , enter your netid, then enter to filter just your processes. For Memory usage, the number you are interested in is RES. In the case below, the YEPNEE.exe programs are each consuming ~600MB of memory and each fully utilizing one CPU. You can press ? for help and q to quit.","title":"top"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#completed-jobs","text":"Slurm records statistics for every job, including how much memory and CPU was used.","title":"Completed Jobs"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#seff","text":"After the job completes, you can run seff <jobid> to get some useful information about your job, including the memory used and what percent of your allocated memory that amounts to. [rdb9@farnam1 ~]$ seff 21294645 Job ID: 21294645 Cluster: farnam User/Group: rdb9/lsprog State: COMPLETED (exit code 0) Cores: 1 CPU Utilized: 00:15:55 CPU Efficiency: 17.04% of 01:33:23 core-walltime Job Wall-clock time: 01:33:23 Memory Utilized: 446.20 MB Memory Efficiency: 8.71% of 5.00 GB","title":"seff"},{"location":"clusters-at-yale/job-scheduling/resource-usage/#sacct","text":"You can also use the more flexible sacct to get that info, along with other more advanced job queries. Unfortunately, the default output from sacct is not as useful. We recommend setting an environment variable to customize the output. [rdb9@farnam1 ~]$ export SACCT_FORMAT=\"JobID%20,JobName,User,Partition,NodeList,Elapsed,State,ExitCode,MaxRSS,AllocTRES%32\" [rdb9@farnam1 ~]$ sacct -j 21294645 JobID JobName User Partition NodeList Elapsed State ExitCode MaxRSS AllocTRES -------------------- ---------- --------- ---------- --------------- ---------- ---------- -------- ---------- -------------------------------- 21294645 bash rdb9 interacti+ c06n09 01:33:23 COMPLETED 0:0 cpu=1,mem=5G,node=1,billing=1 21294645.extern extern c06n09 01:33:23 COMPLETED 0:0 716K cpu=1,mem=5G,node=1,billing=1 21294645.0 bash c06n09 01:33:23 COMPLETED 0:0 456908K cpu=1,mem=5G,node=1 You should look at the MaxRSS value to see your memory usage.","title":"sacct"},{"location":"clusters-at-yale/job-scheduling/scavenge/","text":"Scavenge Partition A scavenge partition is available on all of our clusters. It allows you to (a) run jobs outside of your normal limits (e.g. QOSMaxCpuPerUserLimit ) and (b) use unutilized cores, if available, in any private partition on the cluster. You can also use the scavenge partition to get access to unused cores in special purpose partitions, such as the \"gpu\" or \"mpi\" partitions, and unused GPUs in private partitions. However, any job running in the scavenge partition is subject to preemption if any node in use by the job is required for a job in the node's normal partition. This means that your job may be killed without advance notice, so you should only run jobs in the scavenge partition that either have checkpoint capabilities or that can otherwise be restarted with minimal loss of progress. Warning Not all jobs are a good fit for the scavenge partition, such as jobs with long startup times or jobs that run a long time between checkpoint operations. Automatically Requeue Preempted Jobs If you would like your job to be automatically added back to the queue if preempted, you can add the --requeue flag to your submission script. #SBATCH --requeue Be aware that your job, when started from a requeue, will still re-run the entire original submission script. It will only resume progress if your program has the its own ability to checkpoint and restart from previous progress. Track History of a Requeued Job When a scavenge job is requeued after preemption, it retains the same job id. However, this can make it difficult to track the history of the job (how many times it was requeued, how long it ran for each time). To view the full history of your job use the --duplicates flag for the sacct command. sacct -j <jobid> --duplicates Researching available nodes If you are interested in specific hardware and its availability, you can use the sinfo command to query how many of each type of node is available and what features it lists. For example: sinfo -e -o \"%.6D|%c|%G|%b\" | column -ts \"|\" will show you the kinds of nodes available, and sinfo -e -o \"%.6D|%T|%c|%G|%b\" | column -ts \"|\" will break out how many nodes in each state (e.g. allocated, mixed, idle) there are. For more options see the official sinfo documentation .","title":"Scavenge Partition"},{"location":"clusters-at-yale/job-scheduling/scavenge/#scavenge-partition","text":"A scavenge partition is available on all of our clusters. It allows you to (a) run jobs outside of your normal limits (e.g. QOSMaxCpuPerUserLimit ) and (b) use unutilized cores, if available, in any private partition on the cluster. You can also use the scavenge partition to get access to unused cores in special purpose partitions, such as the \"gpu\" or \"mpi\" partitions, and unused GPUs in private partitions. However, any job running in the scavenge partition is subject to preemption if any node in use by the job is required for a job in the node's normal partition. This means that your job may be killed without advance notice, so you should only run jobs in the scavenge partition that either have checkpoint capabilities or that can otherwise be restarted with minimal loss of progress. Warning Not all jobs are a good fit for the scavenge partition, such as jobs with long startup times or jobs that run a long time between checkpoint operations.","title":"Scavenge Partition"},{"location":"clusters-at-yale/job-scheduling/scavenge/#automatically-requeue-preempted-jobs","text":"If you would like your job to be automatically added back to the queue if preempted, you can add the --requeue flag to your submission script. #SBATCH --requeue Be aware that your job, when started from a requeue, will still re-run the entire original submission script. It will only resume progress if your program has the its own ability to checkpoint and restart from previous progress.","title":"Automatically Requeue Preempted Jobs"},{"location":"clusters-at-yale/job-scheduling/scavenge/#track-history-of-a-requeued-job","text":"When a scavenge job is requeued after preemption, it retains the same job id. However, this can make it difficult to track the history of the job (how many times it was requeued, how long it ran for each time). To view the full history of your job use the --duplicates flag for the sacct command. sacct -j <jobid> --duplicates","title":"Track History of a Requeued Job"},{"location":"clusters-at-yale/job-scheduling/scavenge/#researching-available-nodes","text":"If you are interested in specific hardware and its availability, you can use the sinfo command to query how many of each type of node is available and what features it lists. For example: sinfo -e -o \"%.6D|%c|%G|%b\" | column -ts \"|\" will show you the kinds of nodes available, and sinfo -e -o \"%.6D|%T|%c|%G|%b\" | column -ts \"|\" will break out how many nodes in each state (e.g. allocated, mixed, idle) there are. For more options see the official sinfo documentation .","title":"Researching available nodes"},{"location":"clusters-at-yale/job-scheduling/simplequeue/","text":"SimpleQueue SimpleQueue is a tool written here to streamline submission of a large number of jobs using a task file. It has a number of advantages: You can run more of your sequential jobs concurrently, since there is a limit on the number of individual qsubs you can run simultaneously. You only have one job to keep track of. If you need to shut everything down, you only need to kill one job. SimpleQueue keeps track of the status of individual jobs. Note that version 3.0+ of SimpleQueue differs from earlier versions in important ways, in particular the meaning of -n. If you have been using an earlier version, please read the following carefully! SimpleQueue is available as a module on our clusters. Run: module avail simplequeue to locate the simplequeue module on your cluster of choice. Example SimpleQueue Job For example, imagine that you have 1000 fastq files that correspond to individual samples you want to map to a genome with bowtie2 and convert to bam files with samtools . Given some initial testing, you think that 80 cpus working together will be enough to finish the job in a reasonable time. Step 1: Create Task List The first step is to create a file with a list of the \"tasks\" you want to run. Each task corresponds to what you might otherwise have run as a single job. A task can be a simple command invocation, or a sequence of commands. You can call the task file anything, but for this example assume it's called \"tasklist.txt\" and contains: module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample1 --rg SM:sample1 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1.fastq - | samtools view -Shu - | samtools sort - sample1 module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample2 --rg SM:sample2 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample2.fastq - | samtools view -Shu - | samtools sort - sample2 ... module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample1000 --rg SM:sample1000 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1000.fastq - | samtools view -Shu - | samtools sort - sample1000 For simplicity, we'll assume that tasklist, input fastq files, and indexed genome are in a directory called ~/genome_proj/mapping . Step 2: Create Submission Script Load the SimpleQueue module, then create the launch script using: sqCreateScript -q general -N genome_map -n 80 tasklist.txt > run.sh These parameters specify that the job, named genome_map, will be submitted to the general queue/partition. This job will find 80 free cores, start 80 workers on them, and begin processing tasks from the taskfile tasklist.txt . sqCreateScript takes a number of options. They differ somewhat from cluster to cluster, particularly the default values for queue, walltime, and memory. You can run sqCreateScript without any arguments to see the exact options on your cluster. Usage: -h, --help show this help message and exit -n WORKERS, --workers=WORKERS Number of workers to use. Not required. Defaults to 1. -c CORES, --cores=CORES Number of cores to request per worker. Defaults to 1. -m MEM, --mem=MEM Memory per worker. Not required. Defaults to 1G -w WALLTIME, --walltime=WALLTIME Walltime to request for the Slurm Job in form [[D-]HH:]MM:SS. Not required. Defaults to 1:00:00. -q QUEUE, --queue=QUEUE Name of queue to use. Not required. Defaults to general -N NAME, --name=NAME Base job name to use. Not required. Defaults to SimpleQueue. --logdir=LOGDIR Name of logging directory. Defaults to SQ_Files_ ${ SLURM_JOB_ID } . Step 3: Submit Your Job Now you can simply submit run.sh to the scheduler. All of the important scheduler options (queue, number of tasks, number of cpus per task) will have been set in the script so you needn't worry about them. Shortly after run.sh begins running, you should see a directory appear called SQ_Files_jobid where jobid is the jobid the scheduler assigned your job. This directory contains logs from all the tasks that are run during your job. In addition, there are a few other files that record information about the job as a whole. Of these, the most important one is SQ.log . It should be reviewed if you encounter a problem with a run. Assuming that all goes well, tasks from the tasklist file will be scheduled automatically onto the cpus you acquired until all the tasks have completed. At that time, the job will terminate, and you'll see several summary files: scheduler_jobid_out.txt : this is the stdout from simple queue proper (it is generally empty). scheduler_jobid_err.txt : this is the stderr from simple queue proper (it is generally a copy of SQ.log ). tasklist.txt.STATUS : this contains a list of all the tasks that were run, including exit status, start time, end time, pid, node run on, and the command run. tasklist.txt.REMAINING : Failed or uncompleted tasks will be listed in this file in the same format as tasklist, so that those tasks can be easily rerun. You should review the status files related to these tasks to understand why they did not complete. This list is provided for convenience. It is always a good idea to scan tasklist.STATUS to double check which tasks did in fact complete with a normal exit status. tasklist.txt.ROGUES : The simple queue system attempts to ensure that all tasks launched eventually exit (normally or abnormally). If it fails to get confirmation that a task has exited, information about the command will be written to this file. This information can be used to hunt down and kill run away processes. Other Important Options If your individual tasks need more than the default memory allocated on your cluster, you can specify a different value using -m. For example: sqCreateScript -m 10g -n 4 ... tasklist > run.sh would request 10GB of RAM for each of your workers. If your jobs are themselves multithreaded, you can request that your workers have multiple cores using the -c option: sqCreateScript -c 20 -n 4 ... tasklist > run.sh This would create 4 workers, each having access to 20 cores.","title":"SimpleQueue"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#simplequeue","text":"SimpleQueue is a tool written here to streamline submission of a large number of jobs using a task file. It has a number of advantages: You can run more of your sequential jobs concurrently, since there is a limit on the number of individual qsubs you can run simultaneously. You only have one job to keep track of. If you need to shut everything down, you only need to kill one job. SimpleQueue keeps track of the status of individual jobs. Note that version 3.0+ of SimpleQueue differs from earlier versions in important ways, in particular the meaning of -n. If you have been using an earlier version, please read the following carefully! SimpleQueue is available as a module on our clusters. Run: module avail simplequeue to locate the simplequeue module on your cluster of choice.","title":"SimpleQueue"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#example-simplequeue-job","text":"For example, imagine that you have 1000 fastq files that correspond to individual samples you want to map to a genome with bowtie2 and convert to bam files with samtools . Given some initial testing, you think that 80 cpus working together will be enough to finish the job in a reasonable time.","title":"Example SimpleQueue Job"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#step-1-create-task-list","text":"The first step is to create a file with a list of the \"tasks\" you want to run. Each task corresponds to what you might otherwise have run as a single job. A task can be a simple command invocation, or a sequence of commands. You can call the task file anything, but for this example assume it's called \"tasklist.txt\" and contains: module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample1 --rg SM:sample1 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1.fastq - | samtools view -Shu - | samtools sort - sample1 module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample2 --rg SM:sample2 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample2.fastq - | samtools view -Shu - | samtools sort - sample2 ... module load bowtie2 samtools ; bowtie2 -p 8 --local --rg-id sample1000 --rg SM:sample1000 --rg LB:sci_seq --rg PL:ILLUMINA -x my_genome -U sample1000.fastq - | samtools view -Shu - | samtools sort - sample1000 For simplicity, we'll assume that tasklist, input fastq files, and indexed genome are in a directory called ~/genome_proj/mapping .","title":"Step 1: Create Task List"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#step-2-create-submission-script","text":"Load the SimpleQueue module, then create the launch script using: sqCreateScript -q general -N genome_map -n 80 tasklist.txt > run.sh These parameters specify that the job, named genome_map, will be submitted to the general queue/partition. This job will find 80 free cores, start 80 workers on them, and begin processing tasks from the taskfile tasklist.txt . sqCreateScript takes a number of options. They differ somewhat from cluster to cluster, particularly the default values for queue, walltime, and memory. You can run sqCreateScript without any arguments to see the exact options on your cluster. Usage: -h, --help show this help message and exit -n WORKERS, --workers=WORKERS Number of workers to use. Not required. Defaults to 1. -c CORES, --cores=CORES Number of cores to request per worker. Defaults to 1. -m MEM, --mem=MEM Memory per worker. Not required. Defaults to 1G -w WALLTIME, --walltime=WALLTIME Walltime to request for the Slurm Job in form [[D-]HH:]MM:SS. Not required. Defaults to 1:00:00. -q QUEUE, --queue=QUEUE Name of queue to use. Not required. Defaults to general -N NAME, --name=NAME Base job name to use. Not required. Defaults to SimpleQueue. --logdir=LOGDIR Name of logging directory. Defaults to SQ_Files_ ${ SLURM_JOB_ID } .","title":"Step 2: Create Submission Script"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#step-3-submit-your-job","text":"Now you can simply submit run.sh to the scheduler. All of the important scheduler options (queue, number of tasks, number of cpus per task) will have been set in the script so you needn't worry about them. Shortly after run.sh begins running, you should see a directory appear called SQ_Files_jobid where jobid is the jobid the scheduler assigned your job. This directory contains logs from all the tasks that are run during your job. In addition, there are a few other files that record information about the job as a whole. Of these, the most important one is SQ.log . It should be reviewed if you encounter a problem with a run. Assuming that all goes well, tasks from the tasklist file will be scheduled automatically onto the cpus you acquired until all the tasks have completed. At that time, the job will terminate, and you'll see several summary files: scheduler_jobid_out.txt : this is the stdout from simple queue proper (it is generally empty). scheduler_jobid_err.txt : this is the stderr from simple queue proper (it is generally a copy of SQ.log ). tasklist.txt.STATUS : this contains a list of all the tasks that were run, including exit status, start time, end time, pid, node run on, and the command run. tasklist.txt.REMAINING : Failed or uncompleted tasks will be listed in this file in the same format as tasklist, so that those tasks can be easily rerun. You should review the status files related to these tasks to understand why they did not complete. This list is provided for convenience. It is always a good idea to scan tasklist.STATUS to double check which tasks did in fact complete with a normal exit status. tasklist.txt.ROGUES : The simple queue system attempts to ensure that all tasks launched eventually exit (normally or abnormally). If it fails to get confirmation that a task has exited, information about the command will be written to this file. This information can be used to hunt down and kill run away processes.","title":"Step 3: Submit Your Job"},{"location":"clusters-at-yale/job-scheduling/simplequeue/#other-important-options","text":"If your individual tasks need more than the default memory allocated on your cluster, you can specify a different value using -m. For example: sqCreateScript -m 10g -n 4 ... tasklist > run.sh would request 10GB of RAM for each of your workers. If your jobs are themselves multithreaded, you can request that your workers have multiple cores using the -c option: sqCreateScript -c 20 -n 4 ... tasklist > run.sh This would create 4 workers, each having access to 20 cores.","title":"Other Important Options"},{"location":"clusters-at-yale/job-scheduling/slurm-account/","text":"Coordinating a Slurm Account On the clusters the YCRC maintains, we map your linux user and group to your slurm user and account, which is what actually gives you permission to submit to the various partitions available on the clusters. By changing the slurm accounts associated with your user, you can modify access to partitions. As a coordinator of an account, you have permission to modify users' association with that account and modify jobs running that are associated with that account. Below are some useful example commands where we use an example user with the name \"be59\" where you are the coordinator of the slurm account \"cryoem\". Add/Remove Users From an Account sacctmgr add user be59 account = cryoem # add user sacctmgr remove user where user = be59 and account = cryoem # remove user Show Account Info sacctmgr show assoc user = be59 # show user associations sacctmgr show assoc account = cryoem # show assocations for account Submit Jobs srun -A cryoem ... sbatch -A cryoem my_script.sh List Jobs squeue -A cryoem # by account squeue -u be59 # by user Cancel Jobs scancel 1234 # by job ID scancel -u be59 # kill all jobs by user scancel -u be59 --state = running # kill running jobs by user scancel -u be59 --state = pending # kill pending jobs by user scancel -A cryoem # kill all jobs in the account","title":"Coordinating a Slurm Account"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#coordinating-a-slurm-account","text":"On the clusters the YCRC maintains, we map your linux user and group to your slurm user and account, which is what actually gives you permission to submit to the various partitions available on the clusters. By changing the slurm accounts associated with your user, you can modify access to partitions. As a coordinator of an account, you have permission to modify users' association with that account and modify jobs running that are associated with that account. Below are some useful example commands where we use an example user with the name \"be59\" where you are the coordinator of the slurm account \"cryoem\".","title":"Coordinating a Slurm Account"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#addremove-users-from-an-account","text":"sacctmgr add user be59 account = cryoem # add user sacctmgr remove user where user = be59 and account = cryoem # remove user","title":"Add/Remove Users From an Account"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#show-account-info","text":"sacctmgr show assoc user = be59 # show user associations sacctmgr show assoc account = cryoem # show assocations for account","title":"Show Account Info"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#submit-jobs","text":"srun -A cryoem ... sbatch -A cryoem my_script.sh","title":"Submit Jobs"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#list-jobs","text":"squeue -A cryoem # by account squeue -u be59 # by user","title":"List Jobs"},{"location":"clusters-at-yale/job-scheduling/slurm-account/#cancel-jobs","text":"scancel 1234 # by job ID scancel -u be59 # kill all jobs by user scancel -u be59 --state = running # kill running jobs by user scancel -u be59 --state = pending # kill pending jobs by user scancel -A cryoem # kill all jobs in the account","title":"Cancel Jobs"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/","text":"Submission Script Examples In addition to those below, we have additional example submission scripts for Parallel R, Matlab and Python . Single threaded programs (basic) 1 2 3 4 5 6 #!/bin/bash #SBATCH --job-name=my_job #SBATCH --time=10:00 ./hello.omp Multi-threaded programs 1 2 3 4 5 6 7 8 9 10 #!/bin/bash #SBATCH --job-name=omp_job #SBATCH --output=omp_job.txt #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --time=10:00 export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK ./hello.omp Multi-process programs 1 2 3 4 5 6 7 8 #!/bin/bash #SBATCH --job-name=mpi #SBATCH --output=mpi_job.txt #SBATCH --ntasks=4 #SBATCH --time=10:00 mpirun hello.mpi Tip On Omega, try to make ntasks equal to a multiple of 8. Hybrid (MPI+OpenMP) programs 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash #SBATCH --job-name=hybrid #SBATCH --output=hydrid_job.txt #SBATCH --ntasks=8 #SBATCH --cpus-per-task=5 #SBATCH --nodes=2 #SBATCH --time=10:00 export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK mpirun hello_hybrid.mpi GPU job 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash #SBATCH --job-name=deep_learn #SBATCH --output=gpu_job.txt #SBATCH --ntasks=1 #SBATCH --cpus-per-task=2 #SBATCH --gres=gpu:k80:2 #SBATCH --partition=gpu #SBATCH --gres-flags=enforce-binding #SBATCH --time=10:00 module load CUDA module load cuDNN # using your anaconda environment source activate deep-learn python my_tensorflow.py","title":"Submission Script Examples"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#submission-script-examples","text":"In addition to those below, we have additional example submission scripts for Parallel R, Matlab and Python .","title":"Submission Script Examples"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#single-threaded-programs-basic","text":"1 2 3 4 5 6 #!/bin/bash #SBATCH --job-name=my_job #SBATCH --time=10:00 ./hello.omp","title":"Single threaded programs (basic)"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#multi-threaded-programs","text":"1 2 3 4 5 6 7 8 9 10 #!/bin/bash #SBATCH --job-name=omp_job #SBATCH --output=omp_job.txt #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --time=10:00 export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK ./hello.omp","title":"Multi-threaded programs"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#multi-process-programs","text":"1 2 3 4 5 6 7 8 #!/bin/bash #SBATCH --job-name=mpi #SBATCH --output=mpi_job.txt #SBATCH --ntasks=4 #SBATCH --time=10:00 mpirun hello.mpi Tip On Omega, try to make ntasks equal to a multiple of 8.","title":"Multi-process programs"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#hybrid-mpiopenmp-programs","text":"1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/bash #SBATCH --job-name=hybrid #SBATCH --output=hydrid_job.txt #SBATCH --ntasks=8 #SBATCH --cpus-per-task=5 #SBATCH --nodes=2 #SBATCH --time=10:00 export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK mpirun hello_hybrid.mpi","title":"Hybrid (MPI+OpenMP) programs"},{"location":"clusters-at-yale/job-scheduling/slurm-examples/#gpu-job","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash #SBATCH --job-name=deep_learn #SBATCH --output=gpu_job.txt #SBATCH --ntasks=1 #SBATCH --cpus-per-task=2 #SBATCH --gres=gpu:k80:2 #SBATCH --partition=gpu #SBATCH --gres-flags=enforce-binding #SBATCH --time=10:00 module load CUDA module load cuDNN # using your anaconda environment source activate deep-learn python my_tensorflow.py","title":"GPU job"},{"location":"data/","text":"Data Storage Options Google Drive via Eliapps Capacity: Unlimited. Cost: Free - 5TB max file size No sensitive data (e.g. ePHI, HIPAA) Can be mounted on your local machine and transferred to via Globus Google Drive Connector Google Drive is a cloud service for file storage, document editing and sharing. All members of the Yale community with an Eliapps (Google Apps for Education) account have unlimited (yes, unlimited) storage at no cost in the associated Google Drive account. Moreover, Eliapps users can request Team Drives, which are Google Drive shared spaces where all files in a Team Drive belong to the team instead to any individual. For more information on Google Drive through Eliapps, see our Google Drive documentation for more information. Storage @ Yale Capacity: As requested. Cost: See below No sensitive data (e.g. ePHI, HIPAA) for cluster mounts Can be mounted on the cluster or computers on campus Storage @ Yale (S@Y) is a central storage service provided by ITS. S@Y shares can either be accessible on campus computers or the clusters, but not both. All prices are charged monthly for storage used at that time. Standard (daily use): $10.78/TB/month Archive (long term storage): $3.60/TB/month Enhanced (higher performance): $33.87/TB/month For most up to date pricing information, see the ITS Data Rates . To request a share, press the \u201cRequest this Service\u201d button in the right sidebar on the Storage@Yale website . If you would like to request a share that is mounted on the HPC clusters, fill out the Storage@Yale Request Form on the YCRC website. If you elect to use archive tier storage, be cognizant of its performance characteristics . Box at Yale Capacity: 50GB per user. Cost: Free. 15 GB max file size. Sensitive data (e.g. ePHI, HIPAA) only in Secure Box Can be mounted on your local machine and transferred with rclone All members of the Yale community have access to a share at Box at Yale. Box is another cloud-based file sharing and storage service. You can upload and access your data using the web portal and sync data with your local machines via Box Sync. To access, navigate to yale.box.com and login with your yale.edu account. For sync with your local machine, install Box Sync and authenticate with your yale.edu account. For more information about Box at Yale, see the ITS website.","title":"Data Storage Options"},{"location":"data/#data-storage-options","text":"","title":"Data Storage Options"},{"location":"data/#google-drive-via-eliapps","text":"Capacity: Unlimited. Cost: Free - 5TB max file size No sensitive data (e.g. ePHI, HIPAA) Can be mounted on your local machine and transferred to via Globus Google Drive Connector Google Drive is a cloud service for file storage, document editing and sharing. All members of the Yale community with an Eliapps (Google Apps for Education) account have unlimited (yes, unlimited) storage at no cost in the associated Google Drive account. Moreover, Eliapps users can request Team Drives, which are Google Drive shared spaces where all files in a Team Drive belong to the team instead to any individual. For more information on Google Drive through Eliapps, see our Google Drive documentation for more information.","title":"Google Drive via Eliapps"},{"location":"data/#storage-yale","text":"Capacity: As requested. Cost: See below No sensitive data (e.g. ePHI, HIPAA) for cluster mounts Can be mounted on the cluster or computers on campus Storage @ Yale (S@Y) is a central storage service provided by ITS. S@Y shares can either be accessible on campus computers or the clusters, but not both. All prices are charged monthly for storage used at that time. Standard (daily use): $10.78/TB/month Archive (long term storage): $3.60/TB/month Enhanced (higher performance): $33.87/TB/month For most up to date pricing information, see the ITS Data Rates . To request a share, press the \u201cRequest this Service\u201d button in the right sidebar on the Storage@Yale website . If you would like to request a share that is mounted on the HPC clusters, fill out the Storage@Yale Request Form on the YCRC website. If you elect to use archive tier storage, be cognizant of its performance characteristics .","title":"Storage @ Yale"},{"location":"data/#box-at-yale","text":"Capacity: 50GB per user. Cost: Free. 15 GB max file size. Sensitive data (e.g. ePHI, HIPAA) only in Secure Box Can be mounted on your local machine and transferred with rclone All members of the Yale community have access to a share at Box at Yale. Box is another cloud-based file sharing and storage service. You can upload and access your data using the web portal and sync data with your local machines via Box Sync. To access, navigate to yale.box.com and login with your yale.edu account. For sync with your local machine, install Box Sync and authenticate with your yale.edu account. For more information about Box at Yale, see the ITS website.","title":"Box at Yale"},{"location":"data/archive/","text":"Archive Your Data Clean Out Unnecessary Files Not every file created during a project needs to be archived. If you proactively reduce the number of extraneous files in your archive, you will both reduce storage costs and increase the usefulness of that data upon retrieval. Common files that can be deleted when archiving data include: Compiled codes, such as .o or .pyc files. These files will likely not even work on the next system you may restore these data to and they can contribute significantly to your file count limit. Just keep the source code and clean installation instructions. Some log files. Many log created by the system are not necessary to store indefinitely. Any Slurm logs from failed runs (prior to a successful run) or outputs from Matlab (e.g. hs_error_pid*.log , java.log.* ) can often safely be ignored. Crash files such are core dumps (e.g. core.* , matlab_crash_dump. ). Compress Your Data Most archive locations (S@Y Archive Tier, Google Drive) perform much better with a smaller number of larger files. In fact, Google Team Drives have a file count limit of 400,000 files. Therefore, it is highly recommended that your compress, using zip or tar , portions of your data for ease of storage and retrieval. Tips for S@Y Archive Tier (or Any Tape Archive) The archive tier of Storage@Yale is a tape-based system. To use it effectively, you need to be aware of how it works and follow some best practices. When you write to the archive, you are actually copying to a large hard disk-based cache, so writes are normally fast. Your copy will appear to complete as soon as the file is in the disk cache. It is NOT yet on tape. In the background, the system will flush files to tape and delete them from the cache. If you read a file very soon after you write it, it is probably still in the cache, and your read will be quick. However, once some time has elapsed and the file has been moved to tape, it can take several minutes or even longer to copy a file from the archive. This is because the system has to: Wait until a tape drive is free (there are a limited number of drives) Load the correct tape Find the file on the tape And finally, copy it out to a disk cache in front of the tape Only then is the file available to your copy command. There are only a handful of tape drives, so you may have to wait for one to be available. Some key takeaways: Operations that only read the metadata of files will be fast (ls, find, etc) even if the file is on tape, since metadata is kept in the disk cache. Operations that actually read the file (cp, wc -l, tar, etc) will require recovering the entire file to disk cache first, and can take several minutes or longer depending on how nusy the system is. If many files will need to be recovered together, it is much better to store them as a single file first with tar or zip, then write that file to the archive. Please do NOT write huge numbers of small files. They will be difficult or impossible to restore in large numbers. Please do NOT do repetitive operations like rsyncs to the archive, since they overload the system.","title":"Archive Your Data"},{"location":"data/archive/#archive-your-data","text":"","title":"Archive Your Data"},{"location":"data/archive/#clean-out-unnecessary-files","text":"Not every file created during a project needs to be archived. If you proactively reduce the number of extraneous files in your archive, you will both reduce storage costs and increase the usefulness of that data upon retrieval. Common files that can be deleted when archiving data include: Compiled codes, such as .o or .pyc files. These files will likely not even work on the next system you may restore these data to and they can contribute significantly to your file count limit. Just keep the source code and clean installation instructions. Some log files. Many log created by the system are not necessary to store indefinitely. Any Slurm logs from failed runs (prior to a successful run) or outputs from Matlab (e.g. hs_error_pid*.log , java.log.* ) can often safely be ignored. Crash files such are core dumps (e.g. core.* , matlab_crash_dump. ).","title":"Clean Out Unnecessary Files"},{"location":"data/archive/#compress-your-data","text":"Most archive locations (S@Y Archive Tier, Google Drive) perform much better with a smaller number of larger files. In fact, Google Team Drives have a file count limit of 400,000 files. Therefore, it is highly recommended that your compress, using zip or tar , portions of your data for ease of storage and retrieval.","title":"Compress Your Data"},{"location":"data/archive/#tips-for-sy-archive-tier-or-any-tape-archive","text":"The archive tier of Storage@Yale is a tape-based system. To use it effectively, you need to be aware of how it works and follow some best practices. When you write to the archive, you are actually copying to a large hard disk-based cache, so writes are normally fast. Your copy will appear to complete as soon as the file is in the disk cache. It is NOT yet on tape. In the background, the system will flush files to tape and delete them from the cache. If you read a file very soon after you write it, it is probably still in the cache, and your read will be quick. However, once some time has elapsed and the file has been moved to tape, it can take several minutes or even longer to copy a file from the archive. This is because the system has to: Wait until a tape drive is free (there are a limited number of drives) Load the correct tape Find the file on the tape And finally, copy it out to a disk cache in front of the tape Only then is the file available to your copy command. There are only a handful of tape drives, so you may have to wait for one to be available. Some key takeaways: Operations that only read the metadata of files will be fast (ls, find, etc) even if the file is on tape, since metadata is kept in the disk cache. Operations that actually read the file (cp, wc -l, tar, etc) will require recovering the entire file to disk cache first, and can take several minutes or longer depending on how nusy the system is. If many files will need to be recovered together, it is much better to store them as a single file first with tar or zip, then write that file to the archive. Please do NOT write huge numbers of small files. They will be difficult or impossible to restore in large numbers. Please do NOT do repetitive operations like rsyncs to the archive, since they overload the system.","title":"Tips for S@Y Archive Tier (or Any Tape Archive)"},{"location":"data/google-drive/","text":"Google Drive Through Yale Google Apps for Education (Eliapps), researchers have free, unlimited Google Drive storage. The Globus Google Drive connector allows you to create a Globus endpoint that allows you to use the Globus infrastructure to transfer data into your Google Drive account. As always, no sensitive data (e.g. ePHI, HIPAA) is allowed in Google Drive storage. Eliapps If your Yale email account is already an Eliapps account (Gmail), then you are all set. If your Yale email is in Microsoft Office365, you can request a \"no-email Eliapps\" account. To do this, send an email to the ITS helpdesk to requesting a \"no-email Eliapps account\". Once it is created, you should be able to login to Google Drive using your Eliapps account name, which will be <netid>@yale.edu . The Globus connector is configured to only allow data to be uploaded into Eliapps Google Drive accounts. Google Team Drive Team Drive is an additional feature on Eliapps that is available by request only (at the moment). A Team Drive is a Google Drive space that solves a lot of ownership and permissions issues present with traditional shared Google Drive folder. Once you create a Team Drive, e.g. for a project or research group, any data placed in that Drive are owned by the drive and the permission (which accounts can own or access the data) can be easily managed from the Team Drive interface by drive owners. With Team Drive, you can be sure the data will stay with research group as students and postdocs come and go. To request Team Drive, first make sure you have an Eliapps account (see above) and then send a request to research.computing@yale.edu and we will work with ITS on your behalf to enable the feature. Local File Access You can upload and access your data using the web portal and sync data with your local machines via the Google File Stream software. For sync with your local machine, install Google File Stream . Select the Download button on the left side under \u201cBusiness\u201d and authenticate with your Eliapps account. You will see Google Drive mounted as an additional drive on your machine. Globus Google Drive Connector The Globus connector is configured to only allow data to be uploaded into Eliapps (Yale's GSuite for Education) Google Drive accounts. If you don't have an Eliapps account, request one as described above. Set Up Your Endpoint To set up your Globus Google Drive endpoint, click on the following link: Setup Globus Google Drive Endpoint Log into Globus, if needed. The first time you create an endpoint, you will be presented with a permissions approval page. If you are ok with the Connector manipulating your files through Globus (which is required), click the Allow button. The next page should say \"Create a shared endpoint\". Click on \"Yale Google Drive Gateway (Google Drive)\". Again, the first time you create an endpoint, you will be asked to register your Google Eliapps account with Globus. Put in your Eliapps account (either your email address if you are an Eliapps user, or <netid>@yale.edu if you are no-email Eliapps user) and submit the form. You will then be asked on a series of Google pages to select or login into your Eliapps account and then approve Globus to write to your Google Drive. You will then be redirected back to Globus to fill out a form to \"Create a Shared Endpoint\". The only required field are (all others can be left blank): \"Credentials\" - should be prefilled with your yale account \"Endpoint Display Name\" - this is the name of your endpoint and the name you will use to search for the endpoint in the Globus transfer interface. We recommend including your netid or name in the endpoint so you can uniquely identify it, such as \" Google Drive\" After filling out the form, click the \"Create Endpoint\" button. If your endpoint was successfully created, you should see a page with a green checkmark and three links. Click on the middle link to start transferring data to or from your Google Drive! Using Your Endpoint On the Globus Transfer page, select an endpoint for each side of your transfer. To transfer to or from your Google Drive, simply search in the Endpoint field for the name of the Endpoint you created above (e.g. \" Google Drive\"). There are \"rate limits\" to how much data and how many files you can transfer in any 24 hours period. If you are archiving data to Google Drive, it is much better to first compress folders that contain lots of small files (e.g. using tar ) before transferring. If you have hit your rate limit, Globus should automatically resume the transfer during the next 24 hour period. If you click the \"sync\" checkbox in the Transfer Setting window on the Globus page, Globus should resume the transfer where it left off when it hit the limit. In our testing, we have seen up to 10MB/s upload and 100MB/s download speeds. Managing Your Endpoint To manage your endpoint, such as delete the endpoint, rename it, or share it with additional people (be aware, they will be able to access your Google Drive), go to Manage Endpoint on the Globus website.","title":"Google Drive"},{"location":"data/google-drive/#google-drive","text":"Through Yale Google Apps for Education (Eliapps), researchers have free, unlimited Google Drive storage. The Globus Google Drive connector allows you to create a Globus endpoint that allows you to use the Globus infrastructure to transfer data into your Google Drive account. As always, no sensitive data (e.g. ePHI, HIPAA) is allowed in Google Drive storage.","title":"Google Drive"},{"location":"data/google-drive/#eliapps","text":"If your Yale email account is already an Eliapps account (Gmail), then you are all set. If your Yale email is in Microsoft Office365, you can request a \"no-email Eliapps\" account. To do this, send an email to the ITS helpdesk to requesting a \"no-email Eliapps account\". Once it is created, you should be able to login to Google Drive using your Eliapps account name, which will be <netid>@yale.edu . The Globus connector is configured to only allow data to be uploaded into Eliapps Google Drive accounts.","title":"Eliapps"},{"location":"data/google-drive/#google-team-drive","text":"Team Drive is an additional feature on Eliapps that is available by request only (at the moment). A Team Drive is a Google Drive space that solves a lot of ownership and permissions issues present with traditional shared Google Drive folder. Once you create a Team Drive, e.g. for a project or research group, any data placed in that Drive are owned by the drive and the permission (which accounts can own or access the data) can be easily managed from the Team Drive interface by drive owners. With Team Drive, you can be sure the data will stay with research group as students and postdocs come and go. To request Team Drive, first make sure you have an Eliapps account (see above) and then send a request to research.computing@yale.edu and we will work with ITS on your behalf to enable the feature.","title":"Google Team Drive"},{"location":"data/google-drive/#local-file-access","text":"You can upload and access your data using the web portal and sync data with your local machines via the Google File Stream software. For sync with your local machine, install Google File Stream . Select the Download button on the left side under \u201cBusiness\u201d and authenticate with your Eliapps account. You will see Google Drive mounted as an additional drive on your machine.","title":"Local File Access"},{"location":"data/google-drive/#globus-google-drive-connector","text":"The Globus connector is configured to only allow data to be uploaded into Eliapps (Yale's GSuite for Education) Google Drive accounts. If you don't have an Eliapps account, request one as described above.","title":"Globus Google Drive Connector"},{"location":"data/google-drive/#set-up-your-endpoint","text":"To set up your Globus Google Drive endpoint, click on the following link: Setup Globus Google Drive Endpoint Log into Globus, if needed. The first time you create an endpoint, you will be presented with a permissions approval page. If you are ok with the Connector manipulating your files through Globus (which is required), click the Allow button. The next page should say \"Create a shared endpoint\". Click on \"Yale Google Drive Gateway (Google Drive)\". Again, the first time you create an endpoint, you will be asked to register your Google Eliapps account with Globus. Put in your Eliapps account (either your email address if you are an Eliapps user, or <netid>@yale.edu if you are no-email Eliapps user) and submit the form. You will then be asked on a series of Google pages to select or login into your Eliapps account and then approve Globus to write to your Google Drive. You will then be redirected back to Globus to fill out a form to \"Create a Shared Endpoint\". The only required field are (all others can be left blank): \"Credentials\" - should be prefilled with your yale account \"Endpoint Display Name\" - this is the name of your endpoint and the name you will use to search for the endpoint in the Globus transfer interface. We recommend including your netid or name in the endpoint so you can uniquely identify it, such as \" Google Drive\" After filling out the form, click the \"Create Endpoint\" button. If your endpoint was successfully created, you should see a page with a green checkmark and three links. Click on the middle link to start transferring data to or from your Google Drive!","title":"Set Up Your Endpoint"},{"location":"data/google-drive/#using-your-endpoint","text":"On the Globus Transfer page, select an endpoint for each side of your transfer. To transfer to or from your Google Drive, simply search in the Endpoint field for the name of the Endpoint you created above (e.g. \" Google Drive\"). There are \"rate limits\" to how much data and how many files you can transfer in any 24 hours period. If you are archiving data to Google Drive, it is much better to first compress folders that contain lots of small files (e.g. using tar ) before transferring. If you have hit your rate limit, Globus should automatically resume the transfer during the next 24 hour period. If you click the \"sync\" checkbox in the Transfer Setting window on the Globus page, Globus should resume the transfer where it left off when it hit the limit. In our testing, we have seen up to 10MB/s upload and 100MB/s download speeds.","title":"Using Your Endpoint"},{"location":"data/google-drive/#managing-your-endpoint","text":"To manage your endpoint, such as delete the endpoint, rename it, or share it with additional people (be aware, they will be able to access your Google Drive), go to Manage Endpoint on the Globus website.","title":"Managing Your Endpoint"}]}